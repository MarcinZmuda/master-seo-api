"""
===============================================================================
ğŸ”„ KEYWORD SPACING VALIDATOR v1.0
===============================================================================
RozwiÄ…zuje problem keyword stuffingu przez:

1. SPACING CHECK - minimalna odlegÅ‚oÅ›Ä‡ miÄ™dzy powtÃ³rzeniami
2. SYNONYM SUGGESTIONS - gdy za blisko, sugeruj alternatywy  
3. DISTRIBUTION SCORE - czy frazy sÄ… rÃ³wnomiernie rozÅ‚oÅ¼one
4. PRE-BATCH CONTEXT - info gdzie byÅ‚a ostatnio uÅ¼yta fraza

INTEGRACJA:
- pre_batch_info: dodaj last_usage_info dla kaÅ¼dej frazy
- batch_simple: waliduj spacing przed zatwierdzeniem
- instrukcje agenta: "NIE UÅ»YWAJ X, uÅ¼yj synonimu Y"

===============================================================================
"""

import re
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass
import math


# ============================================================================
# KONFIGURACJA
# ============================================================================

# Minimalna odlegÅ‚oÅ›Ä‡ (w sÅ‚owach) miÄ™dzy powtÃ³rzeniami tej samej frazy
MINIMUM_SPACING = {
    "MAIN": 60,       # Fraza gÅ‚Ã³wna - co ~60 sÅ‚Ã³w OK
    "BASIC": 80,      # BASIC - co ~80 sÅ‚Ã³w  
    "EXTENDED": 120,  # EXTENDED - co ~120 sÅ‚Ã³w
    "H2": 150,        # H2 header terms - co ~150 sÅ‚Ã³w
}

# Maksymalny % exact match (reszta musi byÄ‡ odmianami/synonimami)
MAX_EXACT_MATCH_RATIO = 0.50  # Max 50% moÅ¼e byÄ‡ identyczne

# PrÃ³g stuffingu w akapicie
PARAGRAPH_STUFFING_THRESHOLD = 2  # Max 2x ta sama fraza w akapicie

# PrÃ³g stuffingu w zdaniu
SENTENCE_STUFFING_THRESHOLD = 1  # Max 1x ta sama fraza w zdaniu

# IloÅ›Ä‡ sÅ‚Ã³w na koÅ„cu poprzedniego batcha do sprawdzenia
PREVIOUS_BATCH_CONTEXT_WORDS = 80


# ============================================================================
# SYNONIMY DLA FRAZ SEO (rozszerzenie contextual_synonyms)
# ============================================================================

# Synonimy dla caÅ‚ych fraz (nie pojedynczych sÅ‚Ã³w)
PHRASE_SYNONYMS = {
    # Medyczne
    "zespÃ³Å‚ turnera": [
        "ta aberracja chromosomalna", 
        "to schorzenie genetyczne",
        "omawiany zespÃ³Å‚",
        "ta jednostka kliniczna",
        "turner syndrome"  # dla kontekstu miÄ™dzynarodowego
    ],
    "choroba genetyczna": [
        "schorzenie genetyczne",
        "zaburzenie genetyczne", 
        "wada wrodzona",
        "ta choroba"
    ],
    "aberracja chromosomalna": [
        "zaburzenie chromosomowe",
        "anomalia genetyczna",
        "ta aberracja"
    ],
    
    # Prawne
    "sÄ…d rodzinny": [
        "sÄ…d opiekuÅ„czy",
        "ten sÄ…d",
        "wÅ‚aÅ›ciwy sÄ…d",
        "organ orzekajÄ…cy"
    ],
    "wÅ‚adza rodzicielska": [
        "prawa rodzicielskie",
        "opieka rodzicielska",
        "ta wÅ‚adza"
    ],
    "miejsce pobytu dziecka": [
        "miejsce zamieszkania dziecka",
        "adres dziecka",
        "to miejsce"
    ],
    "porwanie rodzicielskie": [
        "uprowadzenie przez rodzica",
        "samowolne zabranie dziecka",
        "to porwanie"
    ],
    
    # OgÃ³lne
    "niski wzrost": [
        "niskorosÅ‚oÅ›Ä‡",
        "niedobÃ³r wzrostu",
        "mniejszy wzrost"
    ],
}

# Zaimki/okreÅ›lenia zastÄ™pcze uniwersalne
UNIVERSAL_SUBSTITUTES = {
    "MEDICAL": [
        "ta choroba", "to schorzenie", "ta jednostka", 
        "omawiany zespÃ³Å‚", "opisywane zaburzenie"
    ],
    "LEGAL": [
        "ten sÄ…d", "ta instytucja", "wÅ‚aÅ›ciwy organ",
        "omawiana sprawa", "przedmiotowa kwestia"
    ],
    "GENERIC": [
        "ta kwestia", "omawiany temat", "przedmiotowe zagadnienie"
    ]
}


# ============================================================================
# DATACLASSES
# ============================================================================

@dataclass
class PhrasePosition:
    """Pozycja frazy w tekÅ›cie."""
    phrase: str
    word_position: int  # Pozycja w sÅ‚owach od poczÄ…tku
    char_position: int  # Pozycja w znakach
    context: str  # Kilka sÅ‚Ã³w przed i po


@dataclass  
class SpacingViolation:
    """Naruszenie minimalnej odlegÅ‚oÅ›ci."""
    phrase: str
    phrase_type: str
    position1: int
    position2: int
    actual_distance: int
    min_required: int
    severity: str  # CRITICAL, WARNING
    suggestion: str


@dataclass
class LastUsageInfo:
    """Info o ostatnim uÅ¼yciu frazy - dla pre_batch_info."""
    phrase: str
    words_ago: int  # Ile sÅ‚Ã³w temu (od koÅ„ca poprzedniego batcha)
    can_use_now: bool  # Czy moÅ¼na uÅ¼yÄ‡ na poczÄ…tku nowego batcha
    suggested_wait: int  # Ile sÅ‚Ã³w poczekaÄ‡
    alternatives: List[str]  # Synonimy do uÅ¼ycia zamiast


# ============================================================================
# CORE FUNCTIONS
# ============================================================================

def tokenize_to_words(text: str) -> List[str]:
    """Tokenizuje tekst na sÅ‚owa."""
    if not text:
        return []
    return re.findall(r'\b\w+\b', text.lower())


def find_phrase_positions(
    text: str, 
    phrase: str,
    use_lemmatization: bool = False
) -> List[PhrasePosition]:
    """
    Znajduje wszystkie pozycje frazy w tekÅ›cie.
    
    Args:
        text: Tekst do przeszukania
        phrase: Fraza do znalezienia
        use_lemmatization: Czy uÅ¼ywaÄ‡ lemmatyzacji (wymaga polish_lemmatizer)
    
    Returns:
        Lista pozycji frazy
    """
    if not text or not phrase:
        return []
    
    positions = []
    text_lower = text.lower()
    phrase_lower = phrase.lower().strip()
    words = tokenize_to_words(text)
    phrase_words = phrase_lower.split()
    phrase_len = len(phrase_words)
    
    if phrase_len == 0:
        return []
    
    # Szukaj frazy w tekÅ›cie
    for i in range(len(words) - phrase_len + 1):
        window = words[i:i + phrase_len]
        
        # SprawdÅº dopasowanie (z tolerancjÄ… na koÅ„cÃ³wki fleksyjne)
        match = True
        for pw, tw in zip(phrase_words, window):
            # Heurystyka: pierwsze 4 litery muszÄ… siÄ™ zgadzaÄ‡ (dla fleksji)
            min_len = min(len(pw), len(tw), 4)
            if len(pw) >= 4 and len(tw) >= 4:
                if pw[:min_len] != tw[:min_len]:
                    match = False
                    break
            elif pw != tw:
                match = False
                break
        
        if match:
            # ZnajdÅº pozycjÄ™ znakowÄ…
            char_pos = 0
            word_count = 0
            for m in re.finditer(r'\b\w+\b', text_lower):
                if word_count == i:
                    char_pos = m.start()
                    break
                word_count += 1
            
            # Kontekst (3 sÅ‚owa przed i po)
            context_start = max(0, i - 3)
            context_end = min(len(words), i + phrase_len + 3)
            context = " ".join(words[context_start:context_end])
            
            positions.append(PhrasePosition(
                phrase=phrase,
                word_position=i,
                char_position=char_pos,
                context=context
            ))
    
    return positions


def check_spacing_violations(
    text: str,
    phrase: str,
    phrase_type: str = "BASIC"
) -> List[SpacingViolation]:
    """
    Sprawdza czy fraza nie wystÄ™puje zbyt blisko poprzedniego uÅ¼ycia.
    
    Returns:
        Lista naruszeÅ„ (pusta = wszystko OK)
    """
    min_spacing = MINIMUM_SPACING.get(phrase_type.upper(), 80)
    positions = find_phrase_positions(text, phrase)
    
    if len(positions) < 2:
        return []
    
    violations = []
    
    for i in range(1, len(positions)):
        distance = positions[i].word_position - positions[i-1].word_position
        
        if distance < min_spacing:
            severity = "CRITICAL" if distance < min_spacing // 2 else "WARNING"
            
            # Pobierz synonimy
            alternatives = get_phrase_alternatives(phrase, phrase_type)
            alt_str = ", ".join(alternatives[:3]) if alternatives else "uÅ¼yj zaimka"
            
            violations.append(SpacingViolation(
                phrase=phrase,
                phrase_type=phrase_type,
                position1=positions[i-1].word_position,
                position2=positions[i].word_position,
                actual_distance=distance,
                min_required=min_spacing,
                severity=severity,
                suggestion=f"ZamieÅ„ jedno uÅ¼ycie na: {alt_str}"
            ))
    
    return violations


def get_phrase_alternatives(phrase: str, phrase_type: str = "BASIC") -> List[str]:
    """
    Zwraca alternatywy dla frazy (synonimy + zaimki).
    
    Args:
        phrase: Fraza oryginalna
        phrase_type: Typ frazy (MAIN, BASIC, EXTENDED)
    
    Returns:
        Lista alternatyw
    """
    phrase_lower = phrase.lower().strip()
    alternatives = []
    
    # 1. SprawdÅº dokÅ‚adne synonimy frazy
    if phrase_lower in PHRASE_SYNONYMS:
        alternatives.extend(PHRASE_SYNONYMS[phrase_lower])
    
    # 2. Dodaj uniwersalne zamienniki
    # Wykryj domenÄ™ na podstawie frazy
    domain = detect_domain(phrase)
    if domain in UNIVERSAL_SUBSTITUTES:
        alternatives.extend(UNIVERSAL_SUBSTITUTES[domain])
    
    # 3. Dodaj formy fleksyjne jako "alternatywy"
    # (w sensie: "uÅ¼yj dopeÅ‚niacza zamiast mianownika")
    if " " in phrase:
        # Wielowyrazowa - sugeruj odmianÄ™
        alternatives.append(f"odmiana: '{phrase}' w innym przypadku")
    
    # UsuÅ„ duplikaty, zachowaj kolejnoÅ›Ä‡
    seen = set()
    unique = []
    for alt in alternatives:
        if alt.lower() not in seen:
            seen.add(alt.lower())
            unique.append(alt)
    
    return unique[:6]  # Max 6 alternatyw


def detect_domain(phrase: str) -> str:
    """Wykrywa domenÄ™ frazy (MEDICAL, LEGAL, GENERIC)."""
    phrase_lower = phrase.lower()
    
    medical_markers = ["zespÃ³Å‚", "choroba", "schorzenie", "objaw", "leczenie", 
                       "pacjent", "diagnoza", "genetycz", "chromosom"]
    legal_markers = ["sÄ…d", "prawo", "ustawa", "wyrok", "rodzic", "dziecko",
                     "porwanie", "wÅ‚adza", "opiek"]
    
    for marker in medical_markers:
        if marker in phrase_lower:
            return "MEDICAL"
    
    for marker in legal_markers:
        if marker in phrase_lower:
            return "LEGAL"
    
    return "GENERIC"


# ============================================================================
# PRE-BATCH CONTEXT - info dla agenta
# ============================================================================

def get_last_usage_info(
    previous_batch_text: str,
    phrase: str,
    phrase_type: str = "BASIC"
) -> LastUsageInfo:
    """
    Zwraca info o ostatnim uÅ¼yciu frazy w poprzednim batchu.
    Do uÅ¼ycia w pre_batch_info.
    
    Args:
        previous_batch_text: Tekst poprzedniego batcha
        phrase: Fraza do sprawdzenia
        phrase_type: Typ frazy
    
    Returns:
        LastUsageInfo z info czy moÅ¼na uÅ¼yÄ‡ i alternatywami
    """
    min_spacing = MINIMUM_SPACING.get(phrase_type.upper(), 80)
    
    if not previous_batch_text:
        return LastUsageInfo(
            phrase=phrase,
            words_ago=999,
            can_use_now=True,
            suggested_wait=0,
            alternatives=[]
        )
    
    # WeÅº ostatnie N sÅ‚Ã³w
    words = tokenize_to_words(previous_batch_text)
    last_words = words[-PREVIOUS_BATCH_CONTEXT_WORDS:] if len(words) > PREVIOUS_BATCH_CONTEXT_WORDS else words
    last_text = " ".join(last_words)
    
    # ZnajdÅº pozycje frazy
    positions = find_phrase_positions(last_text, phrase)
    
    if not positions:
        return LastUsageInfo(
            phrase=phrase,
            words_ago=999,
            can_use_now=True,
            suggested_wait=0,
            alternatives=[]
        )
    
    # Ostatnia pozycja
    last_pos = positions[-1].word_position
    words_ago = len(last_words) - last_pos
    
    # Czy moÅ¼na uÅ¼yÄ‡ na poczÄ…tku nowego batcha?
    can_use = words_ago >= min_spacing
    suggested_wait = max(0, min_spacing - words_ago) if not can_use else 0
    
    # Alternatywy jeÅ›li nie moÅ¼na
    alternatives = get_phrase_alternatives(phrase, phrase_type) if not can_use else []
    
    return LastUsageInfo(
        phrase=phrase,
        words_ago=words_ago,
        can_use_now=can_use,
        suggested_wait=suggested_wait,
        alternatives=alternatives
    )


def generate_spacing_instructions(
    keywords_state: Dict[str, dict],
    previous_batch_text: str = ""
) -> Dict:
    """
    Generuje instrukcje spacing dla agenta.
    Dodaj wynik do pre_batch_info.
    
    Returns:
        {
            "spacing_rules": [...],
            "avoid_at_start": [...],
            "can_use_freely": [...],
            "fleksja_reminder": str
        }
    """
    result = {
        "spacing_rules": [],
        "avoid_at_start": [],
        "can_use_freely": [],
        "fleksja_reminder": "ğŸ”„ FORMY FLEKSYJNE: 'zespoÅ‚u turnera' = 'zespÃ³Å‚ turnera' = 'zespoÅ‚em turnera'"
    }
    
    for rid, meta in keywords_state.items():
        phrase = meta.get("keyword", "").strip()
        phrase_type = meta.get("type", "BASIC").upper()
        
        if not phrase:
            continue
        
        min_spacing = MINIMUM_SPACING.get(phrase_type, 80)
        
        # Dodaj reguÅ‚Ä™ spacing
        result["spacing_rules"].append({
            "phrase": phrase,
            "type": phrase_type,
            "min_spacing": min_spacing,
            "rule": f"'{phrase}' â†’ min {min_spacing} sÅ‚Ã³w miÄ™dzy uÅ¼yciami"
        })
        
        # SprawdÅº poprzedni batch
        if previous_batch_text:
            last_usage = get_last_usage_info(previous_batch_text, phrase, phrase_type)
            
            if not last_usage.can_use_now:
                result["avoid_at_start"].append({
                    "phrase": phrase,
                    "words_ago": last_usage.words_ago,
                    "wait": last_usage.suggested_wait,
                    "alternatives": last_usage.alternatives,
                    "instruction": f"âš ï¸ '{phrase}' byÅ‚a {last_usage.words_ago} sÅ‚Ã³w temu - poczekaj ~{last_usage.suggested_wait} sÅ‚Ã³w lub uÅ¼yj: {', '.join(last_usage.alternatives[:2])}"
                })
            else:
                result["can_use_freely"].append(phrase)
    
    return result


# ============================================================================
# BATCH VALIDATION
# ============================================================================

def validate_batch_spacing(
    batch_text: str,
    keywords_state: Dict[str, dict],
    previous_batch_text: str = ""
) -> Dict:
    """
    Waliduje spacing w batchu.
    UÅ¼ywaj w batch_simple przed zatwierdzeniem.
    
    Returns:
        {
            "is_valid": bool,
            "score": float (0-100),
            "violations": [...],
            "paragraph_stuffing": [...],
            "sentence_stuffing": [...],
            "suggestions": [...]
        }
    """
    result = {
        "is_valid": True,
        "score": 100.0,
        "violations": [],
        "paragraph_stuffing": [],
        "sentence_stuffing": [],
        "suggestions": []
    }
    
    # PoÅ‚Ä…cz z poprzednim batchem dla sprawdzenia ciÄ…gÅ‚oÅ›ci
    full_text = (previous_batch_text + "\n\n" + batch_text) if previous_batch_text else batch_text
    
    for rid, meta in keywords_state.items():
        phrase = meta.get("keyword", "").strip()
        phrase_type = meta.get("type", "BASIC").upper()
        
        if not phrase:
            continue
        
        # 1. Spacing violations (w poÅ‚Ä…czonym tekÅ›cie)
        violations = check_spacing_violations(full_text, phrase, phrase_type)
        for v in violations:
            result["violations"].append({
                "phrase": v.phrase,
                "type": v.phrase_type,
                "distance": v.actual_distance,
                "min_required": v.min_required,
                "severity": v.severity,
                "suggestion": v.suggestion
            })
            
            if v.severity == "CRITICAL":
                result["score"] -= 15
            else:
                result["score"] -= 8
        
        # 2. Paragraph stuffing (tylko w nowym batchu)
        para_stuff = check_paragraph_stuffing(batch_text, phrase)
        result["paragraph_stuffing"].extend(para_stuff)
        result["score"] -= len(para_stuff) * 5
        
        # 3. Sentence stuffing (tylko w nowym batchu)
        sent_stuff = check_sentence_stuffing(batch_text, phrase)
        result["sentence_stuffing"].extend(sent_stuff)
        result["score"] -= len(sent_stuff) * 20  # Bardzo powaÅ¼ne
    
    # Clamp score
    result["score"] = max(0, min(100, result["score"]))
    
    # Determine validity
    result["is_valid"] = (
        result["score"] >= 60 and
        len(result["sentence_stuffing"]) == 0
    )
    
    # Generate suggestions
    if result["violations"]:
        result["suggestions"].append(
            "RozÅ‚Ã³Å¼ frazy bardziej rÃ³wnomiernie - uÅ¼yj synonimÃ³w lub form fleksyjnych"
        )
    
    if result["paragraph_stuffing"]:
        result["suggestions"].append(
            "NiektÃ³re akapity majÄ… za duÅ¼o tej samej frazy - rozdziel na wiÄ™cej akapitÃ³w"
        )
    
    if result["sentence_stuffing"]:
        result["suggestions"].append(
            "âŒ KRYTYCZNE: Ta sama fraza 2x w jednym zdaniu - przepisz!"
        )
    
    return result


def check_paragraph_stuffing(text: str, phrase: str) -> List[str]:
    """Sprawdza czy fraza nie jest za czÄ™sto w jednym akapicie."""
    warnings = []
    paragraphs = re.split(r'\n\s*\n|\n', text)
    
    for i, para in enumerate(paragraphs):
        if not para.strip():
            continue
        
        positions = find_phrase_positions(para, phrase)
        if len(positions) > PARAGRAPH_STUFFING_THRESHOLD:
            warnings.append(
                f"Akapit {i+1}: '{phrase}' wystÄ™puje {len(positions)}x (max {PARAGRAPH_STUFFING_THRESHOLD})"
            )
    
    return warnings


def check_sentence_stuffing(text: str, phrase: str) -> List[str]:
    """Sprawdza czy fraza nie jest 2x w tym samym zdaniu."""
    warnings = []
    sentences = re.split(r'[.!?]+', text)
    
    for i, sent in enumerate(sentences):
        if not sent.strip():
            continue
        
        positions = find_phrase_positions(sent, phrase)
        if len(positions) > SENTENCE_STUFFING_THRESHOLD:
            warnings.append(
                f"Zdanie {i+1}: '{phrase}' wystÄ™puje {len(positions)}x w jednym zdaniu!"
            )
    
    return warnings


# ============================================================================
# DISTRIBUTION SCORE
# ============================================================================

def calculate_distribution_score(text: str, phrase: str) -> Dict:
    """
    Oblicza jak rÃ³wnomiernie rozÅ‚oÅ¼ona jest fraza w tekÅ›cie.
    
    Returns:
        {
            "score": float (0-100),
            "is_even": bool,
            "gaps": [...],  # Lista odstÄ™pÃ³w miÄ™dzy uÅ¼yciami
            "cv": float,    # Coefficient of variation
            "suggestion": str
        }
    """
    positions = find_phrase_positions(text, phrase)
    
    if len(positions) < 2:
        return {
            "score": 100.0,
            "is_even": True,
            "gaps": [],
            "cv": 0.0,
            "suggestion": ""
        }
    
    # Oblicz odstÄ™py
    gaps = []
    for i in range(1, len(positions)):
        gap = positions[i].word_position - positions[i-1].word_position
        gaps.append(gap)
    
    # Oblicz CV (coefficient of variation)
    mean_gap = sum(gaps) / len(gaps)
    variance = sum((g - mean_gap) ** 2 for g in gaps) / len(gaps)
    std_dev = math.sqrt(variance)
    cv = std_dev / mean_gap if mean_gap > 0 else 0
    
    # Score: niÅ¼sze CV = lepszy rozkÅ‚ad
    # CV < 0.3 = Å›wietny, CV > 0.7 = sÅ‚aby
    if cv < 0.3:
        score = 100.0
    elif cv < 0.5:
        score = 80.0
    elif cv < 0.7:
        score = 60.0
    else:
        score = 40.0
    
    is_even = cv < 0.5
    
    suggestion = ""
    if not is_even:
        min_gap = min(gaps)
        max_gap = max(gaps)
        suggestion = f"Fraza jest nierÃ³wnomiernie rozÅ‚oÅ¼ona (odstÄ™py: {min_gap}-{max_gap} sÅ‚Ã³w). WyrÃ³wnaj rozkÅ‚ad."
    
    return {
        "score": score,
        "is_even": is_even,
        "gaps": gaps,
        "cv": round(cv, 2),
        "suggestion": suggestion
    }


# ============================================================================
# FORMAT FOR PROMPT
# ============================================================================

def format_spacing_instructions_for_prompt(instructions: Dict) -> str:
    """Formatuje instrukcje spacing do promptu agenta."""
    lines = []
    
    lines.append("\n" + "=" * 60)
    lines.append("ğŸ“ SPACING RULES - OdstÄ™py miÄ™dzy frazami")
    lines.append("=" * 60)
    
    # Fleksja reminder
    lines.append(f"\n{instructions['fleksja_reminder']}")
    
    # Avoid at start (najwaÅ¼niejsze!)
    if instructions["avoid_at_start"]:
        lines.append("\nâš ï¸ NA POCZÄ„TKU BATCHA - UNIKAJ:")
        for item in instructions["avoid_at_start"]:
            lines.append(f"   â€¢ {item['instruction']}")
    
    # Spacing rules
    if instructions["spacing_rules"]:
        lines.append("\nğŸ“ MINIMALNE ODSTÄ˜PY:")
        for rule in instructions["spacing_rules"][:5]:
            lines.append(f"   â€¢ {rule['rule']}")
    
    # Can use freely
    if instructions["can_use_freely"]:
        lines.append(f"\nâœ… MOÅ»NA UÅ»YÄ† OD RAZU: {', '.join(instructions['can_use_freely'][:5])}")
    
    return "\n".join(lines)


# ============================================================================
# MAIN - TEST
# ============================================================================

if __name__ == "__main__":
    # Test na tekÅ›cie z keyword stuffingiem
    test_text = """
ZespÃ³Å‚ Turnera jest jednÄ… z rzadkich jednostek klinicznych. ZespÃ³Å‚ Turnera 
to choroba genetyczna, jednak jednoczeÅ›nie podkreÅ›la siÄ™, Å¼e zespÃ³Å‚ Turnera 
nie jest chorobÄ… w rozumieniu stanu caÅ‚kowicie wykluczajÄ…cego funkcjonowanie.

W praktyce klinicznej zespÃ³Å‚ Turnera jest chorobÄ… o bardzo zrÃ³Å¼nicowanym przebiegu.
Przypadki zespoÅ‚u Turnera rÃ³Å¼niÄ… siÄ™ nasileniem objawÃ³w.

Do najczÄ™stszych objawÃ³w zespoÅ‚u Turnera naleÅ¼y niski wzrost. Niski wzrost 
wystÄ™puje u wiÄ™kszoÅ›ci pacjentek z tym zespoÅ‚em.
"""
    
    previous_batch = """
Ostatni akapit poprzedniego batcha wspomina o zespole Turnera i jego objawach.
ZespÃ³Å‚ Turnera jest czÄ™sto diagnozowany w dzieciÅ„stwie.
"""
    
    keywords_state = {
        "k1": {"keyword": "zespÃ³Å‚ turnera", "type": "MAIN"},
        "k2": {"keyword": "choroba genetyczna", "type": "BASIC"},
        "k3": {"keyword": "niski wzrost", "type": "BASIC"},
    }
    
    print("=" * 70)
    print("TEST KEYWORD SPACING VALIDATOR")
    print("=" * 70)
    
    # 1. Test spacing instructions
    print("\nğŸ“‹ SPACING INSTRUCTIONS:")
    instructions = generate_spacing_instructions(keywords_state, previous_batch)
    print(format_spacing_instructions_for_prompt(instructions))
    
    # 2. Test batch validation
    print("\n" + "=" * 70)
    print("ğŸ“Š BATCH VALIDATION:")
    print("=" * 70)
    
    validation = validate_batch_spacing(test_text, keywords_state, previous_batch)
    
    print(f"\nValid: {'âœ… TAK' if validation['is_valid'] else 'âŒ NIE'}")
    print(f"Score: {validation['score']}/100")
    
    if validation["violations"]:
        print("\nâš ï¸ SPACING VIOLATIONS:")
        for v in validation["violations"]:
            print(f"  - '{v['phrase']}': {v['distance']} sÅ‚Ã³w (min {v['min_required']}) [{v['severity']}]")
    
    if validation["paragraph_stuffing"]:
        print("\nâš ï¸ PARAGRAPH STUFFING:")
        for p in validation["paragraph_stuffing"]:
            print(f"  - {p}")
    
    if validation["sentence_stuffing"]:
        print("\nâŒ SENTENCE STUFFING:")
        for s in validation["sentence_stuffing"]:
            print(f"  - {s}")
    
    if validation["suggestions"]:
        print("\nğŸ’¡ SUGGESTIONS:")
        for s in validation["suggestions"]:
            print(f"  - {s}")
    
    # 3. Test distribution
    print("\n" + "=" * 70)
    print("ğŸ“ˆ DISTRIBUTION ANALYSIS:")
    print("=" * 70)
    
    for rid, meta in keywords_state.items():
        phrase = meta.get("keyword", "")
        dist = calculate_distribution_score(test_text, phrase)
        print(f"\n'{phrase}':")
        print(f"  Score: {dist['score']}/100 | CV: {dist['cv']} | Even: {dist['is_even']}")
        if dist["gaps"]:
            print(f"  Gaps: {dist['gaps']}")
        if dist["suggestion"]:
            print(f"  â†’ {dist['suggestion']}")
