"""
===============================================================================
üéØ SEMANTIC PHRASE ASSIGNMENT v1.0
===============================================================================
Przypisuje frazy, encje i triplety do konkretnych H2 na podstawie 
podobie≈Ñstwa semantycznego.

PROBLEM KT√ìRY ROZWIƒÑZUJE:
- Agent dostaje 40 fraz bez kontekstu ‚Üí wybiera "≈Çatwe"
- Frazy nie pasujƒÖ do aktualnego H2 ‚Üí nienaturalne wplecenie

ROZWIƒÑZANIE:
- Analiza semantyczna: kt√≥ra fraza pasuje do kt√≥rego H2
- Agent wie GDZIE u≈ºyƒá frazy (nie "gdziekolwiek")
- Przyk≈Çady zda≈Ñ dopasowane do kontekstu H2

===============================================================================
"""

import re
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, field
from collections import defaultdict


# ============================================================================
# KONFIGURACJA
# ============================================================================

@dataclass
class AssignmentConfig:
    """Konfiguracja przypisywania element√≥w do H2."""
    
    # Minimalne podobie≈Ñstwo do przypisania
    MIN_RELEVANCE_THRESHOLD: float = 0.25
    
    # Max element√≥w MUST per batch
    MAX_MUST_PHRASES_PER_BATCH: int = 5
    MAX_MUST_ENTITIES_PER_BATCH: int = 3
    MAX_MUST_TRIPLETS_PER_BATCH: int = 2
    
    # Wagi dla oblicze≈Ñ similarity
    JACCARD_WEIGHT: float = 0.4
    KEYWORD_OVERLAP_WEIGHT: float = 0.3
    DOMAIN_HINT_WEIGHT: float = 0.3


CONFIG = AssignmentConfig()


# ============================================================================
# DOMAIN HINTS - wskaz√≥wki dla r√≥≈ºnych domen
# ============================================================================

DOMAIN_HINTS = {
    "prawo": {
        "sƒÖd": ["procedura", "postƒôpowanie", "orzeczenie", "rozprawa", "wyrok", "sprawa"],
        "kodeks": ["kara", "odpowiedzialno≈õƒá", "przestƒôpstwo", "przepis", "artyku≈Ç"],
        "konwencja": ["miƒôdzynarodowy", "granica", "zagraniczny", "haska"],
        "ustawa": ["prawo", "regulacja", "przepis", "obowiƒÖzek"],
        "wniosek": ["procedura", "z≈Ço≈ºyƒá", "sƒÖd", "podanie"],
        "orzeczenie": ["sƒÖd", "wyrok", "decyzja", "rozstrzygniƒôcie"],
        "w≈Çadza": ["rodzicielska", "ograniczenie", "pozbawienie", "sƒÖd"],
        "miejsce pobytu": ["dziecko", "ustalenie", "sƒÖd", "rodzic"],
        "uprowadzenie": ["dziecko", "porwanie", "karne", "przestƒôpstwo"],
        "kontakt": ["dziecko", "rodzic", "prawo", "regulacja"],
    },
    "medycyna": {
        "lekarz": ["diagnoza", "leczenie", "badanie", "konsultacja"],
        "pacjent": ["choroba", "leczenie", "objawy", "terapia"],
        "lek": ["dawka", "skutki", "dzia≈Çanie", "recepta"],
    },
    "finanse": {
        "kredyt": ["bank", "rata", "oprocentowanie", "sp≈Çata"],
        "inwestycja": ["zysk", "ryzyko", "portfel", "stopa"],
    }
}


# ============================================================================
# SEMANTIC SIMILARITY
# ============================================================================

def calculate_semantic_similarity(
    phrase: str, 
    h2_title: str,
    domain: str = "prawo"
) -> float:
    """
    Oblicza podobie≈Ñstwo semantyczne frazy do H2.
    
    Sk≈Çadowe:
    1. Jaccard similarity (wsp√≥lne s≈Çowa)
    2. Keyword overlap (s≈Çowa kluczowe domeny)
    3. Domain hints (mapowanie typowe dla domeny)
    
    Returns:
        float: 0.0 - 1.0
    """
    phrase_lower = phrase.lower().strip()
    h2_lower = h2_title.lower().strip()
    
    # Tokenizacja
    phrase_words = set(re.findall(r'\b\w{3,}\b', phrase_lower))
    h2_words = set(re.findall(r'\b\w{3,}\b', h2_lower))
    
    # 1. Jaccard similarity
    intersection = phrase_words & h2_words
    union = phrase_words | h2_words
    jaccard = len(intersection) / len(union) if union else 0
    
    # 2. Keyword overlap - wsp√≥lne s≈Çowa o d≈Çugo≈õci > 4
    significant_intersection = set(w for w in intersection if len(w) > 4)
    keyword_overlap = len(significant_intersection) * 0.2
    
    # 3. Domain hints
    domain_score = 0
    domain_hints = DOMAIN_HINTS.get(domain, {})
    
    for hint_word, related_words in domain_hints.items():
        # Czy fraza zawiera hint?
        if hint_word in phrase_lower:
            # Czy H2 zawiera powiƒÖzane s≈Çowa?
            for related in related_words:
                if related in h2_lower:
                    domain_score += 0.15
                    break
        
        # Odwrotnie: czy H2 zawiera hint, a fraza related?
        if hint_word in h2_lower:
            for related in related_words:
                if related in phrase_lower:
                    domain_score += 0.1
                    break
    
    # Suma wa≈ºona
    total = (
        jaccard * CONFIG.JACCARD_WEIGHT +
        min(keyword_overlap, 0.4) * CONFIG.KEYWORD_OVERLAP_WEIGHT +
        min(domain_score, 0.5) * CONFIG.DOMAIN_HINT_WEIGHT
    )
    
    return min(1.0, total)


# ============================================================================
# PHRASE ASSIGNMENT
# ============================================================================

def assign_phrases_to_h2(
    keywords_state: Dict,
    h2_structure: List[str],
    main_keyword: str,
    domain: str = "prawo"
) -> Dict[str, List[Dict]]:
    """
    Przypisuje frazy do konkretnych H2 na podstawie podobie≈Ñstwa semantycznego.
    
    Args:
        keywords_state: Stan fraz {rid: {keyword, type, actual_uses, ...}}
        h2_structure: Lista tytu≈Ç√≥w H2
        main_keyword: G≈Ç√≥wne s≈Çowo kluczowe
        domain: Domena (prawo, medycyna, finanse, ...)
    
    Returns:
        Dict mapping H2 ‚Üí lista fraz z relevance score
        
    Example:
        {
            "Czym jest porwanie rodzicielskie": [
                {"keyword": "porwanie rodzicielskie", "type": "MAIN", "relevance": 0.95},
                {"keyword": "definicja porwania", "type": "BASIC", "relevance": 0.72}
            ],
            "Procedura sƒÖdowa": [
                {"keyword": "sƒÖd rodzinny", "type": "BASIC", "relevance": 0.88},
                {"keyword": "wniosek do sƒÖdu", "type": "EXTENDED", "relevance": 0.65}
            ]
        }
    """
    assignments = {h2: [] for h2 in h2_structure}
    assigned_keywords = set()  # ≈öled≈∫ ju≈º przypisane
    
    # Zbierz wszystkie frazy
    all_phrases = []
    for rid, meta in keywords_state.items():
        keyword = meta.get("keyword", "")
        if not keyword:
            continue
        
        all_phrases.append({
            "rid": rid,
            "keyword": keyword,
            "type": meta.get("type", "BASIC").upper(),
            "actual_uses": meta.get("actual_uses", 0),
            "target_min": meta.get("target_min", 1),
            "target_max": meta.get("target_max", 10),
            "is_main": meta.get("is_main_keyword", False)
        })
    
    # Sortuj: MAIN > BASIC nieu≈ºyte > BASIC u≈ºyte > EXTENDED
    def phrase_priority(p):
        if p["is_main"]:
            return (0, p["actual_uses"])
        if p["type"] == "BASIC" and p["actual_uses"] == 0:
            return (1, 0)
        if p["type"] == "BASIC":
            return (2, p["actual_uses"])
        return (3, p["actual_uses"])
    
    all_phrases.sort(key=phrase_priority)
    
    # Przypisz ka≈ºdƒÖ frazƒô do najlepszego H2
    for phrase_data in all_phrases:
        keyword = phrase_data["keyword"]
        
        if keyword in assigned_keywords:
            continue
        
        best_h2 = None
        best_score = CONFIG.MIN_RELEVANCE_THRESHOLD
        
        for h2 in h2_structure:
            score = calculate_semantic_similarity(keyword, h2, domain)
            
            # Bonus dla main keyword - przypisz do pierwszego H2 (definicja)
            if phrase_data["is_main"] and h2 == h2_structure[0]:
                score += 0.3
            
            # Bonus je≈õli H2 ma ma≈Ço przypisanych fraz
            current_count = len(assignments[h2])
            if current_count < 3:
                score += 0.05
            
            if score > best_score:
                best_score = score
                best_h2 = h2
        
        if best_h2:
            assignments[best_h2].append({
                **phrase_data,
                "relevance": round(best_score, 3)
            })
            assigned_keywords.add(keyword)
    
    # Sortuj frazy w ka≈ºdym H2 po relevance
    for h2 in assignments:
        assignments[h2].sort(key=lambda x: (-x["relevance"], x["actual_uses"]))
    
    # Rozprowad≈∫ nieprzypisane frazy r√≥wnomiernie
    unassigned = [p for p in all_phrases if p["keyword"] not in assigned_keywords]
    if unassigned:
        # Przypisz do H2 z najmniejszƒÖ liczbƒÖ fraz
        for phrase_data in unassigned:
            min_h2 = min(h2_structure, key=lambda h: len(assignments[h]))
            assignments[min_h2].append({
                **phrase_data,
                "relevance": 0.1,
                "fallback": True
            })
    
    return assignments


# ============================================================================
# ENTITY ASSIGNMENT
# ============================================================================

def assign_entities_to_h2(
    entities: List[Dict],
    h2_structure: List[str],
    domain: str = "prawo"
) -> Dict[str, List[Dict]]:
    """
    Przypisuje encje do H2 na podstawie kontekstu.
    
    Args:
        entities: Lista encji z S1 [{name, importance, sources_count, ...}]
        h2_structure: Lista tytu≈Ç√≥w H2
        domain: Domena
    
    Returns:
        Dict mapping H2 ‚Üí lista encji
    """
    assignments = {h2: [] for h2 in h2_structure}
    
    for entity in entities:
        name = entity.get("name", "")
        if not name:
            continue
        
        best_h2 = None
        best_score = 0
        
        for h2 in h2_structure:
            score = calculate_semantic_similarity(name, h2, domain)
            
            # Bonus dla wa≈ºnych encji - przypisz do wczesnych H2
            importance = entity.get("importance", 0.5)
            if importance >= 0.7:
                h2_idx = h2_structure.index(h2)
                early_bonus = max(0, 0.1 - h2_idx * 0.02)
                score += early_bonus
            
            if score > best_score:
                best_score = score
                best_h2 = h2
        
        if best_h2 and best_score > CONFIG.MIN_RELEVANCE_THRESHOLD:
            assignments[best_h2].append({
                **entity,
                "h2_relevance": round(best_score, 3)
            })
        else:
            # Fallback: przypisz do H2 z najmniejszƒÖ liczbƒÖ encji
            min_h2 = min(h2_structure, key=lambda h: len(assignments[h]))
            assignments[min_h2].append({
                **entity,
                "h2_relevance": 0.1,
                "fallback": True
            })
    
    # Sortuj encje w ka≈ºdym H2 po importance
    for h2 in assignments:
        assignments[h2].sort(key=lambda x: (-x.get("importance", 0), -x.get("h2_relevance", 0)))
    
    return assignments


# ============================================================================
# TRIPLET ASSIGNMENT
# ============================================================================

def assign_triplets_to_h2(
    triplets: List[Dict],
    h2_structure: List[str],
    entity_assignments: Dict[str, List[Dict]],
    domain: str = "prawo"
) -> Dict[str, List[Dict]]:
    """
    Przypisuje triplety do H2 na podstawie encji podmiotu.
    
    Logika: Triplet idzie tam gdzie jest jego SUBJECT entity.
    
    Args:
        triplets: Lista triplet√≥w [{subject, verb, object}]
        h2_structure: Lista tytu≈Ç√≥w H2
        entity_assignments: Wynik assign_entities_to_h2
        domain: Domena
    
    Returns:
        Dict mapping H2 ‚Üí lista triplet√≥w
    """
    assignments = {h2: [] for h2 in h2_structure}
    
    # Zbuduj mapƒô: encja ‚Üí H2
    entity_to_h2 = {}
    for h2, entities in entity_assignments.items():
        for ent in entities:
            ent_name = ent.get("name", "").lower()
            if ent_name:
                entity_to_h2[ent_name] = h2
    
    for triplet in triplets:
        subject = triplet.get("subject", "").lower()
        obj = triplet.get("object", "").lower()
        
        # Szukaj H2 dla podmiotu
        target_h2 = entity_to_h2.get(subject)
        
        if not target_h2:
            # Szukaj czƒô≈õciowego dopasowania
            for ent_name, h2 in entity_to_h2.items():
                if subject in ent_name or ent_name in subject:
                    target_h2 = h2
                    break
        
        if not target_h2:
            # Spr√≥buj po obiekcie
            target_h2 = entity_to_h2.get(obj)
            if not target_h2:
                for ent_name, h2 in entity_to_h2.items():
                    if obj in ent_name or ent_name in obj:
                        target_h2 = h2
                        break
        
        if not target_h2:
            # Semantic similarity jako ostateczno≈õƒá
            best_score = 0
            combined = f"{subject} {triplet.get('verb', '')} {obj}"
            for h2 in h2_structure:
                score = calculate_semantic_similarity(combined, h2, domain)
                if score > best_score:
                    best_score = score
                    target_h2 = h2
        
        if target_h2:
            assignments[target_h2].append({
                **triplet,
                "assigned_by": "subject_entity" if entity_to_h2.get(subject) else "semantic_fallback"
            })
        else:
            # Ostateczny fallback: pierwszy H2
            assignments[h2_structure[0]].append({
                **triplet,
                "assigned_by": "default_fallback"
            })
    
    return assignments


# ============================================================================
# CONTEXT-AWARE EXAMPLE GENERATOR
# ============================================================================

def generate_contextual_example(
    phrase: str,
    h2_title: str,
    assigned_triplets: List[Dict],
    domain: str = "prawo"
) -> str:
    """
    Generuje przyk≈Çadowe zdanie DOPASOWANE do kontekstu H2.
    
    Strategia:
    1. Je≈õli fraza jest w triplecie ‚Üí u≈ºyj tripletu
    2. Je≈õli H2 ma charakterystyczne s≈Çowa ‚Üí dopasuj styl
    3. Fallback: generyczne zdanie z frazƒÖ
    """
    phrase_lower = phrase.lower()
    h2_lower = h2_title.lower()
    
    # 1. Sprawd≈∫ czy fraza jest w kt√≥rym≈õ triplecie
    for triplet in assigned_triplets:
        subj = triplet.get("subject", "").lower()
        obj = triplet.get("object", "").lower()
        verb = triplet.get("verb", "")
        
        if phrase_lower in subj or phrase_lower in obj:
            # Mamy match! U≈ºyj tripletu jako przyk≈Çadu
            return f"{triplet['subject'].capitalize()} {verb} {triplet['object']}."
    
    # 2. Dopasuj do kontekstu H2
    if domain == "prawo":
        if any(w in h2_lower for w in ["procedur", "sƒÖd", "postƒôpowan"]):
            return f"W toku postƒôpowania, {phrase} wymaga szczeg√≥≈Çowej analizy przez sƒÖd."
        
        if any(w in h2_lower for w in ["kar", "przestƒôpst", "odpowiedzialn"]):
            return f"Z perspektywy prawa karnego, {phrase} mo≈ºe prowadziƒá do odpowiedzialno≈õci."
        
        if any(w in h2_lower for w in ["defin", "czym jest", "co to"]):
            return f"{phrase.capitalize()} to termin oznaczajƒÖcy okre≈õlonƒÖ sytuacjƒô prawnƒÖ."
        
        if any(w in h2_lower for w in ["r√≥≈ºnic", "por√≥wnan"]):
            return f"W odr√≥≈ºnieniu od innych pojƒôƒá, {phrase} ma specyficzne znaczenie."
        
        if any(w in h2_lower for w in ["kiedy", "warunek", "przes≈Çank"]):
            return f"O {phrase} m√≥wimy wtedy, gdy spe≈Çnione sƒÖ okre≈õlone przes≈Çanki."
    
    # 3. Fallback
    return f"{phrase.capitalize()} odgrywa istotnƒÖ rolƒô w omawianym kontek≈õcie."


def generate_contextual_short_sentences(
    h2_title: str,
    domain: str = "prawo"
) -> List[str]:
    """
    Generuje REGU≈ÅY tworzenia kr√≥tkich zda≈Ñ (3-8 s≈Ç√≥w) dopasowane do kontekstu H2.
    
    ‚ö†Ô∏è v45.0: Usuniƒôto statyczne zdania ("SƒÖd orzeka.", "Termin biegnie.").
    GPT kopiowa≈Ç je verbatim ‚Üí powtarzalny pattern w setkach artyku≈Ç√≥w.
    
    Teraz zwraca REGU≈ÅY, nie gotowe zdania. GPT tworzy w≈Çasne z materia≈Çu sekcji.
    """
    h2_lower = h2_title.lower()
    
    # Bazowa regu≈Ça ‚Äî zawsze
    rules = [
        f"Kr√≥tkie zdanie MUSI zawieraƒá termin z sekcji \"{h2_title}\"",
    ]
    
    if domain == "prawo":
        if any(w in h2_lower for w in ["sƒÖd", "procedur", "postƒôpowan"]):
            rules.append("Skondensuj kluczowy wym√≥g proceduralny lub termin do 3-5 s≈Ç√≥w")
            rules.append("U≈ºyj nazwy sƒÖdu, terminu lub wymogu z TEGO akapitu")
        
        elif any(w in h2_lower for w in ["kar", "przestƒôpst"]):
            rules.append("Skondensuj konsekwencjƒô prawnƒÖ lub wymiar kary do 3-5 s≈Ç√≥w")
            rules.append("U≈ºyj artyku≈Çu ustawy lub nazwy przestƒôpstwa z TEGO akapitu")
        
        elif any(w in h2_lower for w in ["dziec", "rodzic", "opiek"]):
            rules.append("Skondensuj kluczowy obowiƒÖzek lub prawo do 3-5 s≈Ç√≥w")
            rules.append("U≈ºyj terminu rodzinno-prawnego z TEGO akapitu")
        
        elif any(w in h2_lower for w in ["defin", "czym", "co to"]):
            rules.append("Skondensuj kluczowy element definicji do 3-5 s≈Ç√≥w")
            rules.append("U≈ºyj terminu definiowanego w TEJ sekcji")
        
        else:
            rules.append("WyciƒÖgnij kluczowy fakt prawny z poprzedniego zdania")
    
    elif domain == "medycyna":
        rules.append("U≈ºyj nazwy leku, objawu lub parametru medycznego z TEGO akapitu")
        rules.append("Skondensuj kluczowe zalecenie lub wynik do 3-5 s≈Ç√≥w")
    
    else:
        rules.append("WyciƒÖgnij kluczowy fakt z poprzedniego zdania i skondensuj do 3-5 s≈Ç√≥w")
    
    # Uniwersalna regu≈Ça ko≈Ñcowa
    rules.append("TEST: czy to zdanie pasowa≈Çoby do innego artyku≈Çu? Je≈õli tak ‚Üí przepisz")
    
    return rules


# ============================================================================
# MAIN: GET ASSIGNMENTS FOR BATCH
# ============================================================================

def get_assignments_for_batch(
    keywords_state: Dict,
    s1_data: Dict,
    h2_structure: List[str],
    current_h2: str,
    main_keyword: str,
    domain: str = "prawo"
) -> Dict[str, Any]:
    """
    G≈Ç√≥wna funkcja - zwraca wszystkie przypisania dla konkretnego batcha.
    
    Returns:
        {
            "must_phrases": [...],      # Max 5 fraz MUST dla tego H2
            "should_phrases": [...],    # Opcjonalne frazy
            "must_entities": [...],     # Max 3 encje MUST
            "must_triplets": [...],     # Max 2 triplety MUST
            "short_sentences": [...],   # Kr√≥tkie zdania do H2
            "phrase_examples": {...},   # Przyk≈Çady u≈ºycia fraz
        }
    """
    # 1. Pobierz encje i triplety z S1
    entity_seo = s1_data.get("entity_seo", {})
    entities = entity_seo.get("entities", [])
    triplets = entity_seo.get("entity_relationships", [])
    
    # 2. Przypisz wszystko do H2
    phrase_assignments = assign_phrases_to_h2(
        keywords_state, h2_structure, main_keyword, domain
    )
    
    entity_assignments = assign_entities_to_h2(
        entities, h2_structure, domain
    )
    
    triplet_assignments = assign_triplets_to_h2(
        triplets, h2_structure, entity_assignments, domain
    )
    
    # 3. We≈∫ elementy dla aktualnego H2
    h2_phrases = phrase_assignments.get(current_h2, [])
    h2_entities = entity_assignments.get(current_h2, [])
    h2_triplets = triplet_assignments.get(current_h2, [])
    
    # 4. Podziel na MUST i SHOULD
    # MUST phrases: nieu≈ºyte BASIC (top 5)
    must_phrases = [
        p for p in h2_phrases 
        if p["type"] == "BASIC" and p["actual_uses"] == 0
    ][:CONFIG.MAX_MUST_PHRASES_PER_BATCH]
    
    # Dodaj MAIN je≈õli jest przypisany tu i nieu≈ºyty wystarczajƒÖco
    main_phrases = [p for p in h2_phrases if p.get("is_main")]
    for mp in main_phrases:
        if mp["actual_uses"] < mp["target_min"] and mp not in must_phrases:
            must_phrases.insert(0, mp)
    
    must_phrases = must_phrases[:CONFIG.MAX_MUST_PHRASES_PER_BATCH]
    
    # SHOULD phrases: reszta
    should_phrases = [p for p in h2_phrases if p not in must_phrases][:5]
    
    # MUST entities: importance >= 0.7 (top 3)
    must_entities = [
        e for e in h2_entities 
        if e.get("importance", 0) >= 0.7
    ][:CONFIG.MAX_MUST_ENTITIES_PER_BATCH]
    
    # MUST triplets: top 2
    must_triplets = h2_triplets[:CONFIG.MAX_MUST_TRIPLETS_PER_BATCH]
    
    # 5. Generuj przyk≈Çady
    phrase_examples = {}
    for p in must_phrases + should_phrases[:3]:
        phrase_examples[p["keyword"]] = generate_contextual_example(
            p["keyword"], current_h2, h2_triplets, domain
        )
    
    # 6. Generuj kr√≥tkie zdania
    short_sentences = generate_contextual_short_sentences(current_h2, domain)
    
    return {
        "current_h2": current_h2,
        "must_phrases": must_phrases,
        "should_phrases": should_phrases,
        "must_entities": must_entities,
        "must_triplets": must_triplets,
        "short_sentences": short_sentences,
        "phrase_examples": phrase_examples,
        "stats": {
            "total_phrases_for_h2": len(h2_phrases),
            "total_entities_for_h2": len(h2_entities),
            "total_triplets_for_h2": len(h2_triplets),
            "must_count": len(must_phrases) + len(must_entities) + len(must_triplets)
        }
    }


# ============================================================================
# TEST
# ============================================================================

if __name__ == "__main__":
    # Test data
    keywords_state = {
        "k1": {"keyword": "porwanie rodzicielskie", "type": "MAIN", "actual_uses": 3, "target_min": 10, "is_main_keyword": True},
        "k2": {"keyword": "sƒÖd rodzinny", "type": "BASIC", "actual_uses": 0, "target_min": 3},
        "k3": {"keyword": "ustalenie miejsca pobytu dziecka", "type": "BASIC", "actual_uses": 0, "target_min": 2},
        "k4": {"keyword": "uprowadzenie dziecka", "type": "BASIC", "actual_uses": 0, "target_min": 1},
        "k5": {"keyword": "art. 211 kodeksu karnego", "type": "EXTENDED", "actual_uses": 0, "target_min": 1},
        "k6": {"keyword": "w≈Çadza rodzicielska", "type": "BASIC", "actual_uses": 1, "target_min": 3},
        "k7": {"keyword": "odpowiedzialno≈õƒá karna", "type": "EXTENDED", "actual_uses": 0, "target_min": 1},
        "k8": {"keyword": "Konwencja haska", "type": "EXTENDED", "actual_uses": 0, "target_min": 1},
    }
    
    h2_structure = [
        "Czym jest porwanie rodzicielskie ‚Äì definicja",
        "R√≥≈ºnica miƒôdzy porwaniem rodzicielskim a uprowadzeniem dziecka",
        "Procedura sƒÖdowa w sprawach o miejsce pobytu dziecka",
        "Kiedy porwanie rodzicielskie jest przestƒôpstwem"
    ]
    
    s1_data = {
        "entity_seo": {
            "entities": [
                {"name": "sƒÖd rodzinny", "importance": 0.85, "sources_count": 6},
                {"name": "Kodeks karny", "importance": 0.75, "sources_count": 4},
                {"name": "Konwencja haska", "importance": 0.70, "sources_count": 3},
            ],
            "entity_relationships": [
                {"subject": "sƒÖd rodzinny", "verb": "ustala", "object": "miejsce pobytu dziecka"},
                {"subject": "rodzic", "verb": "narusza", "object": "prawa drugiego rodzica"},
            ]
        }
    }
    
    print("=" * 60)
    print("TEST: SEMANTIC PHRASE ASSIGNMENT")
    print("=" * 60)
    
    # Test assign_phrases_to_h2
    phrase_assignments = assign_phrases_to_h2(keywords_state, h2_structure, "porwanie rodzicielskie")
    
    print("\nüìù PHRASE ASSIGNMENTS:")
    for h2, phrases in phrase_assignments.items():
        print(f"\n  H2: {h2}")
        for p in phrases[:3]:
            print(f"    ‚Ä¢ {p['keyword']} ({p['type']}) - relevance: {p['relevance']}")
    
    # Test full batch assignment
    print("\n" + "=" * 60)
    print("TEST: FULL BATCH ASSIGNMENT")
    print("=" * 60)
    
    for h2 in h2_structure[:2]:
        result = get_assignments_for_batch(
            keywords_state=keywords_state,
            s1_data=s1_data,
            h2_structure=h2_structure,
            current_h2=h2,
            main_keyword="porwanie rodzicielskie"
        )
        
        print(f"\nüìå H2: {h2}")
        print(f"\n  MUST PHRASES ({len(result['must_phrases'])}):")
        for p in result['must_phrases']:
            print(f"    ‚Ä¢ {p['keyword']}")
            if p['keyword'] in result['phrase_examples']:
                print(f"      Przyk≈Çad: {result['phrase_examples'][p['keyword']]}")
        
        print(f"\n  MUST ENTITIES ({len(result['must_entities'])}):")
        for e in result['must_entities']:
            print(f"    ‚Ä¢ {e['name']} (importance: {e.get('importance', 'N/A')})")
        
        print(f"\n  MUST TRIPLETS ({len(result['must_triplets'])}):")
        for t in result['must_triplets']:
            print(f"    ‚Ä¢ {t['subject']} ‚Üí {t['verb']} ‚Üí {t['object']}")
        
        print(f"\n  SHORT SENTENCES: {', '.join(result['short_sentences'][:3])}")
        print(f"\n  STATS: {result['stats']}")
