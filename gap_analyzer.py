"""
===============================================================================
ğŸ“Š GAP ANALYZER v1.0 â€” Information Gain dla S1
===============================================================================
Identyfikuje tematy, ktÃ³rych NIKT w top 10 nie pokrywa,
ale uÅ¼ytkownicy o nie pytajÄ… (PAA, related searches).

3 typy gapÃ³w:
1. PAA_UNANSWERED â€” pytania PAA, na ktÃ³re nikt nie odpowiada
2. SUBTOPIC_MISSING â€” related searches nieobecne w H2 konkurencji
3. DEPTH_MISSING â€” sekcje u wszystkich, ale pÅ‚ytkie (<120 sÅ‚Ã³w)

Integracja: index.py â†’ po entity_seo â†’ dodaje "content_gaps" do response

Autor: BRAJEN Team
Data: 2025
===============================================================================
"""

import re
from typing import List, Dict, Set, Optional
from collections import Counter
from dataclasses import dataclass, asdict


# ================================================================
# ğŸ“¦ STRUKTURY DANYCH
# ================================================================

@dataclass
class ContentGap:
    """Pojedynczy zidentyfikowany gap w treÅ›ci konkurencji."""
    topic: str
    gap_type: str           # "paa_unanswered", "subtopic_missing", "depth_missing"
    evidence: str           # dlaczego to gap
    priority: str           # "high", "medium", "low"
    suggested_h2: str       # propozycja H2

    def to_dict(self) -> Dict:
        return asdict(self)


# ================================================================
# ğŸ§¹ HELPERY
# ================================================================

# Polskie stop words (rozszerzone)
_STOP_WORDS_PL = {
    "i", "w", "na", "z", "do", "Å¼e", "siÄ™", "nie", "to", "jest", "za", "po",
    "od", "o", "jak", "ale", "co", "ten", "tym", "byÄ‡", "moÅ¼e", "juÅ¼", "tak",
    "gdy", "lub", "czy", "tego", "tej", "sÄ…", "dla", "ich", "przez", "jako",
    "te", "ze", "tych", "byÅ‚o", "ma", "przy", "tym", "ktÃ³re", "ktÃ³ry", "ktÃ³ra",
    "ktÃ³rych", "jego", "jej", "tego", "takÅ¼e", "wiÄ™c", "tylko", "teÅ¼", "sobie",
    "bardzo", "jeszcze", "wszystko", "przed", "miÄ™dzy", "pod", "nad", "bez",
    "oraz", "gdzie", "kiedy", "ile", "jeÅ›li", "jaki", "jaka", "jakie",
}


def _extract_content_words(text: str, min_len: int = 4) -> Set[str]:
    """WyciÄ…ga znaczÄ…ce sÅ‚owa z tekstu (bez stop words)."""
    words = set(re.findall(r'\b[a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]{' + str(min_len) + r',}\b', text.lower()))
    return words - _STOP_WORDS_PL


def _normalize_h2(h2: str) -> str:
    """Normalizuje H2 do porÃ³wnaÅ„."""
    return re.sub(r'[^a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼0-9\s]', '', h2.lower()).strip()


def _words_overlap(words_a: Set[str], words_b: Set[str], threshold: int = 2) -> bool:
    """Sprawdza czy dwa zestawy sÅ‚Ã³w majÄ… wystarczajÄ…cy overlap."""
    return len(words_a & words_b) >= threshold


# ================================================================
# ğŸ“Š GÅÃ“WNA ANALIZA
# ================================================================

def analyze_content_gaps(
    competitor_texts: List[str],
    competitor_h2s: List[str],
    paa_questions: List[Dict],
    related_searches: List[str],
    main_keyword: str,
    max_gaps: int = 15
) -> Dict:
    """
    Identyfikuje 3 typy gapÃ³w w treÅ›ci konkurencji.

    Args:
        competitor_texts: PeÅ‚ne treÅ›ci stron z top 10
        competitor_h2s: Wszystkie H2 ze stron konkurencji
        paa_questions: Pytania People Also Ask (list of dicts z "question")
        related_searches: Related searches z Google
        main_keyword: GÅ‚Ã³wna fraza kluczowa
        max_gaps: Max liczba gapÃ³w do zwrÃ³cenia

    Returns:
        Dict z content_gaps + agent_instruction
    """
    # Przygotuj dane
    combined_competitor = " ".join(t.lower() for t in competitor_texts if t)[:500000]
    
    # Normalizuj H2 konkurencji
    competitor_h2_normalized = set()
    competitor_h2_words = []
    for h2 in competitor_h2s:
        norm = _normalize_h2(h2)
        if norm:
            competitor_h2_normalized.add(norm)
            competitor_h2_words.append(_extract_content_words(h2))

    gaps = []

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. PAA UNANSWERED
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    for paa in paa_questions:
        question = paa.get("question", "") if isinstance(paa, dict) else str(paa)
        if not question or len(question) < 10:
            continue

        question_words = _extract_content_words(question)
        if len(question_words) < 2:
            continue

        # Czy jakiÅ› H2 konkurencji pokrywa to pytanie?
        covered_by_h2 = any(
            _words_overlap(question_words, h2w)
            for h2w in competitor_h2_words
        )

        # Czy treÅ›Ä‡ konkurencji odpowiada na pytanie?
        # (sprawdzamy ile kluczowych sÅ‚Ã³w z pytania pojawia siÄ™ w treÅ›ciach)
        words_in_content = sum(1 for w in question_words if w in combined_competitor)
        content_coverage = words_in_content / max(1, len(question_words))

        if not covered_by_h2 and content_coverage < 0.6:
            # Nikt nie pokrywa tego pytania
            gaps.append(ContentGap(
                topic=question,
                gap_type="paa_unanswered",
                evidence=f"PAA pytanie niepokryte: {words_in_content}/{len(question_words)} "
                         f"sÅ‚Ã³w kluczowych w treÅ›ciach, brak H2",
                priority="high",
                suggested_h2=_question_to_h2(question)
            ))
        elif not covered_by_h2 and content_coverage < 0.8:
            # TreÅ›Ä‡ czÄ™Å›ciowo pokrywa, ale brak dedykowanego H2
            gaps.append(ContentGap(
                topic=question,
                gap_type="paa_unanswered",
                evidence=f"PAA pytanie bez dedykowanego H2 ({content_coverage:.0%} pokrycia w treÅ›ci)",
                priority="medium",
                suggested_h2=_question_to_h2(question)
            ))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2. SUBTOPIC MISSING
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    for search in related_searches:
        if not search or len(search) < 5:
            continue
            
        search_words = _extract_content_words(search)
        if len(search_words) < 2:
            continue

        # Czy jakiÅ› H2 pokrywa ten related search?
        covered_by_h2 = any(
            _words_overlap(search_words, h2w)
            for h2w in competitor_h2_words
        )

        if not covered_by_h2:
            # SprawdÅº czy to nie duplikat main_keyword
            main_kw_words = _extract_content_words(main_keyword)
            if search_words == main_kw_words:
                continue

            gaps.append(ContentGap(
                topic=search,
                gap_type="subtopic_missing",
                evidence=f"Related search '{search}' nieobecny w H2 konkurencji",
                priority="medium",
                suggested_h2=search.strip().capitalize()
            ))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3. DEPTH MISSING â€” pÅ‚ytkie sekcje
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Policz H2 pojawiajÄ…ce siÄ™ u wielu konkurentÃ³w
    h2_counter = Counter(_normalize_h2(h) for h in competitor_h2s if h.strip())
    common_h2s = [h2 for h2, count in h2_counter.items() if count >= 3 and h2]

    for h2_norm in common_h2s:
        # Szacuj Å›redniÄ… dÅ‚ugoÅ›Ä‡ sekcji pod tym H2 u konkurencji
        section_lengths = _estimate_section_lengths(competitor_texts, h2_norm)

        if section_lengths:
            avg_words = sum(section_lengths) / len(section_lengths)
            if avg_words < 120:
                # Znaleziono H2 w oryginale (nienormalizowany)
                original_h2 = _find_original_h2(competitor_h2s, h2_norm)
                
                gaps.append(ContentGap(
                    topic=original_h2 or h2_norm,
                    gap_type="depth_missing",
                    evidence=f"Sekcja '{original_h2 or h2_norm}' u konkurencji ma Å›r. "
                             f"{int(avg_words)} sÅ‚Ã³w â€” moÅ¼na rozbudowaÄ‡",
                    priority="medium" if avg_words > 80 else "high",
                    suggested_h2=original_h2 or h2_norm.capitalize()
                ))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DEDUPLIKACJA I SORTOWANIE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    gaps = _deduplicate_gaps(gaps)
    
    # Sortuj: high > medium > low
    priority_order = {"high": 0, "medium": 1, "low": 2}
    gaps.sort(key=lambda g: priority_order.get(g.priority, 2))
    gaps = gaps[:max_gaps]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FORMATOWANIE DLA AGENTA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    agent_instruction = _format_gaps_for_agent(gaps, main_keyword)

    return {
        "total_gaps": len(gaps),
        "paa_unanswered": [g.to_dict() for g in gaps if g.gap_type == "paa_unanswered"],
        "subtopic_missing": [g.to_dict() for g in gaps if g.gap_type == "subtopic_missing"],
        "depth_missing": [g.to_dict() for g in gaps if g.gap_type == "depth_missing"],
        "all_gaps": [g.to_dict() for g in gaps],
        "suggested_new_h2s": [
            g.suggested_h2 for g in gaps 
            if g.priority == "high" and g.gap_type in ("paa_unanswered", "subtopic_missing")
        ][:3],
        "agent_instruction": agent_instruction,
        "status": "OK"
    }


# ================================================================
# ğŸ”§ HELPER FUNCTIONS
# ================================================================

def _question_to_h2(question: str) -> str:
    """Konwertuje pytanie PAA na propozycjÄ™ H2."""
    # UsuÅ„ znaki zapytania i capitalize
    h2 = question.rstrip("?").strip()
    # JeÅ›li zaczyna siÄ™ od "Czy ", zmieÅ„ na stwierdzenie
    if h2.lower().startswith("czy "):
        h2 = h2[4:]
    # Capitalize first letter
    if h2:
        h2 = h2[0].upper() + h2[1:]
    return h2


def _estimate_section_lengths(
    competitor_texts: List[str],
    h2_normalized: str
) -> List[int]:
    """
    Szacuje dÅ‚ugoÅ›Ä‡ sekcji pod danym H2 u konkurencji.
    Proste podejÅ›cie: znajdÅº H2 â†’ policz sÅ‚owa do nastÄ™pnego H2.
    """
    lengths = []
    h2_words = set(h2_normalized.split())
    
    for text in competitor_texts:
        if not text:
            continue
        text_lower = text.lower()
        
        # ZnajdÅº pozycjÄ™ H2 (przybliÅ¼one â€” szukamy sÅ‚Ã³w z H2)
        # Szukamy linii, ktÃ³ra wyglÄ…da jak H2 i zawiera nasze sÅ‚owa
        lines = text_lower.split('\n')
        found_pos = -1
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            if len(line_stripped) < 100:  # H2 raczej krÃ³tkie
                line_words = set(re.findall(r'\b[a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼]{3,}\b', line_stripped))
                if len(h2_words & line_words) >= max(2, len(h2_words) - 1):
                    found_pos = i
                    break
        
        if found_pos == -1:
            continue
        
        # Policz sÅ‚owa do nastÄ™pnego "H2-like" nagÅ‚Ã³wka
        section_text = []
        for j in range(found_pos + 1, min(found_pos + 50, len(lines))):
            line = lines[j].strip()
            # Heurystyka: krÃ³tka linia (<80 znakÃ³w) po pustej linii = prawdopodobnie H2
            if (len(line) < 80 and len(line) > 5 and 
                j > found_pos + 2 and not lines[j-1].strip()):
                break
            section_text.append(line)
        
        word_count = len(" ".join(section_text).split())
        if word_count > 10:  # Ignoruj bardzo krÃ³tkie (prawdopodobnie bÅ‚Ä™dne matche)
            lengths.append(word_count)
    
    return lengths


def _find_original_h2(h2_list: List[str], normalized: str) -> Optional[str]:
    """Znajduje oryginalny (nienormalizowany) H2."""
    for h2 in h2_list:
        if _normalize_h2(h2) == normalized:
            return h2
    return None


def _deduplicate_gaps(gaps: List[ContentGap]) -> List[ContentGap]:
    """Usuwa duplikaty na podstawie podobieÅ„stwa tematu."""
    seen_topics = set()
    unique = []
    
    for gap in gaps:
        # Klucz dedup: pierwsze 3 znaczÄ…ce sÅ‚owa
        topic_words = sorted(_extract_content_words(gap.topic))[:3]
        key = "|".join(topic_words)
        
        if key not in seen_topics and key:
            seen_topics.add(key)
            unique.append(gap)
    
    return unique


def _format_gaps_for_agent(gaps: List[ContentGap], main_keyword: str) -> str:
    """Formatuje gapy jako instrukcjÄ™ dla agenta GPT â€” v45.1 aggressive Information Gain."""
    if not gaps:
        return ""

    lines = [
        f"ğŸ† TWOJA PRZEWAGA NAD KONKURENCJÄ„ â€” \"{main_keyword}\":",
        "=" * 50,
        "Å»aden z TOP 10 wynikÃ³w w Google NIE pokrywa tych tematÃ³w.",
        "To Twoja UNIKALNA szansa na Information Gain â€” Google nagradza",
        "artykuÅ‚y wnoszÄ…ce NOWÄ„ informacjÄ™ vs to co juÅ¼ jest w SERP.",
        ""
    ]

    high_gaps = [g for g in gaps if g.priority == "high"]
    medium_gaps = [g for g in gaps if g.priority == "medium"]

    if high_gaps:
        lines.append("ğŸ”´ OBOWIÄ„ZKOWE â€” napisz min 2-3 zdania o KAÅ»DYM:")
        for g in high_gaps[:5]:
            gap_type_label = {
                "paa_unanswered": "PAA â€” uÅ¼ytkownicy pytajÄ…, nikt nie odpowiada",
                "subtopic_missing": "TEMAT â€” konkurencja go pomija",
                "depth_missing": "GÅÄ˜BIA â€” wszyscy piszÄ… ogÃ³lnikowo"
            }.get(g.gap_type, g.gap_type)
            lines.append(f"  â–¶ {g.topic}")
            lines.append(f"    [{gap_type_label}]")
            if g.suggested_h2:
                lines.append(f"    â†’ Proponowany H2: \"{g.suggested_h2}\"")
        lines.append("")

    if medium_gaps:
        lines.append("ğŸŸ¡ WPLEÄ† w istniejÄ…ce sekcje (min 1 zdanie kaÅ¼dy):")
        for g in medium_gaps[:5]:
            lines.append(f"  â–¶ {g.topic}")
            if g.evidence:
                lines.append(f"    (dowÃ³d: {g.evidence[:80]})")
        lines.append("")

    lines.append("âš ï¸ ZASADA: Skup siÄ™ na tych gapach BARDZIEJ niÅ¼ na powtarzaniu")
    lines.append("tego co wszyscy piszÄ…. Powtarzanie = zero wartoÅ›ci dla Google.")
    lines.append("Unikalne treÅ›ci = wyÅ¼szy ranking. To proste.")
    lines.append("=" * 50)

    return "\n".join(lines)


# ================================================================
# EXPORTS
# ================================================================

__all__ = [
    'analyze_content_gaps',
    'ContentGap',
]
