import uuid
import re
import os
import json
import math
import spacy
from flask import Blueprint, request, jsonify
from firebase_admin import firestore
from firestore_tracker_routes import process_batch_in_firestore
import google.generativeai as genai
from seo_optimizer import unified_prevalidation

# Gemini API configuration
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
else:
    print("[WARNING] ‚ö†Ô∏è GEMINI_API_KEY not set - LSI enrichment fallback mode")

# spaCy model
try:
    nlp = spacy.load("pl_core_news_md")
    print("[INIT] ‚úÖ spaCy pl_core_news_md loaded")
except OSError:
    from spacy.cli import download
    print("‚ö†Ô∏è Downloading pl_core_news_md fallback...")
    download("pl_core_news_md")
    nlp = spacy.load("pl_core_news_md")

project_routes = Blueprint("project_routes", __name__)

# ‚≠ê GEMINI MODEL - centralnie zdefiniowany
GEMINI_MODEL = "gemini-2.5-flash"


# ================================================================
# üß† H2 SUGGESTIONS (Gemini-powered)
# ================================================================
@project_routes.post("/api/project/s1_h2_suggestions")
def generate_h2_suggestions():
    """
    Generuje sugestie H2 u≈ºywajƒÖc Gemini na podstawie:
    - topic/main_keyword
    - wzorc√≥w H2 z konkurencji (serp_h2_patterns)
    - target keywords
    
    Zwraca listƒô maksymalnie 6 H2 (hard limit zgodny z seo_rules.json).
    Wstƒôp (intro) NIE jest H2 - to osobny element bez nag≈Ç√≥wka.
    
    ‚ö†Ô∏è WA≈ªNE: To sƒÖ tylko PROPOZYCJE. User musi podaƒá SWOJE H2,
    kt√≥re zostanƒÖ po≈ÇƒÖczone z propozycjami w finalnƒÖ strukturƒô.
    """
    data = request.get_json()
    if not data:
        return jsonify({"error": "No JSON data provided"}), 400
    
    topic = data.get("topic") or data.get("main_keyword", "")
    if not topic:
        return jsonify({"error": "Required: topic or main_keyword"}), 400
    
    serp_h2_patterns = data.get("serp_h2_patterns", [])
    target_keywords = data.get("target_keywords", [])
    target_count = min(data.get("target_count", 6), 6)
    
    # Je≈õli brak Gemini API - zwr√≥ƒá podstawowe sugestie
    if not GEMINI_API_KEY:
        fallback_suggestions = [
            f"Czym jest {topic}?",
            f"Jak dzia≈Ça {topic}?",
            f"Korzy≈õci z {topic}",
            f"Kiedy warto skorzystaƒá z {topic}?",
            f"Ile kosztuje {topic}?",
            f"Najczƒôstsze pytania o {topic}"
        ]
        return jsonify({
            "status": "FALLBACK",
            "suggestions": fallback_suggestions[:target_count],
            "message": "Gemini unavailable - basic suggestions generated",
            "model": "fallback",
            "action_required": "USER_H2_INPUT_NEEDED"
        }), 200
    
    try:
        model = genai.GenerativeModel(GEMINI_MODEL)
        
        competitor_context = ""
        if serp_h2_patterns:
            competitor_context = f"""
WZORCE H2 Z KONKURENCJI (TOP 10 SERP):
{chr(10).join(f"- {h2}" for h2 in serp_h2_patterns[:20])}
"""
        
        keywords_context = ""
        if target_keywords:
            keywords_context = f"""
FRAZY KLUCZOWE DO WPLECENIA W H2:
{', '.join(target_keywords[:10])}
"""
        
        prompt = f"""
Wygeneruj DOK≈ÅADNIE {target_count} nag≈Ç√≥wk√≥w H2 dla artyku≈Çu SEO o temacie: "{topic}"

WA≈ªNE: Artyku≈Ç bƒôdzie mia≈Ç WSTƒòP (bez nag≈Ç√≥wka H2) + {target_count} sekcji H2.

{competitor_context}
{keywords_context}

ZASADY:
1. Wygeneruj DOK≈ÅADNIE {target_count} H2 (nie wiƒôcej, nie mniej)
2. Ka≈ºdy H2 powinien mieƒá 6-12 s≈Ç√≥w
3. Minimum 50% H2 powinno byƒá w formie pytania (Jak...?, Dlaczego...?, Kiedy...?, Co...?)
4. Maksimum 30% H2 mo≈ºe zawieraƒá g≈Ç√≥wne s≈Çowo kluczowe
5. NIE u≈ºywaj og√≥lnikowych tytu≈Ç√≥w jak: "Wstƒôp", "Podsumowanie", "Zako≈Ñczenie", "FAQ"
6. H2 powinny tworzyƒá logicznƒÖ strukturƒô artyku≈Çu
7. Uwzglƒôdnij intencjƒô wyszukiwania (informacyjna/transakcyjna)
8. Wpleƒá naturalnie frazy kluczowe gdzie to mo≈ºliwe

FORMAT ODPOWIEDZI:
Zwr√≥ƒá TYLKO listƒô {target_count} H2, ka≈ºdy w nowej linii, bez numeracji ani punktor√≥w.
"""
        
        print(f"[H2_SUGGESTIONS] Generating {target_count} H2 for: {topic}")
        response = model.generate_content(prompt)
        
        raw_suggestions = response.text.strip().split('\n')
        suggestions = [
            h2.strip().lstrip('‚Ä¢-‚Äì‚Äî0123456789.). ')
            for h2 in raw_suggestions 
            if h2.strip() and len(h2.strip()) > 5
        ][:target_count]
        
        print(f"[H2_SUGGESTIONS] ‚úÖ Generated {len(suggestions)} H2 suggestions")
        
        return jsonify({
            "status": "OK",
            "suggestions": suggestions,
            "topic": topic,
            "model": GEMINI_MODEL,
            "count": len(suggestions),
            "action_required": "USER_H2_INPUT_NEEDED",
            "message": "To sƒÖ PROPOZYCJE. Teraz podaj SWOJE H2, kt√≥re chcesz wple≈õƒá, a system po≈ÇƒÖczy je w finalnƒÖ strukturƒô."
        }), 200
        
    except Exception as e:
        print(f"[H2_SUGGESTIONS] ‚ùå Error: {e}")
        return jsonify({
            "status": "ERROR",
            "error": str(e),
            "suggestions": []
        }), 500


# ================================================================
# üß± PROJECT CREATE
# ================================================================
@project_routes.post("/api/project/create")
def create_project():
    """
    Tworzy nowy projekt SEO w Firestore.
    
    ‚≠ê NOWA LOGIKA LIMIT√ìW:
    - GPT widzi: target_min + 1 (ostrzegawczy limit)
    - Backend liczy do: target_max (rzeczywisty limit)
    """
    data = request.get_json()
    if not data:
        return jsonify({"error": "No JSON data provided"}), 400
    
    topic = data.get("topic") or data.get("main_keyword", "").strip()
    if not topic:
        return jsonify({"error": "Required field: topic or main_keyword"}), 400
    
    h2_structure = data.get("h2_structure", [])
    raw_keywords = data.get("keywords_list") or data.get("keywords", [])
    target_length = data.get("target_length", 3000)
    source = data.get("source", "unknown")
    
    # ‚≠ê NOWE: Liczba planowanych batch√≥w (domy≈õlnie = liczba H2 / 2, min 2, max 6)
    total_planned_batches = data.get("total_planned_batches")
    if not total_planned_batches:
        total_planned_batches = max(2, min(6, math.ceil(len(h2_structure) / 2))) if h2_structure else 4

    firestore_keywords = {}
    for item in raw_keywords:
        term = item.get("term") or item.get("keyword", "")
        term = term.strip() if term else ""
        
        if not term:
            continue
        
        doc = nlp(term)
        search_lemma = " ".join(t.lemma_.lower() for t in doc if t.is_alpha)
        
        min_val = item.get("min") or item.get("target_min", 1)
        max_val = item.get("max") or item.get("target_max", 5)
        
        row_id = item.get("id") or str(uuid.uuid4())
        
        firestore_keywords[row_id] = {
            "keyword": term,
            "search_term_exact": term.lower(),
            "search_lemma": search_lemma,
            "target_min": min_val,
            "target_max": max_val,
            # ‚≠ê NOWE: display_limit to co widzi GPT (min+1)
            "display_limit": min_val + 1,
            "actual_uses": 0,
            "status": "UNDER",
            "type": item.get("type", "BASIC").upper(),
            "remaining_max": max_val,
            "optimal_target": max_val
        }

    db = firestore.client()
    doc_ref = db.collection("seo_projects").document()
    project_data = {
        "topic": topic,
        "h2_structure": h2_structure,
        "keywords_state": firestore_keywords,
        "created_at": firestore.SERVER_TIMESTAMP,
        "batches": [],
        "batches_plan": [],
        "total_batches": 0,
        "total_planned_batches": total_planned_batches,  # ‚≠ê NOWE
        "target_length": target_length,
        "source": source,
        "version": "v22.1",
        "manual_mode": False if source == "n8n-brajen-workflow" else True,
        # ‚≠ê NOWE: format output
        "output_format": "clean_text_with_headers"
    }
    doc_ref.set(project_data)
    
    print(f"[PROJECT] ‚úÖ Created project {doc_ref.id}: {topic} ({len(firestore_keywords)} keywords, {total_planned_batches} planned batches)")

    return jsonify({
        "status": "CREATED",
        "project_id": doc_ref.id,
        "topic": topic,
        "keywords_count": len(firestore_keywords),
        "keywords": len(firestore_keywords),
        "h2_sections": len(h2_structure),
        "total_planned_batches": total_planned_batches,  # ‚≠ê NOWE
        "target_length": target_length,
        "source": source
    }), 201


# ================================================================
# üìä GET PROJECT STATUS - z info o LOCKED frazach
# ================================================================
@project_routes.get("/api/project/<project_id>/status")
def get_project_status(project_id):
    """
    Zwraca aktualny status projektu z informacjƒÖ o LOCKED frazach.
    
    ‚≠ê NOWE:
    - locked_keywords: lista fraz kt√≥re osiƒÖgnƒô≈Çy limit
    - display_limits: limity kt√≥re widzi GPT (min+1)
    """
    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404
    
    data = doc.to_dict()
    keywords_state = data.get("keywords_state", {})
    batches = data.get("batches", [])
    
    keyword_summary = []
    locked_keywords = []
    near_limit_keywords = []
    
    for rid, meta in keywords_state.items():
        actual = meta.get("actual_uses", 0)
        target_min = meta.get("target_min", 0)
        target_max = meta.get("target_max", 999)
        remaining = max(0, target_max - actual)
        display_limit = target_min + 1  # Co widzi GPT
        
        kw_info = {
            "keyword": meta.get("keyword"),
            "type": meta.get("type", "BASIC"),
            "actual": actual,
            "display_limit": display_limit,  # ‚≠ê GPT widzi to
            "target_max": target_max,  # Backend limit
            "status": meta.get("status"),
            "remaining_max": remaining
        }
        keyword_summary.append(kw_info)
        
        # ‚≠ê Zbierz LOCKED frazy
        if remaining == 0:
            locked_keywords.append({
                "keyword": meta.get("keyword"),
                "message": f"üîí LOCKED: '{meta.get('keyword')}' osiƒÖgnƒô≈Ço limit {target_max}x. U≈ºyj SYNONIM√ìW!"
            })
        elif remaining <= 3:
            near_limit_keywords.append({
                "keyword": meta.get("keyword"),
                "remaining": remaining,
                "message": f"‚ö†Ô∏è NEAR LIMIT: '{meta.get('keyword')}' - zosta≈Ço tylko {remaining}x"
            })
    
    return jsonify({
        "project_id": project_id,
        "topic": data.get("topic"),
        "total_batches": len(batches),
        "keywords_count": len(keywords_state),
        "keywords": keyword_summary,
        # ‚≠ê NOWE - wyra≈∫ne info o blokadach
        "locked_keywords": locked_keywords,
        "near_limit_keywords": near_limit_keywords,
        "warnings_before_batch": locked_keywords + near_limit_keywords,
        "source": data.get("source", "unknown"),
        "has_final_review": "final_review" in data
    }), 200


# ================================================================
# üìã PRE-BATCH INFO - PE≈ÅNA OPTYMALIZACJA TARGET=MAX
# ================================================================
@project_routes.get("/api/project/<project_id>/pre_batch_info")
def get_pre_batch_info(project_id):
    """
    Zwraca PE≈ÅNY PLAN przed napisaniem batcha:
    - Rozk≈Çad fraz w batchach (A)
    - Priorytetyzacja UNDER (B)
    - Automatyczne balansowanie (C)
    - Smart display_limit (D)
    - Max per batch (E) - zapobiega przeoptymalizacji
    """
    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404
    
    data = doc.to_dict()
    keywords_state = data.get("keywords_state", {})
    batches = data.get("batches", [])
    h2_structure = data.get("h2_structure", [])
    total_planned_batches = data.get("total_planned_batches", 4)  # Domy≈õlnie 4
    
    current_batch_num = len(batches) + 1
    remaining_batches = max(1, total_planned_batches - len(batches))
    
    # ================================================================
    # üìä ANALIZA FRAZ Z PE≈ÅNYM PLANEM
    # ================================================================
    keyword_plan = []
    critical_keywords = []  # UNDER + ostatni batch
    high_priority = []      # UNDER
    normal_keywords = []    # OK, mo≈ºna u≈ºyƒá
    low_priority = []       # Ju≈º >= min
    locked_keywords = []    # = max, nie u≈ºywaj
    exceeded_keywords = []  # > max
    
    for rid, meta in keywords_state.items():
        keyword = meta.get("keyword")
        kw_type = meta.get("type", "BASIC")
        actual = meta.get("actual_uses", 0)
        target_min = meta.get("target_min", 0)
        target_max = meta.get("target_max", 999)
        
        remaining_to_max = max(0, target_max - actual)
        remaining_to_min = max(0, target_min - actual)
        
        # ‚≠ê SMART CALCULATIONS
        # Max per batch = r√≥wnomierny rozk≈Çad (zapobiega stuffing)
        max_per_batch = max(1, math.ceil(target_max / total_planned_batches))
        
        # Suggested = ile u≈ºyƒá w tym batchu dla r√≥wnomiernego rozk≈Çadu
        if remaining_to_max > 0 and remaining_batches > 0:
            suggested = min(
                math.ceil(remaining_to_max / remaining_batches),
                max_per_batch
            )
        else:
            suggested = 0
        
        # Je≈õli UNDER - zwiƒôksz suggested ≈ºeby nadrobiƒá
        if remaining_to_min > 0:
            min_needed_per_batch = math.ceil(remaining_to_min / remaining_batches)
            suggested = max(suggested, min_needed_per_batch)
            # Ale nie wiƒôcej ni≈º max_per_batch
            suggested = min(suggested, max_per_batch + 1)  # +1 dla UNDER
        
        # ‚≠ê PRIORYTET
        if actual > target_max:
            priority = "EXCEEDED"
            reason = f"‚ùå Ju≈º {actual}x (max {target_max}x) - NIE U≈ªYWAJ!"
            suggested = 0
        elif remaining_to_max == 0:
            priority = "LOCKED"
            reason = f"üîí OsiƒÖgniƒôto max {target_max}x - u≈ºyj SYNONIM√ìW"
            suggested = 0
        elif remaining_to_min > 0 and remaining_batches == 1:
            priority = "CRITICAL"
            reason = f"üî¥ OSTATNI BATCH! Potrzeba {remaining_to_min}x do min!"
            suggested = remaining_to_min
        elif remaining_to_min > 0:
            priority = "HIGH"
            reason = f"üü† UNDER - brakuje {remaining_to_min}x (cel: {target_min}x)"
        elif actual >= target_min and remaining_to_max > 0:
            priority = "NORMAL"
            reason = f"üü¢ OK ({actual}/{target_min}-{target_max}) - sugerowane {suggested}x"
        else:
            priority = "LOW"
            reason = f"‚ö™ Wystarczy ({actual}x >= min {target_min}x)"
            suggested = 0
        
        kw_info = {
            "keyword": keyword,
            "type": kw_type,
            "priority": priority,
            "actual": actual,
            "target_min": target_min,
            "target_max": target_max,
            "remaining_to_min": remaining_to_min,
            "remaining_to_max": remaining_to_max,
            "max_per_batch": max_per_batch,
            "suggested": suggested,
            "reason": reason
        }
        
        keyword_plan.append(kw_info)
        
        # Kategoryzuj
        if priority == "EXCEEDED":
            exceeded_keywords.append(kw_info)
        elif priority == "LOCKED":
            locked_keywords.append(kw_info)
        elif priority == "CRITICAL":
            critical_keywords.append(kw_info)
        elif priority == "HIGH":
            high_priority.append(kw_info)
        elif priority == "NORMAL":
            normal_keywords.append(kw_info)
        else:
            low_priority.append(kw_info)
    
    # Sortuj keyword_plan po priorytecie
    priority_order = {"CRITICAL": 0, "HIGH": 1, "NORMAL": 2, "LOW": 3, "LOCKED": 4, "EXCEEDED": 5}
    keyword_plan.sort(key=lambda x: priority_order.get(x["priority"], 99))
    
    # ================================================================
    # üìù ANALIZA POPRZEDNICH BATCH√ìW
    # ================================================================
    used_h2 = []
    used_h3 = []
    all_topics_covered = []
    last_sentences = ""
    
    for i, batch in enumerate(batches):
        batch_text = batch.get("text", "")
        
        h2_in_batch = re.findall(r'<h2[^>]*>(.*?)</h2>', batch_text, re.IGNORECASE | re.DOTALL)
        h3_in_batch = re.findall(r'<h3[^>]*>(.*?)</h3>', batch_text, re.IGNORECASE | re.DOTALL)
        
        used_h2.extend([h.strip() for h in h2_in_batch])
        used_h3.extend([h.strip() for h in h3_in_batch])
        all_topics_covered.extend(h2_in_batch + h3_in_batch)
    
    if batches:
        last_batch_text = batches[-1].get("text", "")
        clean_last = re.sub(r'<[^>]+>', '', last_batch_text)
        sentences = re.split(r'[.!?]+', clean_last)
        sentences = [s.strip() for s in sentences if s.strip()]
        if len(sentences) >= 2:
            last_sentences = ". ".join(sentences[-2:]) + "."
        elif sentences:
            last_sentences = sentences[-1] + "."
    
    remaining_h2 = [h2 for h2 in h2_structure if h2 not in used_h2]
    
    # ================================================================
    # üìù GENERUJ PROMPT DLA GPT
    # ================================================================
    prompt_sections = []
    prompt_sections.append(f"üìã BATCH #{current_batch_num} z {total_planned_batches} (zosta≈Ço: {remaining_batches})")
    prompt_sections.append("")
    
    # CRITICAL
    if critical_keywords:
        prompt_sections.append("üî¥ CRITICAL (MUSISZ u≈ºyƒá - ostatni batch!):")
        for kw in critical_keywords:
            prompt_sections.append(f"  ‚Ä¢ {kw['keyword']}: U≈ªYJ {kw['suggested']}x!")
    
    # EXCEEDED
    if exceeded_keywords:
        prompt_sections.append("\n‚ùå EXCEEDED (NIE U≈ªYWAJ!):")
        for kw in exceeded_keywords:
            prompt_sections.append(f"  ‚Ä¢ {kw['keyword']}")
    
    # LOCKED
    if locked_keywords:
        prompt_sections.append("\nüîí LOCKED (u≈ºyj SYNONIM√ìW):")
        for kw in locked_keywords:
            prompt_sections.append(f"  ‚Ä¢ {kw['keyword']}")
    
    # HIGH (UNDER)
    if high_priority:
        prompt_sections.append("\nüü† PRIORYTET (UNDER - wpleƒá!):")
        for kw in high_priority:
            prompt_sections.append(f"  ‚Ä¢ {kw['keyword']}: u≈ºyj {kw['suggested']}x (brakuje {kw['remaining_to_min']}x)")
    
    # NORMAL
    if normal_keywords:
        prompt_sections.append("\nüü¢ NORMALNE (sugerowane u≈ºycie):")
        for kw in normal_keywords[:5]:  # Max 5
            prompt_sections.append(f"  ‚Ä¢ {kw['keyword']}: max {kw['max_per_batch']}x (sugerowane: {kw['suggested']}x)")
    
    # LOW
    if low_priority:
        prompt_sections.append("\n‚ö™ OPCJONALNE (ju≈º OK):")
        for kw in low_priority[:3]:  # Max 3
            prompt_sections.append(f"  ‚Ä¢ {kw['keyword']}")
    
    # Poprzednie tematy
    if all_topics_covered:
        prompt_sections.append("\n\nüìñ NIE POWIELAJ:")
        for topic in all_topics_covered[:8]:
            prompt_sections.append(f"  ‚Ä¢ {topic}")
    
    # Ostatnie zdania
    if last_sentences:
        prompt_sections.append(f"\n\nüîó KONTYNUUJ OD:")
        prompt_sections.append(f"  \"{last_sentences[:150]}...\"" if len(last_sentences) > 150 else f"  \"{last_sentences}\"")
    
    # H2 do napisania
    if remaining_h2:
        prompt_sections.append(f"\n\n‚úèÔ∏è H2 DO NAPISANIA:")
        for h2 in remaining_h2[:3]:
            prompt_sections.append(f"  ‚Ä¢ {h2}")
    
    gpt_prompt = "\n".join(prompt_sections)
    
    # ================================================================
    # üìä PODSUMOWANIE
    # ================================================================
    return jsonify({
        "project_id": project_id,
        "topic": data.get("topic"),
        "batch_number": current_batch_num,
        "total_planned_batches": total_planned_batches,
        "remaining_batches": remaining_batches,
        
        # ‚≠ê PE≈ÅNY PLAN FRAZ (posortowany po priorytecie)
        "keyword_plan": keyword_plan,
        
        # Kategoryzowane
        "critical_keywords": critical_keywords,
        "high_priority_keywords": high_priority,
        "normal_keywords": normal_keywords,
        "low_priority_keywords": low_priority,
        "locked_keywords": locked_keywords,
        "exceeded_keywords": exceeded_keywords,
        
        # Struktura
        "h2_structure": h2_structure,
        "h2_already_written": used_h2,
        "h2_remaining": remaining_h2,
        "topics_already_covered": all_topics_covered,
        "last_sentences": last_sentences,
        
        # Podsumowanie
        "summary": {
            "critical_count": len(critical_keywords),
            "high_priority_count": len(high_priority),
            "normal_count": len(normal_keywords),
            "low_count": len(low_priority),
            "locked_count": len(locked_keywords),
            "exceeded_count": len(exceeded_keywords),
            "h2_written": len(used_h2),
            "h2_remaining": len(remaining_h2)
        },
        
        # ‚≠ê GOTOWY PROMPT
        "gpt_prompt": gpt_prompt,
        
        "instructions": {
            "critical": "MUSISZ u≈ºyƒá tych fraz - ostatnia szansa!",
            "high": "Priorytet - wpleƒá w batch",
            "normal": "U≈ºyj sugerowanƒÖ ilo≈õƒá (max_per_batch)",
            "low": "Opcjonalne - ju≈º wystarczy",
            "locked": "NIE U≈ªYWAJ - u≈ºyj synonim√≥w",
            "exceeded": "B≈ÅƒÑD - za du≈ºo u≈ºyƒá!",
            "max_per_batch": "Nie przekraczaj max_per_batch ≈ºeby uniknƒÖƒá stuffingu"
        }
    }), 200


# ================================================================
# ‚úèÔ∏è ADD BATCH
# ================================================================
@project_routes.post("/api/project/<project_id>/add_batch")
def add_batch_to_project(project_id):
    data = request.get_json()
    if not data:
        return jsonify({"error": "No JSON data provided"}), 400

    batch_text = data.get("text") or data.get("batch_text")
    if not batch_text:
        return jsonify({"error": "Field 'text' or 'batch_text' is required"}), 400

    meta_trace = data.get("meta_trace", {})

    result = process_batch_in_firestore(project_id, batch_text, meta_trace)
    
    rhythm = result.get("meta", {}).get("paragraph_rhythm", "Unknown")
    result["batch_text_snippet"] = batch_text[:50] + "..."
    result["paragraph_rhythm"] = rhythm

    return jsonify(result), 200


# ================================================================
# üîç MANUAL CORRECTION ENDPOINT
# ================================================================
@project_routes.post("/api/project/<project_id>/manual_correct")
def manual_correct_batch(project_id):
    data = request.get_json()
    if not data:
        return jsonify({"error": "No JSON data provided"}), 400

    corrected_text = data.get("text") or data.get("batch_text") or data.get("corrected_text")
    if not corrected_text:
        return jsonify({"error": "Field 'text' or 'batch_text' is required"}), 400

    meta_trace = data.get("meta_trace", {})
    forced = data.get("forced", False)

    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404

    project_data = doc.to_dict()
    keywords_state = project_data.get("keywords_state", {})
    precheck = unified_prevalidation(corrected_text, keywords_state)

    summary = (
        f"Semantic drift: {precheck['semantic_score']:.2f}, "
        f"Transition: {precheck['transition_score']:.2f}, "
        f"Density: {precheck['density']:.2f}, "
        f"Warnings: {len(precheck['warnings'])}"
    )

    if forced:
        print("[FORCED APPROVAL] Saving corrected batch despite warnings.")

    batch_data = {
        "id": str(uuid.uuid4()),
        "text": corrected_text,
        "meta_trace": meta_trace,
        "status": "FORCED" if forced else "APPROVED",
        "language_audit": {
            "semantic_score": precheck["semantic_score"],
            "transition_score": precheck["transition_score"],
            "density": precheck["density"]
        },
        "warnings": precheck["warnings"],
        "corrected": True
    }

    doc_ref = db.collection("seo_projects").document(project_id)
    doc_ref.update({
        "batches": firestore.ArrayUnion([batch_data]),
        "total_batches": firestore.Increment(1)
    })

    return jsonify({
        "status": "CORRECTED_SAVED",
        "project_id": project_id,
        "summary": summary,
        "forced": forced
    }), 200


# ================================================================
# üÜï AUTO-CORRECT ENDPOINT
# ================================================================
@project_routes.post("/api/project/<project_id>/auto_correct")
def auto_correct_batch(project_id):
    """
    Automatyczna korekta batcha u≈ºywajƒÖc Gemini.
    Je≈õli nie podano tekstu, pobiera ostatni batch z projektu.
    """
    data = request.get_json() or {}

    batch_text = data.get("text") or data.get("batch_text") or data.get("corrected_text")
    
    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404

    project_data = doc.to_dict()
    
    # ‚≠ê NOWE: Je≈õli brak tekstu, pobierz ostatni batch
    if not batch_text:
        batches = project_data.get("batches", [])
        if batches:
            batch_text = batches[-1].get("text", "")
            print(f"[AUTO_CORRECT] üì• Pobrano ostatni batch z Firestore ({len(batch_text)} znak√≥w)")
        
    if not batch_text:
        return jsonify({"error": "No text provided and no batches in project"}), 400
    
    keywords_state = project_data.get("keywords_state", {})
    
    under_keywords = []
    over_keywords = []
    
    for rid, meta in keywords_state.items():
        actual = meta.get("actual_uses", 0)
        min_target = meta.get("target_min", 0)
        max_target = meta.get("target_max", 999)
        keyword = meta.get("keyword", "")
        kw_type = meta.get("type", "BASIC")
        
        if actual < min_target:
            under_keywords.append({
                "keyword": keyword,
                "missing": min_target - actual,
                "type": kw_type,
                "current": actual,
                "target_min": min_target
            })
        elif actual > max_target:
            over_keywords.append({
                "keyword": keyword,
                "excess": actual - max_target,
                "type": kw_type,
                "current": actual,
                "target_max": max_target
            })
    
    if not under_keywords and not over_keywords:
        return jsonify({
            "status": "NO_CORRECTIONS_NEEDED",
            "message": "All keywords within target ranges",
            "corrected_text": batch_text,
            "keyword_report": {"under": [], "over": []}
        }), 200
    
    if not GEMINI_API_KEY:
        return jsonify({
            "status": "ERROR",
            "error": "Gemini API key not configured",
            "keyword_report": {"under": under_keywords, "over": over_keywords}
        }), 500
    
    try:
        model = genai.GenerativeModel(GEMINI_MODEL)
        
        correction_instructions = []
        
        if under_keywords:
            under_list = "\n".join([
                f"  - '{kw['keyword']}': Dodaj {kw['missing']}√ó (obecnie {kw['current']}/{kw['target_min']})"
                for kw in under_keywords
            ])
            correction_instructions.append(f"DODAJ te frazy naturalnie:\n{under_list}")
        
        if over_keywords:
            over_list = "\n".join([
                f"  - '{kw['keyword']}': Usu≈Ñ {kw['excess']}√ó (obecnie {kw['current']}, max {kw['target_max']})"
                for kw in over_keywords
            ])
            correction_instructions.append(f"USU≈É nadmiar tych fraz:\n{over_list}")
        
        correction_prompt = f"""
Popraw poni≈ºszy tekst SEO wed≈Çug instrukcji:

{chr(10).join(correction_instructions)}

ZASADY:
1. Zachowaj nag≈Ç√≥wki <h2> i <h3>
2. Reszta tekstu ma byƒá CZYSTYM TEKSTEM (bez <p>, bez <strong>, bez list)
3. Dodawaj frazy naturalnie w kontek≈õcie
4. Usuwaj frazy poprzez parafrazy lub synonimy
5. Zachowaj profesjonalny, formalny styl
6. Ta sama fraza BASIC nie mo≈ºe wystƒôpowaƒá czƒô≈õciej ni≈º 1x na 3 zdania

TEKST DO POPRAWY:
---
{batch_text[:10000]}
---

Zwr√≥ƒá TYLKO poprawiony tekst, bez ≈ºadnych komentarzy.
"""
        
        print(f"[AUTO_CORRECT] Wysy≈Çam do Gemini: {len(under_keywords)} UNDER, {len(over_keywords)} OVER")
        response = model.generate_content(correction_prompt)
        corrected_text = response.text.strip()
        
        # Usu≈Ñ ewentualne markdown/html wrappery
        corrected_text = re.sub(r'^```(?:html)?\n?', '', corrected_text)
        corrected_text = re.sub(r'\n?```$', '', corrected_text)
        
        print(f"[AUTO_CORRECT] ‚úÖ Gemini zwr√≥ci≈Ç poprawiony tekst ({len(corrected_text)} znak√≥w)")
        
        return jsonify({
            "status": "AUTO_CORRECTED",
            "corrected_text": corrected_text,
            "added_keywords": [kw["keyword"] for kw in under_keywords],
            "removed_keywords": [kw["keyword"] for kw in over_keywords],
            "keyword_report": {"under": under_keywords, "over": over_keywords},
            "correction_summary": f"Dodano {len(under_keywords)} fraz, usuniƒôto nadmiar {len(over_keywords)} fraz"
        }), 200
        
    except Exception as e:
        print(f"[AUTO_CORRECT] ‚ùå B≈ÇƒÖd Gemini: {e}")
        return jsonify({
            "status": "ERROR",
            "error": str(e),
            "keyword_report": {"under": under_keywords, "over": over_keywords}
        }), 500


# ================================================================
# üß† UNIFIED PRE-VALIDATION
# ================================================================
@project_routes.post("/api/project/<project_id>/preview_all_checks")
def preview_all_checks(project_id):
    data = request.get_json()
    if not data:
        return jsonify({"error": "No JSON data provided"}), 400

    text = data.get("text") or data.get("batch_text")
    if not text:
        return jsonify({"error": "Field 'text' or 'batch_text' is required"}), 400

    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404

    project_data = doc.to_dict()
    keywords_state = project_data.get("keywords_state", {})

    report = unified_prevalidation(text, keywords_state)

    return jsonify({
        "status": "CHECKED",
        "semantic_score": report["semantic_score"],
        "transition_score": report["transition_score"],
        "density": report["density"],
        "warnings": report["warnings"],
        "summary": f"Semantic: {report['semantic_score']:.2f}, "
                   f"Transition: {report['transition_score']:.2f}, "
                   f"Density: {report['density']:.2f}, "
                   f"Warnings: {len(report['warnings'])}"
    }), 200


# ================================================================
# üÜï FORCE APPROVE
# ================================================================
@project_routes.post("/api/project/<project_id>/force_approve_batch")
def force_approve_batch(project_id):
    data = request.get_json()
    if not data:
        return jsonify({"error": "No JSON data provided"}), 400

    batch_text = data.get("text") or data.get("batch_text")
    if not batch_text:
        return jsonify({"error": "Field 'text' or 'batch_text' is required"}), 400

    meta_trace = data.get("meta_trace", {})

    print("[FORCE APPROVE] User requested forced save.")
    return manual_correct_batch(project_id)


# ================================================================
# üì¶ EXPORT
# ================================================================
@project_routes.get("/api/project/<project_id>/export")
def export_project_data(project_id):
    db = firestore.client()
    doc_ref = db.collection("seo_projects").document(project_id)
    doc = doc_ref.get()
    if not doc.exists:
        return jsonify({"error": "Not found"}), 404

    data = doc.to_dict()
    batches = data.get("batches", [])
    full_text = "\n\n".join(b.get("text", "") for b in batches)

    return jsonify({
        "status": "EXPORT_READY",
        "topic": data.get("topic"),
        "article_text": full_text,  # ‚≠ê Czysty tekst, nie HTML
        "batch_count": len(batches),
        "version": "v22.1"
    }), 200


# ================================================================
# üîÑ ALIAS: auto_correct_keywords
# ================================================================
@project_routes.post("/api/project/<project_id>/auto_correct_keywords")
def auto_correct_keywords_alias(project_id):
    """Alias dla auto_correct - kompatybilno≈õƒá z OpenAPI schema."""
    return auto_correct_batch(project_id)
