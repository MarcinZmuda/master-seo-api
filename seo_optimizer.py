# ================================================================
# ðŸ§  SEO Optimizer â€” SpaCy + Semantic Engine v19.5 (Light Edition)
# ================================================================
# Åadowanie SpaCy w trybie "bezpiecznym" (bez runtime download)
# ObsÅ‚uga NLP dla jÄ™zyka polskiego â€” z pl_core_news_md
# ================================================================

import spacy
from spacy.matcher import PhraseMatcher
from spacy.tokens import Doc
from rich import print
import textstat
import re
from typing import List, Dict

# ================================================================
# ðŸ§© SAFE MODEL LOADER â€” bezpieczne Å‚adowanie modelu SpaCy
# ================================================================
def load_polish_model():
    """
    Bezpieczne Å‚adowanie modelu SpaCy dla jÄ™zyka polskiego.
    UÅ¼ywa pl_core_news_md (Å›redni model ~200 MB).
    Nigdy nie pobiera duÅ¼ego modelu 'lg' w runtime (oszczÄ™dnoÅ›Ä‡ RAM).
    """
    try:
        nlp = spacy.load("pl_core_news_md")
        print("[SEO_OPT] âœ… ZaÅ‚adowano model pl_core_news_md (Light Edition)")
        return nlp
    except OSError:
        try:
            print("[SEO_OPT] âš ï¸ Model MD nieznaleziony, prÃ³ba pobierania...")
            from spacy.cli import download
            download("pl_core_news_md")
            nlp = spacy.load("pl_core_news_md")
            return nlp
        except Exception as e:
            print("[SEO_OPT] âŒ BÅ‚Ä…d przy Å‚adowaniu modelu SpaCy:", e)
            raise SystemExit("âŒ Nie udaÅ‚o siÄ™ zaÅ‚adowaÄ‡ modelu NLP. Zatrzymano proces.")

# Inicjalizacja globalnego modelu SpaCy
nlp = load_polish_model()


# ================================================================
# ðŸ” Keyword density & semantic checks
# ================================================================
def calculate_keyword_density(text: str, keywords: List[str]) -> Dict[str, float]:
    """
    Oblicza gÄ™stoÅ›Ä‡ sÅ‚Ã³w kluczowych (w %) dla zadanej listy fraz.
    """
    text_lower = text.lower()
    total_words = len(text.split())
    densities = {}

    for kw in keywords:
        count = len(re.findall(rf"\b{re.escape(kw.lower())}\b", text_lower))
        densities[kw] = round((count / total_words) * 100, 2) if total_words > 0 else 0.0

    return densities


# ================================================================
# ðŸ§  Semantic similarity checks
# ================================================================
def compute_semantic_similarity(text_a: str, text_b: str) -> float:
    """
    Oblicza semantyczne podobieÅ„stwo (cosine similarity) miÄ™dzy dwoma tekstami.
    """
    doc_a = nlp(text_a)
    doc_b = nlp(text_b)
    similarity = round(doc_a.similarity(doc_b), 4)
    return similarity


# ================================================================
# ðŸ§® Readability metrics (SMOG / FOG / Flesch)
# ================================================================
def compute_readability_metrics(text: str) -> Dict[str, float]:
    """
    Oblicza podstawowe wskaÅºniki czytelnoÅ›ci.
    """
    metrics = {
        "flesch_reading_ease": textstat.flesch_reading_ease(text),
        "smog_index": textstat.smog_index(text),
        "gunning_fog": textstat.gunning_fog(text),
        "avg_sentence_length": textstat.avg_sentence_length(text),
    }
    return metrics


# ================================================================
# ðŸ§± Keyword phrase matcher
# ================================================================
def find_keyword_occurrences(text: str, keywords: List[str]) -> Dict[str, int]:
    """
    Znajduje wystÄ…pienia fraz kluczowych w tekÅ›cie (dokÅ‚adne dopasowania).
    """
    matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
    patterns = [nlp.make_doc(kw) for kw in keywords]
    matcher.add("KEYWORDS", patterns)

    doc = nlp(text)
    matches = matcher(doc)
    occurrences = {}

    for match_id, start, end in matches:
        span = doc[start:end].text
        occurrences[span.lower()] = occurrences.get(span.lower(), 0) + 1

    return occurrences


# ================================================================
# ðŸ§© SEO Optimizer Core
# ================================================================
def analyze_text(text: str, keywords: List[str]) -> Dict:
    """
    GÅ‚Ã³wna funkcja optymalizacji SEO:
    - Liczy wystÄ…pienia sÅ‚Ã³w kluczowych
    - Oblicza gÄ™stoÅ›Ä‡
    - Analizuje czytelnoÅ›Ä‡
    - Zwraca wyniki w formacie JSON-ready
    """
    if not text.strip():
        return {"error": "Brak treÅ›ci do analizy"}

    occurrences = find_keyword_occurrences(text, keywords)
    density = calculate_keyword_density(text, keywords)
    readability = compute_readability_metrics(text)

    report = {
        "keyword_occurrences": occurrences,
        "keyword_density": density,
        "readability": readability,
        "total_words": len(text.split()),
        "unique_keywords_used": len([k for k, v in occurrences.items() if v > 0]),
    }

    print("[SEO_OPT] ðŸ” Analiza SEO zakoÅ„czona pomyÅ›lnie.")
    return report


# ================================================================
# ðŸ§  Semantic drift checker
# ================================================================
def check_semantic_drift(reference_text: str, generated_text: str) -> Dict:
    """
    Sprawdza, czy wygenerowany tekst nie odchodzi semantycznie od oryginaÅ‚u.
    """
    similarity = compute_semantic_similarity(reference_text, generated_text)
    drift = round((1 - similarity) * 100, 2)
    status = "OK" if similarity >= 0.75 else "DRIFT"

    result = {
        "semantic_similarity": similarity,
        "drift_percent": drift,
        "status": status,
    }

    print(f"[SEO_OPT] ðŸ§© Semantyka: {similarity} ({status})")
    return result


# ================================================================
# ðŸ§ª Local test entrypoint (optional)
# ================================================================
if __name__ == "__main__":
    sample_text = """
    Prawo jazdy to dokument potwierdzajÄ…cy uprawnienia do prowadzenia pojazdÃ³w mechanicznych.
    Aby je uzyskaÄ‡, naleÅ¼y zdaÄ‡ egzamin teoretyczny i praktyczny w oÅ›rodku WORD.
    """
    keywords = ["prawo jazdy", "egzamin", "WORD"]

    report = analyze_text(sample_text, keywords)
    print("\n=== SEO REPORT ===")
    for k, v in report.items():
        print(f"{k}: {v}")
