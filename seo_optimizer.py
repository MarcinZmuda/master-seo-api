import os
import re
import json
import spacy
import textstat
from collections import Counter
from typing import List, Dict
import google.generativeai as genai
from rich import print

# ================================================================
# âš™ï¸ Konfiguracja Å›rodowiska
# ================================================================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    print("[SEO_OPT] âœ… Gemini API Key configured")
else:
    print("[SEO_OPT] âš ï¸ GEMINI_API_KEY not set â€“ generative features disabled")

# ================================================================
# ğŸ§  SEMANTIC EMBEDDINGS - Dodatkowa analiza
# ================================================================
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    
    semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    print("[SEO_OPT] âœ… Sentence Transformers loaded (Semantic Analysis)")
    SEMANTIC_ENABLED = True
except ImportError:
    print("[SEO_OPT] âš ï¸ Sentence Transformers not installed - semantic analysis disabled")
    SEMANTIC_ENABLED = False

# ================================================================
# ğŸ§  Åadowanie modelu spaCy (Polish)
# ================================================================
try:
    nlp = spacy.load("pl_core_news_md")
    print("[SEO_OPT] âœ… ZaÅ‚adowano model pl_core_news_md (Light Edition)")
except OSError:
    from spacy.cli import download
    print("[SEO_OPT] âš ï¸ Model pl_core_news_md nieznaleziony â€“ prÃ³ba pobrania...")
    download("pl_core_news_md")
    nlp = spacy.load("pl_core_news_md")
    print("[SEO_OPT] âœ… Model pobrany i zaÅ‚adowany")

# ================================================================
# ğŸ›¡ï¸ HELPER: Safe Gemini Call (Anti-Crash)
# ================================================================
def safe_generate_content(model, prompt: str, max_retries=1):
    """
    Bezpieczne wywoÅ‚anie Gemini z obsÅ‚ugÄ… bÅ‚Ä™dÃ³w dÅ‚ugoÅ›ci.
    JeÅ›li prompt jest za dÅ‚ugi, przycina go i prÃ³buje ponownie.
    """
    try:
        return model.generate_content(prompt)
    except Exception as e:
        error_msg = str(e).lower()
        if "too large" in error_msg or "exhausted" in error_msg or "400" in error_msg:
            print(f"[SEO_OPT] âš ï¸ Gemini Payload too large! Truncating input... Error: {e}")
            if max_retries > 0:
                # Drastyczne ciÄ™cie - bierzemy ostatnie 15k znakÃ³w lub pierwsze 15k
                safe_prompt = prompt[:15000] + "\n\n[TRUNCATED FOR SAFETY]"
                return safe_generate_content(model, safe_prompt, max_retries - 1)
        
        # JeÅ›li to inny bÅ‚Ä…d lub retries siÄ™ skoÅ„czyÅ‚y
        print(f"[SEO_OPT] âŒ Gemini Critical Error: {e}")
        raise e

# ================================================================
# ğŸ§© Funkcja: ekstrakcja sÅ‚Ã³w kluczowych z tekstu
# ================================================================
def extract_keywords(text: str, top_n: int = 15) -> List[str]:
    """Ekstrahuje najczÄ™Å›ciej wystÄ™pujÄ…ce rzeczowniki i frazy."""
    if not text.strip():
        return []

    doc = nlp(text.lower())
    words = [t.lemma_ for t in doc if t.pos_ in {"NOUN", "PROPN"} and len(t.text) > 2]
    freq = Counter(words)
    return [w for w, _ in freq.most_common(top_n)]

# ================================================================
# ğŸ§  Funkcja: ocena czytelnoÅ›ci
# ================================================================
def assess_readability(text: str) -> Dict[str, float]:
    """Zwraca ocenÄ™ trudnoÅ›ci czytania tekstu."""
    try:
        score = textstat.flesch_reading_ease(text)
        grade = textstat.flesch_kincaid_grade(text)
        smog = textstat.smog_index(text)
        return {
            "readability_score": score, 
            "grade_level": grade,
            "smog": smog
        }
    except Exception as e:
        print(f"[SEO_OPT] âš ï¸ Readability error: {e}")
        return {"readability_score": 0, "grade_level": 0, "smog": 0}

# ================================================================
# ğŸ§© Funkcja: optymalizacja semantyczna przez Gemini
# ================================================================
def generate_semantic_outline(topic: str, keywords: List[str]) -> str:
    """Tworzy szkic SEO na podstawie tematu i sÅ‚Ã³w kluczowych."""
    if not GEMINI_API_KEY:
        return "Brak API KEY â€“ tryb offline."

    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        prompt = f"""
        Przygotuj logiczny szkic nagÅ‚Ã³wkÃ³w H2/H3 dla artykuÅ‚u SEO o temacie:
        "{topic}".
        Wykorzystaj moÅ¼liwie duÅ¼o z tych fraz kluczowych:
        {', '.join(keywords)}

        Format:
        - H2: ...
        - H3: ...
        """
        response = safe_generate_content(model, prompt)
        return response.text.strip()
    except Exception as e:
        print(f"[SEO_OPT] âŒ Gemini Outline Error: {e}")
        return "BÅ‚Ä…d podczas generowania outline (API Error)."

# ================================================================
# ğŸ§© Funkcja: prewalidacja tekstu SEO
# ================================================================
def validate_batch_keywords(text: str, required_keywords: List[str]) -> Dict[str, int]:
    """Sprawdza, ile sÅ‚Ã³w kluczowych z listy wystÄ™puje w tekÅ›cie."""
    text_lower = text.lower()
    results = {}
    for kw in required_keywords:
        results[kw] = len(re.findall(rf"\b{re.escape(kw.lower())}\b", text_lower))
    return results

# ================================================================
# ğŸ§  Funkcja: optymalizacja tekstu
# ================================================================
def optimize_text(text: str) -> Dict[str, any]:
    """Wykonuje kompleksowÄ… optymalizacjÄ™ SEO tekstu."""
    if not text.strip():
        return {"optimized_text": "", "readability_score": 0, "keywords_found": []}

    keywords = extract_keywords(text)
    readability = assess_readability(text)

    optimized_text = text
    try:
        # Dodaj przecinki, popraw kapitalizacjÄ™ (prosta heurystyka)
        optimized_text = re.sub(r"\s+", " ", optimized_text).strip()
        optimized_text = optimized_text[0].upper() + optimized_text[1:]
    except Exception as e:
        print(f"[SEO_OPT] âš ï¸ Text cleanup failed: {e}")

    return {
        "optimized_text": optimized_text,
        "keywords_found": keywords,
        "readability_score": readability.get("readability_score", 0),
        "smog": readability.get("smog", 0),
    }

# ================================================================
# ğŸ§© Funkcja: walidacja SEO przez AI (opcjonalnie)
# ================================================================
def ai_validate_text(text: str, topic: str = "") -> Dict[str, any]:
    """UÅ¼ywa Gemini do walidacji SEO tekstu pod kÄ…tem kompletnoÅ›ci."""
    if not GEMINI_API_KEY:
        return {"status": "skipped", "reason": "Brak klucza Gemini"}

    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        prompt = f"""
        OceÅ„, czy poniÅ¼szy tekst dobrze pokrywa temat "{topic}".
        ZwrÃ³Ä‡ ocenÄ™ od 0 do 100 i listÄ™ brakujÄ…cych elementÃ³w.

        Tekst:
        {text[:25000]} 
        """
        # UÅ¼ywamy safe_generate_content zamiast bezpoÅ›redniego wywoÅ‚ania
        response = safe_generate_content(model, prompt)
        return {"status": "ok", "validation_result": response.text}
    except Exception as e:
        print(f"[SEO_OPT] âŒ AI validation failed: {e}")
        return {"status": "error", "error": str(e)}

# ================================================================
# ğŸ§© Pomocnicza funkcja: scalanie danych do Firestore
# ================================================================
def enrich_with_semantics(project_data: dict, text: str) -> dict:
    """Dodaje metadane semantyczne do projektu SEO."""
    try:
        keywords = extract_keywords(text)
        outline = generate_semantic_outline(project_data.get("topic", ""), keywords)
        return {
            **project_data,
            "semantic_enrichment": {
                "keywords": keywords,
                "outline": outline,
            },
        }
    except Exception as e:
        print(f"[SEO_OPT] âŒ enrich_with_semantics error: {e}")
        return project_data

# ================================================================
# ğŸ†• Funkcja: Analiza rytmu akapitÃ³w (Essential for S1/S2)
# ================================================================
def detect_paragraph_rhythm(text: str) -> str:
    """
    Analizuje strukturÄ™ akapitÃ³w w tekÅ›cie.
    Zwraca prosty opis rytmu (np. 'Dynamiczny', 'Monotonny', 'Zbyt dÅ‚ugie bloki').
    """
    if not text:
        return "Brak tekstu"

    paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
    if not paragraphs:
        return "Brak akapitÃ³w"

    # Liczba sÅ‚Ã³w w kaÅ¼dym akapicie
    lengths = [len(p.split()) for p in paragraphs]
    
    if not lengths:
        return "Pusty tekst"

    avg_len = sum(lengths) / len(lengths)
    max_len = max(lengths)

    # Prosta logika oceny rytmu SEO
    if max_len > 300:
        return "ğŸš¨ Zbyt dÅ‚ugie bloki tekstu (SEO Warning)"
    
    if avg_len < 20:
        return "Dynamiczny (krÃ³tkie akapity)"
    
    if avg_len > 80:
        return "CiÄ™Å¼ki / Akademicki"
    
    # Sprawdzenie wariancji (czy akapity sÄ… rÃ³Å¼nej dÅ‚ugoÅ›ci)
    variance = max(lengths) - min(lengths)
    if variance < 10 and len(paragraphs) > 3:
        return "Monotonny (powtarzalna dÅ‚ugoÅ›Ä‡)"

    return "Zbalansowany"

# ================================================================
# ğŸ§© Funkcja: analiza gÄ™stoÅ›ci sÅ‚Ã³w kluczowych
# ================================================================
def calculate_keyword_density(text: str, keywords_state: dict) -> float:
    """
    Oblicza gÄ™stoÅ›Ä‡ sÅ‚Ã³w kluczowych w tekÅ›cie.
    Zwraca procent (0-100).
    """
    if not text or not keywords_state:
        return 0.0
    
    text_lower = text.lower()
    total_words = len(text.split())
    
    if total_words == 0:
        return 0.0
    
    keyword_count = 0
    for rid, meta in keywords_state.items():
        keyword = meta.get("keyword", "").lower()
        if keyword:
            keyword_count += len(re.findall(rf"\b{re.escape(keyword)}\b", text_lower))
    
    density = (keyword_count / total_words) * 100
    return round(density, 2)

# ================================================================
# ğŸ§© Funkcja: Semantic Keyword Coverage (obok n-gramÃ³w)
# ================================================================
def semantic_keyword_coverage(text: str, keywords_state: dict) -> dict:
    """
    Analizuje pokrycie sÅ‚Ã³w kluczowych semantycznie (obok count_robust).
    Zwraca dict z semantic similarity scores dla kaÅ¼dego keyword.
    """
    if not SEMANTIC_ENABLED or not keywords_state:
        return {"semantic_enabled": False, "coverage": {}}
    
    try:
        # Embedding caÅ‚ego tekstu
        text_embedding = semantic_model.encode(text)
        
        coverage = {}
        for rid, meta in keywords_state.items():
            keyword = meta.get("keyword", "")
            if not keyword:
                continue
            
            # Embedding sÅ‚owa kluczowego
            keyword_embedding = semantic_model.encode(keyword)
            
            # Cosine similarity
            similarity = cosine_similarity(
                [text_embedding],
                [keyword_embedding]
            )[0][0]
            
            coverage[keyword] = {
                "semantic_similarity": round(float(similarity), 3),
                "status": "COVERED" if similarity > 0.60 else "WEAK",
                "actual_uses": meta.get("actual_uses", 0),
                "type": meta.get("type", "BASIC")
            }
        
        return {
            "semantic_enabled": True,
            "coverage": coverage,
            "avg_similarity": round(
                sum(c["semantic_similarity"] for c in coverage.values()) / len(coverage),
                3
            ) if coverage else 0.0
        }
        
    except Exception as e:
        print(f"[SEO_OPT] âš ï¸ Semantic coverage error: {e}")
        return {"semantic_enabled": False, "error": str(e), "coverage": {}}

# ================================================================
# ğŸ§  Funkcja: Semantic Drift (cosine similarity miÄ™dzy paragrafami)
# ================================================================
def calculate_semantic_drift(text: str) -> float:
    """
    Oblicza semantic drift - spÃ³jnoÅ›Ä‡ semantycznÄ… miÄ™dzy kolejnymi paragrafami.
    Zwraca wartoÅ›Ä‡ 0-1, gdzie 1 = idealna spÃ³jnoÅ›Ä‡.
    """
    if not SEMANTIC_ENABLED:
        return 0.85  # fallback jeÅ›li brak modelu
    
    # Podziel na paragrafy
    paragraphs = [p.strip() for p in text.split('\n') if p.strip() and len(p.strip()) > 50]
    
    if len(paragraphs) < 2:
        return 1.0  # jeden paragraf = brak driftu
    
    try:
        # Embeddingi wszystkich paragrafÃ³w
        embeddings = semantic_model.encode(paragraphs)
        
        # Oblicz cosine similarity miÄ™dzy kolejnymi paragrafami
        similarities = []
        for i in range(len(embeddings) - 1):
            sim = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]
            similarities.append(float(sim))
        
        # Åšrednia spÃ³jnoÅ›Ä‡
        avg_similarity = sum(similarities) / len(similarities) if similarities else 1.0
        return round(avg_similarity, 3)
        
    except Exception as e:
        print(f"[SEO_OPT] âš ï¸ Semantic drift error: {e}")
        return 0.85  # fallback


# ================================================================
# ğŸ§  Funkcja: Transition Score (analiza sÅ‚Ã³w Å‚Ä…czÄ…cych)
# ================================================================
def calculate_transition_score(text: str) -> float:
    """
    Oblicza jakoÅ›Ä‡ przejÅ›Ä‡ miÄ™dzy zdaniami na podstawie transition words.
    Zwraca wartoÅ›Ä‡ 0-1.
    """
    # Polskie sÅ‚owa przejÅ›ciowe
    transition_words = [
        # Dodawanie
        "ponadto", "dodatkowo", "rÃ³wnieÅ¼", "takÅ¼e", "co wiÄ™cej", "oprÃ³cz tego",
        "poza tym", "w dodatku", "nie tylko", "ale takÅ¼e",
        # Kontrast
        "jednak", "jednakÅ¼e", "natomiast", "ale", "z drugiej strony", "mimo to",
        "niemniej", "pomimo", "choÄ‡", "chociaÅ¼", "wprawdzie",
        # Przyczyna/skutek
        "dlatego", "w zwiÄ…zku z tym", "w rezultacie", "wskutek", "poniewaÅ¼",
        "zatem", "wiÄ™c", "stÄ…d", "w konsekwencji", "przez co",
        # PrzykÅ‚ady
        "na przykÅ‚ad", "przykÅ‚adowo", "miÄ™dzy innymi", "m.in.", "np.",
        # Podsumowanie
        "podsumowujÄ…c", "reasumujÄ…c", "w skrÃ³cie", "ogÃ³lnie rzecz biorÄ…c",
        # Sekwencja
        "po pierwsze", "po drugie", "nastÄ™pnie", "potem", "w koÅ„cu", "na koniec"
    ]
    
    text_lower = text.lower()
    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
    
    if len(sentences) < 2:
        return 1.0
    
    # Ile zdaÅ„ zaczyna siÄ™ od transition word
    transition_count = 0
    for sentence in sentences[1:]:  # pomijamy pierwsze zdanie
        sentence_start = sentence[:50].lower()
        if any(tw in sentence_start for tw in transition_words):
            transition_count += 1
    
    # Optymalne: ~30-50% zdaÅ„ z transition words
    ratio = transition_count / (len(sentences) - 1)
    
    # Mapowanie na score (0.3-0.5 ratio = 1.0 score)
    if 0.25 <= ratio <= 0.55:
        score = 1.0
    elif ratio < 0.25:
        score = 0.5 + (ratio / 0.25) * 0.5
    else:  # ratio > 0.55 (za duÅ¼o)
        score = max(0.5, 1.0 - (ratio - 0.55) * 2)
    
    return round(score, 3)


# ================================================================
# ğŸ§© Backward Compatibility Layer â€“ unified_prevalidation()
# ================================================================
def unified_prevalidation(text: str, keywords_state: dict = None) -> dict:
    """
    POPRAWIONA implementacja unified_prevalidation â€“ zgodna z v19.x API.
    + NOWE: Semantic keyword coverage analysis
    
    Wykonuje wstÄ™pnÄ… walidacjÄ™ i optymalizacjÄ™ batcha SEO przed analizÄ… w Firestore.
    
    Args:
        text: Tekst do walidacji
        keywords_state: SÅ‚ownik ze sÅ‚owami kluczowymi (opcjonalny dla backward compatibility)
    
    Returns:
        Dict z wynikami walidacji
    """
    try:
        # Podstawowa optymalizacja tekstu
        result = optimize_text(text)
        
        # WywoÅ‚ujemy funkcjÄ™ rytmu
        rhythm = detect_paragraph_rhythm(text)
        
        # Ocena czytelnoÅ›ci
        readability = assess_readability(text)
        
        # Obliczenie gÄ™stoÅ›ci sÅ‚Ã³w kluczowych (jeÅ›li podano)
        density = 0.0
        if keywords_state:
            density = calculate_keyword_density(text, keywords_state)
        
        # â­ NOWE: Semantic coverage analysis
        semantic_coverage = {}
        if keywords_state and SEMANTIC_ENABLED:
            semantic_coverage = semantic_keyword_coverage(text, keywords_state)
        
        # Sprawdzenie ostrzeÅ¼eÅ„
        warnings = []
        if "Warning" in rhythm or "ğŸš¨" in rhythm:
            warnings.append(rhythm)
        
        # OstrzeÅ¼enie o zbyt wysokiej gÄ™stoÅ›ci
        if density > 5.0:
            warnings.append(f"âš ï¸ Zbyt wysoka gÄ™stoÅ›Ä‡ sÅ‚Ã³w kluczowych: {density}%")
        
        # â­ RZECZYWISTE semantic scores (zamiast mock)
        semantic_score = calculate_semantic_drift(text)
        transition_score = calculate_transition_score(text)
        
        # Dodatkowe warningi dla niskich scores
        if semantic_score < 0.6:
            warnings.append(f"âš ï¸ Niski semantic drift ({semantic_score}) - paragrafy sÅ‚abo powiÄ…zane")
        if transition_score < 0.5:
            warnings.append(f"âš ï¸ SÅ‚abe przejÅ›cia miÄ™dzy zdaniami ({transition_score})")
        
        return {
            "status": "success",
            "semantic_score": semantic_score,
            "transition_score": transition_score,
            "density": density,
            "smog": readability.get("smog", 0),
            "readability": readability.get("readability_score", 0),
            "optimized_text": result.get("optimized_text", text),
            "warnings": warnings,
            "semantic_coverage": semantic_coverage,  # â­ NOWE
            "meta": {
                "readability_score": readability.get("readability_score"),
                "grade_level": readability.get("grade_level"),
                "keywords_found": result.get("keywords_found", []),
                "paragraph_rhythm": rhythm,
            },
        }
    except Exception as e:
        print(f"[SEO_OPT] âŒ unified_prevalidation error: {e}")
        return {
            "status": "error",
            "semantic_score": 0,
            "transition_score": 0,
            "density": 0,
            "smog": 0,
            "readability": 0,
            "error": str(e),
            "warnings": [str(e)],
            "optimized_text": text,
            "semantic_coverage": {"semantic_enabled": False}  # â­ NOWE
        }
