import os
import re
import json
import spacy
import textstat
from collections import Counter
from typing import List, Dict
import google.generativeai as genai
from rich import print

# ================================================================
# âš™ï¸ Konfiguracja Å›rodowiska
# ================================================================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    print("[SEO_OPT] âœ… Gemini API Key configured")
else:
    print("[SEO_OPT] âš ï¸ GEMINI_API_KEY not set â€” generative features disabled")

# ================================================================
# ğŸ§  Åadowanie modelu spaCy (Polish)
# ================================================================
try:
    nlp = spacy.load("pl_core_news_md")
    print("[SEO_OPT] âœ… ZaÅ‚adowano model pl_core_news_md (Light Edition)")
except OSError:
    from spacy.cli import download
    print("[SEO_OPT] âš ï¸ Model pl_core_news_md nieznaleziony â€” prÃ³ba pobrania...")
    download("pl_core_news_md")
    nlp = spacy.load("pl_core_news_md")
    print("[SEO_OPT] âœ… Model pobrany i zaÅ‚adowany")

# ================================================================
# ğŸ§© Funkcja: ekstrakcja sÅ‚Ã³w kluczowych z tekstu
# ================================================================
def extract_keywords(text: str, top_n: int = 15) -> List[str]:
    """Ekstrahuje najczÄ™Å›ciej wystÄ™pujÄ…ce rzeczowniki i frazy."""
    if not text.strip():
        return []

    doc = nlp(text.lower())
    words = [t.lemma_ for t in doc if t.pos_ in {"NOUN", "PROPN"} and len(t.text) > 2]
    freq = Counter(words)
    return [w for w, _ in freq.most_common(top_n)]

# ================================================================
# ğŸ§  Funkcja: ocena czytelnoÅ›ci
# ================================================================
def assess_readability(text: str) -> Dict[str, float]:
    """Zwraca ocenÄ™ trudnoÅ›ci czytania tekstu."""
    try:
        score = textstat.flesch_reading_ease(text)
        grade = textstat.flesch_kincaid_grade(text)
        return {"readability_score": score, "grade_level": grade}
    except Exception as e:
        print(f"[SEO_OPT] âš ï¸ Readability error: {e}")
        return {"readability_score": 0, "grade_level": 0}

# ================================================================
# ğŸ§© Funkcja: optymalizacja semantyczna przez Gemini
# ================================================================
def generate_semantic_outline(topic: str, keywords: List[str]) -> str:
    """Tworzy szkic SEO na podstawie tematu i sÅ‚Ã³w kluczowych."""
    if not GEMINI_API_KEY:
        return "Brak API KEY â€” tryb offline."

    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        prompt = f"""
        Przygotuj logiczny szkic nagÅ‚Ã³wkÃ³w H2/H3 dla artykuÅ‚u SEO o temacie:
        "{topic}".
        Wykorzystaj moÅ¼liwie duÅ¼o z tych fraz kluczowych:
        {', '.join(keywords)}

        Format:
        - H2: ...
        - H3: ...
        """
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"[SEO_OPT] âŒ Gemini Outline Error: {e}")
        return "BÅ‚Ä…d podczas generowania outline."

# ================================================================
# ğŸ§© Funkcja: prewalidacja tekstu SEO
# ================================================================
def validate_batch_keywords(text: str, required_keywords: List[str]) -> Dict[str, int]:
    """Sprawdza, ile sÅ‚Ã³w kluczowych z listy wystÄ™puje w tekÅ›cie."""
    text_lower = text.lower()
    results = {}
    for kw in required_keywords:
        results[kw] = len(re.findall(rf"\\b{re.escape(kw.lower())}\\b", text_lower))
    return results

# ================================================================
# ğŸ§  Funkcja: optymalizacja tekstu
# ================================================================
def optimize_text(text: str) -> Dict[str, any]:
    """Wykonuje kompleksowÄ… optymalizacjÄ™ SEO tekstu."""
    if not text.strip():
        return {"optimized_text": "", "readability_score": 0, "keywords_found": []}

    keywords = extract_keywords(text)
    readability = assess_readability(text)

    optimized_text = text
    try:
        # Dodaj przecinki, popraw kapitalizacjÄ™ (prosta heurystyka)
        optimized_text = re.sub(r"\\s+", " ", optimized_text).strip()
        optimized_text = optimized_text[0].upper() + optimized_text[1:]
    except Exception as e:
        print(f"[SEO_OPT] âš ï¸ Text cleanup failed: {e}")

    return {
        "optimized_text": optimized_text,
        "keywords_found": keywords,
        "readability_score": readability.get("readability_score", 0),
    }

# ================================================================
# ğŸ§© Funkcja: walidacja SEO przez AI (opcjonalnie)
# ================================================================
def ai_validate_text(text: str, topic: str = "") -> Dict[str, any]:
    """UÅ¼ywa Gemini do walidacji SEO tekstu pod kÄ…tem kompletnoÅ›ci."""
    if not GEMINI_API_KEY:
        return {"status": "skipped", "reason": "Brak klucza Gemini"}

    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        prompt = f"""
        OceÅ„, czy poniÅ¼szy tekst dobrze pokrywa temat "{topic}".
        ZwrÃ³Ä‡ ocenÄ™ od 0 do 100 i listÄ™ brakujÄ…cych elementÃ³w.

        Tekst:
        {text[:8000]}
        """
        response = model.generate_content(prompt)
        return {"status": "ok", "validation_result": response.text}
    except Exception as e:
        print(f"[SEO_OPT] âŒ AI validation failed: {e}")
        return {"status": "error", "error": str(e)}

# ================================================================
# ğŸ§© Pomocnicza funkcja: scalanie danych do Firestore
# ================================================================
def enrich_with_semantics(project_data: dict, text: str) -> dict:
    """Dodaje metadane semantyczne do projektu SEO."""
    try:
        keywords = extract_keywords(text)
        outline = generate_semantic_outline(project_data.get("topic", ""), keywords)
        return {
            **project_data,
            "semantic_enrichment": {
                "keywords": keywords,
                "outline": outline,
            },
        }
    except Exception as e:
        print(f"[SEO_OPT] âŒ enrich_with_semantics error: {e}")
        return project_data

# ================================================================
# ğŸ†• Funkcja: Analiza rytmu akapitÃ³w (DODANO BRAKUJÄ„CÄ„ FUNKCJÄ˜)
# ================================================================
def detect_paragraph_rhythm(text: str) -> str:
    """
    Analizuje strukturÄ™ akapitÃ³w w tekÅ›cie.
    Zwraca prosty opis rytmu (np. 'Dynamiczny', 'Monotonny', 'Zbyt dÅ‚ugie bloki').
    """
    if not text:
        return "Brak tekstu"

    paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
    if not paragraphs:
        return "Brak akapitÃ³w"

    # Liczba sÅ‚Ã³w w kaÅ¼dym akapicie
    lengths = [len(p.split()) for p in paragraphs]
    
    if not lengths:
        return "Pusty tekst"

    avg_len = sum(lengths) / len(lengths)
    max_len = max(lengths)

    # Prosta logika oceny rytmu SEO
    if max_len > 300:
        return "ğŸš¨ Zbyt dÅ‚ugie bloki tekstu (SEO Warning)"
    
    if avg_len < 20:
        return "Dynamiczny (krÃ³tkie akapity)"
    
    if avg_len > 80:
        return "CiÄ™Å¼ki / Akademicki"
    
    # Sprawdzenie wariancji (czy akapity sÄ… rÃ³Å¼nej dÅ‚ugoÅ›ci)
    variance = max(lengths) - min(lengths)
    if variance < 10 and len(paragraphs) > 3:
        return "Monotonny (powtarzalna dÅ‚ugoÅ›Ä‡)"

    return "Zbalansowany"

# ================================================================
# ğŸ§© Backward Compatibility Layer â€” unified_prevalidation()
# ================================================================
def unified_prevalidation(text: str, project_id: str = None) -> dict:
    """
    ZastÄ™pcza implementacja unified_prevalidation â€” zgodna z v18.x API.
    Wykonuje wstÄ™pnÄ… walidacjÄ™ i optymalizacjÄ™ batcha SEO przed analizÄ… w Firestore.
    """
    try:
        result = optimize_text(text)
        # WywoÅ‚ujemy nowÄ… funkcjÄ™ rytmu, Å¼eby byÅ‚a teÅ¼ w metadanych
        rhythm = detect_paragraph_rhythm(text)
        
        return {
            "status": "success",
            "semantic_score": 0.85, # Mock value for backward compat
            "transition_score": 0.80, # Mock value
            "density": 0.02, # Mock value
            "optimized_text": result.get("optimized_text", text),
            "warnings": [] if "Warning" not in rhythm else [rhythm],
            "meta": {
                "readability_score": result.get("readability_score"),
                "keywords_found": result.get("keywords_found", []),
                "paragraph_rhythm": rhythm,
                "project_id": project_id,
            },
        }
    except Exception as e:
        return {
            "status": "error",
            "semantic_score": 0,
            "transition_score": 0,
            "density": 0,
            "error": str(e),
            "warnings": [str(e)],
            "optimized_text": text,
        }
