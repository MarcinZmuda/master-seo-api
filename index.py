import os
import json
import re
import requests
from collections import Counter, defaultdict
from flask import Flask, request, jsonify
import spacy
import google.generativeai as genai
import firebase_admin
from firebase_admin import credentials, firestore

# ======================================================
# ‚≠ê v22.3 LIMITS - zapobieganie OOM
# ======================================================
MAX_CONTENT_SIZE = 30000      # Max 30KB per page (by≈Ço unlimited ‚Üí 175KB crash)
MAX_TOTAL_CONTENT = 200000    # Max 200KB total content
SCRAPE_TIMEOUT = 8            # 8 sekund timeout per page (by≈Ço 10)
SKIP_DOMAINS = ['bip.', '.pdf', 'gov.pl/dana/', '/uploads/files/']  # Skip du≈ºe dokumenty

# ======================================================
# üîë SerpAPI Configuration
# ======================================================
SERPAPI_KEY = os.getenv("SERPAPI_KEY")
if SERPAPI_KEY:
    print("[S1] ‚úÖ SerpAPI key configured")
else:
    print("[S1] ‚ö†Ô∏è SERPAPI_KEY not set ‚Äî auto-fetch disabled")

# ======================================================
# üî• Firebase Initialization (Safe for Render & Local)
# ======================================================
if not firebase_admin._apps:
    cred_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    try:
        if cred_path and os.path.exists(cred_path):
            cred = credentials.Certificate(cred_path)
            firebase_admin.initialize_app(cred)
            print(f"[S1] ‚úÖ Firebase initialized from credentials file: {cred_path}")
        else:
            firebase_admin.initialize_app()
            print("[S1] ‚úÖ Firebase initialized with default credentials")
    except Exception as e:
        print(f"[S1] ‚ö†Ô∏è Firebase init skipped: {e}")

# ======================================================
# ‚öôÔ∏è Gemini (Google Generative AI) Configuration
# ======================================================
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    print("[S1] ‚úÖ Gemini API configured")
else:
    print("[S1] ‚ö†Ô∏è GEMINI_API_KEY not set ‚Äî semantic extraction fallback active")

# ======================================================
# üß† Import local modules (compatible with both local and Render)
# ======================================================
try:
    from .synthesize_topics import synthesize_topics
    from .generate_compliance_report import generate_compliance_report
except ImportError:
    from synthesize_topics import synthesize_topics
    from generate_compliance_report import generate_compliance_report

app = Flask(__name__)

# ======================================================
# üß© Load spaCy model (preinstalled lightweight version)
# ======================================================
try:
    nlp = spacy.load("pl_core_news_sm")
    print("[S1] ‚úÖ spaCy pl_core_news_sm loaded")
except OSError:
    from spacy.cli import download
    download("pl_core_news_sm")
    nlp = spacy.load("pl_core_news_sm")
    print("[S1] ‚úÖ spaCy model downloaded and loaded")

# ======================================================
# ‚≠ê v22.3 Helper: Check if URL should be skipped
# ======================================================
def should_skip_url(url):
    """Sprawdza czy URL powinien byƒá pominiƒôty (du≈ºe dokumenty, PDF, BIP)."""
    url_lower = url.lower()
    for skip_pattern in SKIP_DOMAINS:
        if skip_pattern in url_lower:
            return True
    # Skip je≈õli URL ko≈Ñczy siƒô na rozszerzenie pliku
    if any(url_lower.endswith(ext) for ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx']):
        return True
    return False

# ======================================================
# üß† Helper: Semantic extraction using Gemini Flash
# ======================================================
def extract_semantic_tags_gemini(text, top_n=10):
    """U≈ºywa Google Gemini Flash do wyciƒÖgniƒôcia fraz semantycznych."""
    if not GEMINI_API_KEY or not (text or "").strip():
        return []

    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        prompt = f"""
        Jeste≈õ ekspertem SEO. Przeanalizuj poni≈ºszy tekst i wypisz {top_n} najwa≈ºniejszych fraz kluczowych (semantic keywords), kt√≥re najlepiej oddajƒÖ jego sens.
        Zwr√≥ƒá TYLKO listƒô po przecinku, bez numerowania.

        TEKST: {text[:8000]}...
        """
        response = model.generate_content(prompt)
        keywords = [k.strip() for k in (response.text or "").split(",") if k.strip()]
        return [{"phrase": kw, "score": 0.95 - (i * 0.02)} for i, kw in enumerate(keywords[:top_n])]
    except Exception as e:
        print(f"[S1] ‚ùå Gemini Semantic Error: {e}")
        return []

# ======================================================
# üí° Helper: Generate Content Hints (inspiracje dla GPT)
# ======================================================
def generate_content_hints(serp_analysis, main_keyword):
    """
    Przekszta≈Çca surowe dane SERP w subtelne wskaz√≥wki dla GPT.
    To sƒÖ INSPIRACJE, nie twarde regu≈Çy - GPT ma je traktowaƒá jako t≈Ço.
    """
    hints = {}

    # 1Ô∏è‚É£ INTRO INSPIRATION - z Featured Snippet / AI Overview
    featured = serp_analysis.get("featured_snippet")
    if featured and isinstance(featured, dict) and featured.get("answer"):
        hints["intro_inspiration"] = {
            "google_promotes": featured.get("answer", "")[:500],
            "source_type": featured.get("type", "unknown"),
            "hint": "Google wyr√≥≈ºnia tƒô odpowied≈∫ w wynikach. Rozwa≈º napisanie lepszego/pe≈Çniejszego wstƒôpu kt√≥ry naturalnie odpowiada na to samo pytanie. NIE kopiuj - napisz warto≈õciowszƒÖ wersjƒô."
        }

    # 2Ô∏è‚É£ QUESTIONS USERS ASK - z PAA
    paa = serp_analysis.get("paa_questions", [])
    if paa:
        questions = [q.get("question", "") for q in paa if isinstance(q, dict) and q.get("question")][:6]
        hints["questions_users_ask"] = {
            "questions": questions,
            "hint": "U≈ºytkownicy czƒôsto pytajƒÖ o te rzeczy. Je≈õli pasujƒÖ do tematu, rozwa≈º naturalne poruszenie w tre≈õci. Nie musisz odpowiadaƒá na wszystkie - wybierz relevantne."
        }

        # Bonus: kr√≥tkie odpowiedzi jako kontekst
        qa_context = []
        for q in paa[:3]:
            if isinstance(q, dict) and q.get("question") and q.get("answer"):
                qa_context.append({
                    "q": q.get("question"),
                    "current_answer": (q.get("answer", "") or "")[:200]
                })
        if qa_context:
            hints["questions_users_ask"]["current_answers_preview"] = qa_context

    # 3Ô∏è‚É£ RELATED TOPICS - z Related Searches
    related = serp_analysis.get("related_searches", [])
    if related:
        hints["related_topics"] = {
            "topics": related[:8],
            "hint": "PowiƒÖzane frazy wyszukiwane przez u≈ºytkownik√≥w. MogƒÖ naturalnie pojawiƒá siƒô w tek≈õcie je≈õli sƒÖ relevantne. Nie upychaj na si≈Çƒô."
        }

    # 4Ô∏è‚É£ COMPETITOR INSIGHTS - z tytu≈Ç√≥w i snippet√≥w
    titles = serp_analysis.get("competitor_titles", [])
    snippets = serp_analysis.get("competitor_snippets", [])
    if titles or snippets:
        hints["competitor_insights"] = {
            "hint": "Tak konkurencja prezentuje temat w SERP. Tylko dla orientacji - Twoje podej≈õcie mo≈ºe byƒá inne i lepsze."
        }
        if titles:
            hints["competitor_insights"]["title_patterns"] = titles[:5]
        if snippets:
            hints["competitor_insights"]["description_samples"] = snippets[:3]

    # 5Ô∏è‚É£ STRUCTURE INSPIRATION - z H2 konkurencji
    h2_patterns = serp_analysis.get("competitor_h2_patterns", [])
    if h2_patterns:
        unique_h2 = list(dict.fromkeys(h2_patterns))[:10]
        hints["structure_inspiration"] = {
            "competitor_sections": unique_h2,
            "hint": "Przyk≈Çadowe sekcje u≈ºywane przez konkurencjƒô. Twoja struktura mo≈ºe byƒá inna - to tylko kontekst co inni poruszajƒÖ."
        }

    # 6Ô∏è‚É£ META HINT - og√≥lna wskaz√≥wka
    hints["_meta"] = {
        "interpretation": "Te wskaz√≥wki to T≈ÅO i INSPIRACJA, nie checklist. Artyku≈Ç ma byƒá naturalny, warto≈õciowy i unikalny. U≈ºywaj tych danych ≈ºeby lepiej zrozumieƒá intencjƒô u≈ºytkownika, nie ≈ºeby mechanicznie odpowiadaƒá na ka≈ºdy punkt.",
        "priority": "Jako≈õƒá tre≈õci > dopasowanie do SERP"
    }

    return hints

# ======================================================
# üîç Helper: Fetch sources from SerpAPI (FULL SERP DATA)
# ======================================================
def fetch_serp_sources(keyword, num_results=10):
    """
    Pobiera PE≈ÅNE dane z Google przez SerpAPI:
    - Organic results (top 10 stron) + scrapuje ich pe≈ÇnƒÖ tre≈õƒá
    - PAA (People Also Ask)
    - Featured Snippet
    - Related Searches
    - Tytu≈Çy i snippety z SERP
    
    ‚≠ê v22.3: Dodano limity rozmiaru i skip dla du≈ºych dokument√≥w
    """
    empty_result = {
        "sources": [],
        "paa": [],
        "featured_snippet": None,
        "related_searches": [],
        "serp_titles": [],
        "serp_snippets": []
    }

    if not SERPAPI_KEY:
        print("[S1] ‚ö†Ô∏è SerpAPI key not configured - cannot fetch sources")
        return empty_result

    try:
        print(f"[S1] üîç Fetching FULL SERP data for: {keyword}")
        serp_response = requests.get(
            "https://serpapi.com/search",
            params={
                "q": keyword,
                "api_key": SERPAPI_KEY,
                "num": num_results,
                "hl": "pl",
                "gl": "pl"
            },
            timeout=30
        )

        if serp_response.status_code != 200:
            print(f"[S1] ‚ùå SerpAPI error: {serp_response.status_code}")
            return empty_result

        serp_data = serp_response.json()

        # ‚≠ê 2. WyciƒÖgnij PAA (People Also Ask)
        paa_questions = []
        related_questions = serp_data.get("related_questions", [])
        for q in related_questions:
            paa_questions.append({
                "question": q.get("question", ""),
                "answer": q.get("snippet", ""),
                "source": q.get("link", ""),
                "title": q.get("title", "")
            })
        if paa_questions:
            print(f"[S1] ‚úÖ Found {len(paa_questions)} PAA questions")

        # ‚≠ê 3. WyciƒÖgnij Featured Snippet (Answer Box)
        featured_snippet = None
        answer_box = serp_data.get("answer_box", {})
        if answer_box:
            featured_snippet = {
                "type": answer_box.get("type", "unknown"),
                "title": answer_box.get("title", ""),
                "answer": answer_box.get("answer", "") or answer_box.get("snippet", ""),
                "source": answer_box.get("link", ""),
                "displayed_link": answer_box.get("displayed_link", "")
            }
            print(f"[S1] ‚úÖ Found Featured Snippet: {featured_snippet.get('type')}")

        # ‚≠ê 4. WyciƒÖgnij Related Searches
        related_searches = []
        for rs in serp_data.get("related_searches", []):
            query = rs.get("query", "")
            if query:
                related_searches.append(query)
        if related_searches:
            print(f"[S1] ‚úÖ Found {len(related_searches)} related searches")

        # ‚≠ê 5. WyciƒÖgnij tytu≈Çy i snippety z organic results
        organic_results = serp_data.get("organic_results", [])
        serp_titles = []
        serp_snippets = []

        for result in organic_results:
            title = result.get("title", "")
            snippet = result.get("snippet", "")
            if title:
                serp_titles.append(title)
            if snippet:
                serp_snippets.append(snippet)

        if not organic_results:
            print("[S1] ‚ö†Ô∏è No organic results from SerpAPI")
            return {
                "sources": [],
                "paa": paa_questions,
                "featured_snippet": featured_snippet,
                "related_searches": related_searches,
                "serp_titles": serp_titles,
                "serp_snippets": serp_snippets
            }

        print(f"[S1] ‚úÖ Found {len(organic_results)} SERP results")

        # ‚≠ê 6. Scrapuj PE≈ÅNƒÑ tre≈õƒá ka≈ºdej strony + strukturƒô H2
        sources = []
        total_content_size = 0  # ‚≠ê v22.3: Track total size
        
        for result in organic_results[:num_results]:
            url = result.get("link", "")
            title = result.get("title", "")
            if not url:
                continue
            
            # ‚≠ê v22.3: Skip du≈ºe dokumenty (BIP, PDF, etc.)
            if should_skip_url(url):
                print(f"[S1] ‚è≠Ô∏è Skipping large doc pattern: {url[:50]}...")
                continue
            
            # ‚≠ê v22.3: Stop je≈õli przekroczono total limit
            if total_content_size >= MAX_TOTAL_CONTENT:
                print(f"[S1] ‚ö†Ô∏è Total content limit reached ({MAX_TOTAL_CONTENT} chars), stopping scrape")
                break

            try:
                print(f"[S1] üìÑ Scraping: {url[:60]}...")
                page_response = requests.get(
                    url,
                    timeout=SCRAPE_TIMEOUT,  # ‚≠ê v22.3: Reduced timeout
                    headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
                )

                if page_response.status_code == 200:
                    content = page_response.text
                    
                    # ‚≠ê v22.3: Limit content size PRZED przetwarzaniem
                    if len(content) > MAX_CONTENT_SIZE * 2:  # Raw HTML jest ~2x wiƒôkszy
                        print(f"[S1] ‚ö†Ô∏è Content too large ({len(content)} chars), truncating: {url[:40]}")
                        content = content[:MAX_CONTENT_SIZE * 2]

                    # ‚≠ê WyciƒÖgnij H2 przed usuniƒôciem tag√≥w
                    h2_tags = re.findall(r'<h2[^>]*>(.*?)</h2>', content, re.IGNORECASE | re.DOTALL)
                    h2_clean = [re.sub(r'<[^>]+>', '', h).strip() for h in h2_tags]
                    h2_clean = [h for h in h2_clean if h and len(h) < 200]  # ‚≠ê v22.3: Skip too long H2

                    # Usu≈Ñ script, style, nav, footer, header
                    content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    content = re.sub(r'<nav[^>]*>.*?</nav>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    content = re.sub(r'<footer[^>]*>.*?</footer>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    content = re.sub(r'<header[^>]*>.*?</header>', '', content, flags=re.DOTALL | re.IGNORECASE)
                    # Usu≈Ñ wszystkie tagi HTML
                    content = re.sub(r'<[^>]+>', ' ', content)
                    # Usu≈Ñ wielokrotne spacje
                    content = re.sub(r'\s+', ' ', content).strip()
                    
                    # ‚≠ê v22.3: Final content limit
                    content = content[:MAX_CONTENT_SIZE]

                    if len(content) > 500:
                        sources.append({
                            "url": url,
                            "title": title,
                            "content": content,
                            "h2_structure": h2_clean[:15]
                        })
                        total_content_size += len(content)  # ‚≠ê v22.3: Track size
                        print(f"[S1] ‚úÖ Scraped {len(content)} chars, {len(h2_clean)} H2 from {url[:40]}")
                    else:
                        print(f"[S1] ‚ö†Ô∏è Too short content from {url[:40]}")

            except requests.exceptions.Timeout:
                print(f"[S1] ‚è±Ô∏è Timeout for {url[:40]} (>{SCRAPE_TIMEOUT}s)")
                continue
            except Exception as e:
                print(f"[S1] ‚ö†Ô∏è Scrape error for {url[:40]}: {e}")
                continue

        print(f"[S1] ‚úÖ Successfully scraped {len(sources)} sources ({total_content_size} total chars)")

        return {
            "sources": sources,
            "paa": paa_questions,
            "featured_snippet": featured_snippet,
            "related_searches": related_searches,
            "serp_titles": serp_titles,
            "serp_snippets": serp_snippets
        }

    except Exception as e:
        print(f"[S1] ‚ùå SerpAPI fetch error: {e}")
        return empty_result

# ======================================================
# üîç Endpoint: N-gram + Semantic + SERP Analysis + Firestore Save
# ======================================================
@app.route("/api/ngram_entity_analysis", methods=["POST"])
def perform_ngram_analysis():
    data = request.get_json(force=True)
    main_keyword = data.get("main_keyword", "")
    sources = data.get("sources", [])
    top_n = int(data.get("top_n", 30))
    project_id = data.get("project_id")

    # ‚≠ê Zmienne na dodatkowe dane SERP
    paa_questions = []
    featured_snippet = None
    related_searches = []
    serp_titles = []
    serp_snippets = []
    h2_patterns = []

    # ‚≠ê AUTO-FETCH: Je≈õli brak sources, pobierz PE≈ÅNE dane z SerpAPI
    if not sources:
        if not main_keyword:
            return jsonify({"error": "Brak main_keyword do analizy"}), 400

        print(f"[S1] üîÑ No sources provided - auto-fetching FULL SERP data...")
        serp_result = fetch_serp_sources(main_keyword, num_results=8)  # ‚≠ê v22.3: Reduced from 10 to 8

        # WyciƒÖgnij wszystkie dane z rezultatu
        sources = serp_result.get("sources", [])
        paa_questions = serp_result.get("paa", [])
        featured_snippet = serp_result.get("featured_snippet")
        related_searches = serp_result.get("related_searches", [])
        serp_titles = serp_result.get("serp_titles", [])
        serp_snippets = serp_result.get("serp_snippets", [])

        if not sources:
            return jsonify({
                "error": "Nie uda≈Ço siƒô pobraƒá ≈∫r√≥de≈Ç z SerpAPI",
                "hint": "Sprawd≈∫ czy SERPAPI_KEY jest ustawiony i wa≈ºny",
                "main_keyword": main_keyword,
                "paa": paa_questions,
                "related_searches": related_searches
            }), 400

    print(f"[S1] üîç Analiza n-gram√≥w dla: {main_keyword}")

    # 1Ô∏è‚É£ NLP Statystyczne (N-gramy)
    ngram_presence = defaultdict(set)
    ngram_freqs = Counter()
    all_text_content = []

    for src in sources:
        content = (src.get("content", "") or "").lower()
        if not content.strip():
            continue

        all_text_content.append(src.get("content", ""))

        # ‚≠ê Zbierz struktury H2 z konkurencji
        src_h2 = src.get("h2_structure", [])
        if src_h2:
            h2_patterns.extend(src_h2)

        # ‚≠ê v22.3: Limit content for NLP processing
        doc = nlp(content[:50000])  # Reduced from 100000
        tokens = [t.text.lower() for t in doc if t.is_alpha]

        for n in range(2, 5):
            for i in range(len(tokens) - n + 1):
                ngram = " ".join(tokens[i:i + n])
                ngram_freqs[ngram] += 1
                ngram_presence[ngram].add(src.get("url", "unknown"))

    max_freq = max(ngram_freqs.values()) if ngram_freqs else 1
    results = []

    for ngram, freq in ngram_freqs.items():
        if freq < 2:
            continue
        freq_norm = freq / max_freq
        site_score = len(ngram_presence[ngram]) / len(sources) if sources else 0
        weight = round(freq_norm * 0.5 + site_score * 0.5, 4)
        if main_keyword and main_keyword.lower() in ngram:
            weight += 0.1
        results.append({
            "ngram": ngram,
            "freq": freq,
            "weight": min(1.0, weight),
            "site_distribution": f"{len(ngram_presence[ngram])}/{len(sources)}"
        })

    results = sorted(results, key=lambda x: x["weight"], reverse=True)[:top_n]

    # 2Ô∏è‚É£ Semantyka (Gemini Flash)
    full_text_sample = " ".join(all_text_content)[:15000]
    semantic_keyphrases = extract_semantic_tags_gemini(full_text_sample)

    # ‚≠ê Unikalne H2 z konkurencji (bez duplikat√≥w)
    unique_h2_patterns = list(dict.fromkeys(h2_patterns))[:30]

    # ‚≠ê Przygotuj serp_analysis
    serp_analysis_data = {
        "paa_questions": paa_questions,
        "featured_snippet": featured_snippet,
        "related_searches": related_searches,
        "competitor_titles": serp_titles[:10],
        "competitor_snippets": serp_snippets[:10],
        "competitor_h2_patterns": unique_h2_patterns,
    }

    # 3Ô∏è‚É£ Content Hints - subtelne wskaz√≥wki dla GPT
    content_hints = generate_content_hints(serp_analysis_data, main_keyword)

    # ‚≠ê PE≈ÅNA ODPOWIED≈π z wszystkimi danymi SERP
    response_payload = {
        "main_keyword": main_keyword,
        "ngrams": results,
        "semantic_keyphrases": semantic_keyphrases,

        # ‚úÖ NOWE (MINIMALNA ZMIANA): zwracamy pr√≥bkƒô pe≈Çnych tre≈õci konkurencji,
        # aby Master API mog≈Ço liczyƒá semantic coverage na realnym korpusie.
        # Zachowujemy kompatybilno≈õƒá wstecznƒÖ przez alias "serp_content".
        "full_text_sample": full_text_sample,
        "serp_content": full_text_sample,

        # ‚≠ê Pe≈Çna analiza SERP (surowe dane)
        "serp_analysis": serp_analysis_data,

        # ‚≠ê Content Hints - inspiracje dla GPT
        "content_hints": content_hints,

        "summary": {
            "total_sources": len(sources),
            "sources_auto_fetched": not bool(data.get("sources", [])),
            "paa_count": len(paa_questions),
            "has_featured_snippet": featured_snippet is not None,
            "related_searches_count": len(related_searches),
            "h2_patterns_found": len(unique_h2_patterns),
            "content_hints_generated": bool(content_hints),
            "engine": "v22.3-oom-fix",  # ‚≠ê v22.3
            "lsi_candidates": len(semantic_keyphrases),
        }
    }

    # 3Ô∏è‚É£ Firestore Save (optional)
    if project_id:
        try:
            db = firestore.client()
            doc_ref = db.collection("seo_projects").document(project_id)
            if doc_ref.get().exists:
                avg_len = (
                    sum(len(t.split()) for t in all_text_content) // len(all_text_content)
                    if all_text_content else 0
                )
                doc_ref.update({
                    "s1_data": response_payload,
                    "lsi_enrichment": {"enabled": True, "count": len(semantic_keyphrases)},
                    "avg_competitor_length": avg_len,
                    "updated_at": firestore.SERVER_TIMESTAMP
                })
                response_payload["saved_to_firestore"] = True
                print(f"[S1] ‚úÖ Wyniki n-gram√≥w zapisane do Firestore ‚Üí {project_id}")
            else:
                response_payload["saved_to_firestore"] = False
                print(f"[S1] ‚ö†Ô∏è Nie znaleziono projektu {project_id}")
        except Exception as e:
            print(f"[S1] ‚ùå Firestore error: {e}")
            response_payload["firestore_error"] = str(e)

    return jsonify(response_payload)

# ======================================================
# üß© Pozosta≈Çe Endpointy (Proxy)
# ======================================================
@app.route("/api/synthesize_topics", methods=["POST"])
def perform_synthesize_topics():
    data = request.get_json(force=True)
    ngrams = data.get("ngrams", [])

    # ‚úÖ NOWE (MINIMALNA ZMIANA): obs≈Çuga listy dict√≥w {ngram: "..."} dla kompatybilno≈õci.
    if isinstance(ngrams, list) and ngrams and isinstance(ngrams[0], dict):
        ngrams = [x.get("ngram", "") for x in ngrams if isinstance(x, dict) and x.get("ngram")]

    return jsonify(synthesize_topics(ngrams, data.get("headings", [])))

@app.route("/api/generate_compliance_report", methods=["POST"])
def perform_generate_compliance_report():
    data = request.get_json(force=True)
    return jsonify(generate_compliance_report(data.get("text", ""), data.get("keyword_state", {})))

@app.route("/health", methods=["GET"])
def health():
    return jsonify({
        "status": "ok",
        "engine": "v22.3-oom-fix",  # ‚≠ê v22.3
        "limits": {
            "max_content_per_page": MAX_CONTENT_SIZE,
            "max_total_content": MAX_TOTAL_CONTENT,
            "scrape_timeout": SCRAPE_TIMEOUT,
            "skip_domains": SKIP_DOMAINS
        },
        "features": {
            "gemini_enabled": bool(GEMINI_API_KEY),
            "serpapi_enabled": bool(SERPAPI_KEY),
            "paa_extraction": True,
            "featured_snippet_extraction": True,
            "related_searches_extraction": True,
            "competitor_h2_analysis": True,
            "full_content_scraping": True,
            "content_hints_generation": True,
            "oom_protection": True  # ‚≠ê v22.3
        }
    })

# ======================================================
# üß© Uruchomienie lokalne
# ======================================================
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
