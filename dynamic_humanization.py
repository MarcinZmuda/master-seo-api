"""
===============================================================================
üéØ DYNAMIC HUMANIZATION MODULE v40.1
===============================================================================
Zastƒôpuje s≈Çabe SHORT_INSERTS_LIBRARY dynamicznym systemem.

PROBLEMY ZE STARYM SYSTEMEM:
1. Tylko 9 fraz statycznych
2. Generyczne - nie pasujƒÖ do tematu
3. Sztuczne - "Efekt? Natychmiastowy." brzmi jak reklama

NOWE PODEJ≈öCIE:
1. Dynamiczne kr√≥tkie zdania generowane na podstawie TEMATU
2. Wzorce gramatyczne zamiast gotowych fraz
3. Tematyczne biblioteki (prawo, medycyna, tech, etc.)

ZMIANY v40.1:
- Integracja z synonym_service.py (plWordNet + Firestore cache + LLM fallback)
- CONTEXTUAL_SYNONYMS jako pierwsza warstwa, synonym_service jako fallback
- Wsparcie dla get_synonyms_batch() dla wielu s≈Ç√≥w

Autor: BRAJEN SEO Master API v40.1
===============================================================================
"""

from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import re

# üÜï v41.0: Import rozszerzonej mapy synonim√≥w (105 s≈Ç√≥w zamiast 25)
from contextual_synonyms_v41 import (
    CONTEXTUAL_SYNONYMS_V41,
    get_synonyms_v41,
    get_synonyms_batch_v41,
    get_stats_v41 as get_synonyms_stats_v41
)

# ============================================================================
# üÜï v40.1: INTEGRACJA Z SYNONYM_SERVICE
# ============================================================================

SYNONYM_SERVICE_AVAILABLE = False

try:
    from synonym_service import (
        get_synonyms as _get_synonyms_external,
        get_synonyms_batch as _get_synonyms_batch_external,
        suggest_synonym_for_repetition
    )
    SYNONYM_SERVICE_AVAILABLE = True
    print("[DYNAMIC_HUMANIZATION] ‚úÖ synonym_service loaded (plWordNet + cache)")
except ImportError as e:
    print(f"[DYNAMIC_HUMANIZATION] ‚ö†Ô∏è synonym_service not available: {e}")
    print("[DYNAMIC_HUMANIZATION] ‚ÑπÔ∏è Using local CONTEXTUAL_SYNONYMS only")
    
    # Fallback funkcje
    def _get_synonyms_external(word: str, context: str = "", use_cache: bool = True) -> Dict:
        return {"word": word, "synonyms": [], "source": "none", "count": 0}
    
    def _get_synonyms_batch_external(words: List[str], context: str = "") -> Dict[str, List[str]]:
        return {}
    
    def suggest_synonym_for_repetition(word: str, count: int, context: str = "") -> Dict:
        return {"word": word, "suggestions": [], "source": "none"}


# ============================================================================
# KONTEKSTOWE KR√ìTKIE ZDANIA v41.0
# ============================================================================
# Zamiast statycznych zda≈Ñ ("Lekarz decyduje.", "SƒÖd orzeka.") system
# generuje WZORCE GRAMATYCZNE + INSTRUKCJE dla GPT, ≈ºeby sam tworzy≈Ç
# kr√≥tkie zdania pasujƒÖce do kontekstu aktualnej sekcji H2.
#
# DLACZEGO:
# - Statyczne zdania brzmia≈Çy sztucznie i oderwanie od tematu
# - "Rokowania dobre." wstawione losowo w akapicie o diagnostyce = cringe
# - GPT potrafi stworzyƒá dobre kr√≥tkie zdania, je≈õli dostanie wzorce
# ============================================================================

# Wzorce gramatyczne (3-8 s≈Ç√≥w) ‚Äî GPT wype≈Çnia kontekstem z aktualnej sekcji
SHORT_SENTENCE_GRAMMAR_PATTERNS = {
    # Wzorce stwierdzajƒÖce ‚Äî GPT wstawia podmiot/dope≈Çnienie z tematu sekcji
    "stwierdzenie": [
        "[Podmiot z akapitu] + orzeczenie (3-5 s≈Ç√≥w)",
        "To + przymiotnik kontekstowy (np. 'To czƒôste.', 'To ryzykowne.')",
        "Kr√≥tkie podsumowanie ostatniego zdania (max 5 s≈Ç√≥w)",
        "Zdanie nominalne ‚Äî sam rzeczownik + przymiotnik (np. 'Czƒôsty problem.', 'Wa≈ºna r√≥≈ºnica.')",
    ],
    # Pytania retoryczne ‚Äî nawiƒÖzujƒÖ do tego co jest DALEJ w akapicie
    "pytanie": [
        "Pytanie zaczynajƒÖce nowy wƒÖtek (np. 'A co z dawkowaniem?')",
        "'Dlaczego/Jak/Kiedy + nawiƒÖzanie do nastƒôpnego zdania'",
        "Pytanie potwierdzajƒÖce (np. 'Brzmi skomplikowanie?')",
    ],
    # Tranzycje ‚Äî ≈ÇƒÖczƒÖ my≈õli
    "tranzycja": [
        "Ale/Jednak + kr√≥tka uwaga (np. 'Ale jest wyjƒÖtek.')",
        "Kontrast do poprzedniego zdania (3-6 s≈Ç√≥w)",
        "Zapowied≈∫ zwrotu (np. 'Tu robi siƒô ciekawie.')",
    ],
}

# Domeny tematyczne ‚Äî s≈Çowa kluczowe do detekcji + KONTEKSTOWE PODPOWIEDZI
# (nie gotowe zdania, a wskaz√≥wki jakie kr√≥tkie zdania pasujƒÖ do domeny)
TOPIC_DOMAIN_CONFIG = {
    "prawo": {
        "keywords": ["sƒÖd", "ustawa", "kodeks", "prawo", "wyrok", "pozew",
                     "ubezw≈Çasnowolnienie", "kuratela", "opiekun", "prawny",
                     "notariusz", "akt", "przepis", "roszczenie", "apelacja"],
        "context_hints": [
            "Kr√≥tkie zdania prawnicze: odniesienie do terminu, procedury lub konsekwencji",
            "Np. po opisie procedury: 'Termin jest sztywny.' / 'Tu nie ma wyjƒÖtk√≥w.'",
            "Np. po opisie ryzyka: 'Warto to sprawdziƒá wcze≈õniej.'",
        ],
    },
    "medycyna": {
        "keywords": ["lekarz", "choroba", "leczenie", "diagnoza", "objawy",
                     "terapia", "pacjent", "zdrowie", "psychiczny", "psychiatra",
                     "badanie", "lek", "dawka", "zabieg", "profilaktyka"],
        "context_hints": [
            "Kr√≥tkie zdania medyczne: odniesienie do objawu, leczenia lub rokowania",
            "Np. po opisie objaw√≥w: 'Nie u ka≈ºdego.' / 'Zale≈ºy od pacjenta.'",
            "Np. po opisie leczenia: 'Efekty nie sƒÖ natychmiastowe.'",
        ],
    },
    "finanse": {
        "keywords": ["podatek", "op≈Çata", "koszt", "bud≈ºet", "finanse",
                     "pieniƒÖdze", "kredyt", "rata", "faktura", "rozliczenie"],
        "context_hints": [
            "Kr√≥tkie zdania finansowe: odniesienie do kwoty, terminu lub ryzyka",
            "Np. po opisie koszt√≥w: 'To sporo.' / 'Zale≈ºy od umowy.'",
            "Np. po opisie procedury: 'Warto policzyƒá wcze≈õniej.'",
        ],
    },
    "technologia": {
        "keywords": ["system", "aplikacja", "software", "kod", "program",
                     "technologia", "digital", "online", "algorytm", "serwer"],
        "context_hints": [
            "Kr√≥tkie zdania tech: odniesienie do dzia≈Çania, wymaga≈Ñ lub ogranicze≈Ñ",
            "Np. po opisie funkcji: 'Dzia≈Ça automatycznie.' / 'Nie zawsze.'",
            "Np. po opisie problemu: '≈Åatwa poprawka.' / 'To znany problem.'",
        ],
    },
    "edukacja": {
        "keywords": ["dziecko", "nauka", "szko≈Ça", "rozw√≥j", "edukacja",
                     "terapia", "ƒáwiczenia", "przedszkole", "ucze≈Ñ", "nauczyciel"],
        "context_hints": [
            "Kr√≥tkie zdania edukacyjne: odniesienie do postƒôp√≥w, metod lub efekt√≥w",
            "Np. po opisie metody: 'Wymaga cierpliwo≈õci.' / 'Efekty przyjdƒÖ.'",
            "Np. po opisie problemu: 'To normalne na tym etapie.'",
        ],
    },
    "universal": {
        "keywords": [],
        "context_hints": [
            "Kr√≥tkie zdania odnoszƒÖce siƒô do tre≈õci poprzedniego lub nastƒôpnego zdania",
            "Unikaj og√≥lnik√≥w ‚Äî zdanie musi wynikaƒá z kontekstu akapitu",
        ],
    },
}


# ============================================================================
# G≈Å√ìWNA FUNKCJA - GENEROWANIE KR√ìTKICH ZDA≈É
# ============================================================================

def detect_topic_domain(main_keyword: str, h2_titles: List[str] = None) -> str:
    """
    Wykrywa domenƒô tematycznƒÖ na podstawie s≈Ç√≥w kluczowych.
    
    Returns:
        Nazwa domeny: "prawo", "medycyna", "finanse", "technologia", "edukacja", "universal"
    """
    text_to_check = main_keyword.lower()
    if h2_titles:
        text_to_check += " " + " ".join(h2_titles).lower()
    
    # Sprawd≈∫ ka≈ºdƒÖ domenƒô
    domain_scores = {}
    for domain, config in TOPIC_DOMAIN_CONFIG.items():
        if domain == "universal":
            continue
        score = 0
        for keyword in config["keywords"]:
            if keyword in text_to_check:
                score += 1
        domain_scores[domain] = score
    
    # Zwr√≥ƒá domenƒô z najwy≈ºszym score (lub universal)
    if domain_scores:
        best_domain = max(domain_scores, key=domain_scores.get)
        if domain_scores[best_domain] > 0:
            return best_domain
    
    return "universal"


def get_dynamic_short_sentences(
    main_keyword: str,
    h2_titles: List[str] = None,
    count: int = 8,
    include_questions: bool = True,
    current_h2: str = None,
    batch_num: int = None,
) -> Dict[str, any]:
    """
    Generuje KONTEKSTOWE instrukcje kr√≥tkich zda≈Ñ ‚Äî GPT tworzy je sam
    na podstawie wzorc√≥w gramatycznych i podpowiedzi domenowych.
    
    ZMIANA v41.0: Zamiast statycznych zda≈Ñ ("Lekarz decyduje.") system
    daje GPT wzorce + kontekst, ≈ºeby tworzy≈Ç zdania pasujƒÖce do
    aktualnej sekcji H2.
    
    Args:
        main_keyword: G≈Ç√≥wna fraza kluczowa
        h2_titles: Lista tytu≈Ç√≥w H2 (opcjonalnie)
        count: Ile wzorc√≥w zwr√≥ciƒá (nieu≈ºywane, zachowane dla kompatybilno≈õci)
        include_questions: Czy do≈ÇƒÖczyƒá wzorce pyta≈Ñ retorycznych
        current_h2: Aktualny tytu≈Ç H2 (dla lepszego kontekstu)
        batch_num: Numer aktualnego batcha
        
    Returns:
        Dict z:
        - domain: wykryta domena
        - grammar_patterns: wzorce gramatyczne do zastosowania
        - context_hints: podpowiedzi domenowe
        - instruction: pe≈Çna instrukcja dla GPT
    """
    # Wykryj domenƒô
    domain = detect_topic_domain(main_keyword, h2_titles)
    
    # Pobierz podpowiedzi domenowe
    domain_config = TOPIC_DOMAIN_CONFIG.get(domain, TOPIC_DOMAIN_CONFIG["universal"])
    context_hints = domain_config.get("context_hints", [])
    
    # Zbierz wzorce gramatyczne
    patterns_to_use = []
    patterns_to_use.extend(SHORT_SENTENCE_GRAMMAR_PATTERNS["stwierdzenie"])
    if include_questions:
        patterns_to_use.extend(SHORT_SENTENCE_GRAMMAR_PATTERNS["pytanie"])
    patterns_to_use.extend(SHORT_SENTENCE_GRAMMAR_PATTERNS["tranzycja"])
    
    # Kontekst sekcji ‚Äî je≈õli znamy aktualne H2
    section_context = ""
    if current_h2:
        section_context = f"\nAktualna sekcja: \"{current_h2}\" ‚Äî kr√≥tkie zdania MUSZƒÑ dotyczyƒá tego tematu."
    
    instruction = f"""‚úÇÔ∏è KR√ìTKIE ZDANIA (3-8 s≈Ç√≥w) ‚Äî tw√≥rz W≈ÅASNE, pasujƒÖce do kontekstu:

TEMAT ARTYKU≈ÅU: {main_keyword}
DOMENA: {domain}{section_context}

ZASADY TWORZENIA (2-4 kr√≥tkie zdania na batch):
1. Ka≈ºde kr√≥tkie zdanie MUSI wynikaƒá z poprzedniego lub nastƒôpnego zdania
2. NIE wstawiaj og√≥lnik√≥w oderwanych od tre≈õci
3. Wstaw po d≈Çugim zdaniu (>25 s≈Ç√≥w) jako "oddech" dla czytelnika
4. Przed zmianƒÖ wƒÖtku w akapicie

WZORCE GRAMATYCZNE (wype≈Çnij tre≈õciƒÖ z akapitu):
‚Ä¢ Stwierdzenie: podmiot z akapitu + kr√≥tkie orzeczenie (np. "Termin jest sztywny.", "To zale≈ºy od dawki.")
‚Ä¢ Zdanie nominalne: rzeczownik + przymiotnik z kontekstu (np. "Czƒôsty b≈ÇƒÖd.", "Wa≈ºna r√≥≈ºnica.")
‚Ä¢ Pytanie retoryczne: nawiƒÖzanie do nastƒôpnego zdania (np. "A co z kosztami?", "Jak to wyglƒÖda w praktyce?")
‚Ä¢ Kontrast/tranzycja: kr√≥tki zwrot akcji (np. "Ale jest wyjƒÖtek.", "Nie zawsze.")
‚Ä¢ Podsumowanie: esencja ostatniego zdania w 3-5 s≈Ç√≥w

PODPOWIEDZI DLA TEJ DOMENY ({domain}):
{chr(10).join(f"‚Ä¢ {h}" for h in context_hints)}

‚ùå NIE R√ìB TAK (oderwane od kontekstu):
‚Ä¢ "To wa≈ºne." (og√≥lnik)
‚Ä¢ "Warto wiedzieƒá." (nic nie m√≥wi)
‚Ä¢ "Pamiƒôtaj." (pusty rozkaz)

‚úÖ R√ìB TAK (wynika z kontekstu):
‚Ä¢ Po akapicie o skutkach ubocznych leku: "Nie u ka≈ºdego pacjenta."
‚Ä¢ Po akapicie o terminach sƒÖdowych: "Termin jest nieprzekraczalny."
‚Ä¢ Po opisie skomplikowanej procedury: "Brzmi skomplikowanie? Niekoniecznie."

üí° DZIELENIE D≈ÅUGICH ZDA≈É:
Je≈õli zdanie ma >25 s≈Ç√≥w, podziel je na dwa kr√≥tsze w naturalnym punkcie:
‚Ä¢ Przed "ale", "jednak", "natomiast" ‚Üí kropka, usu≈Ñ sp√≥jnik, capitalize resztƒô
‚Ä¢ Przy ≈õredniku ‚Üí zamie≈Ñ na kropkƒô
‚Ä¢ Przed "poniewa≈º", "gdy≈º" ‚Üí przebuduj na samodzielne zdanie przyczynowe
Przyk≈Çad: "Leczenie trwa kilka tygodni, ale efekty sƒÖ widoczne ju≈º po pierwszym cyklu."
‚Üí "Leczenie trwa kilka tygodni. Efekty sƒÖ widoczne ju≈º po pierwszym cyklu."
"""
    
    return {
        "domain": domain,
        "grammar_patterns": patterns_to_use,
        "context_hints": context_hints,
        "instruction": instruction,
        # Zachowane dla kompatybilno≈õci wstecznej ‚Äî puste, bo GPT ma tworzyƒá w≈Çasne
        "sentences": [],
        "patterns": SHORT_SENTENCE_GRAMMAR_PATTERNS,
    }


# ============================================================================
# üÜï v41.0: DZIELENIE D≈ÅUGICH ZDA≈É (SENTENCE SPLITTER)
# ============================================================================
# Zamiast wstawiaƒá sztuczne kr√≥tkie zdania, dzielimy istniejƒÖce d≈Çugie
# zdania w naturalnych punktach gramatycznych polszczyzny.
#
# Efekt:
# - Burstiness ro≈õnie organicznie (wiƒôcej kr√≥tkich zda≈Ñ)
# - Tre≈õƒá pozostaje kontekstowa (bo pochodzi z oryginalnego zdania)
# - Czytelno≈õƒá siƒô poprawia (kr√≥tsze zdania = ≈Çatwiejszy odbi√≥r)
# ============================================================================

import re as _re

# Punkty podzia≈Çu zda≈Ñ ‚Äî posortowane wg bezpiecze≈Ñstwa (od najbezpieczniejszych)

# TIER 1: Bardzo bezpieczne ‚Äî prawie zawsze dajƒÖ poprawne dwa zdania
SPLIT_POINTS_TIER1 = [
    # ≈örednik ‚Üí kropka (zawsze bezpieczne)
    (r';\s+', '. ', 'semicolon'),
    # My≈õlnik em-dash z spacjami ‚Äî czƒôsto oddziela niezale≈ºne my≈õli
    (r'\s+‚Äì\s+', '. ', 'em_dash'),
]

# TIER 2: Bezpieczne ‚Äî sp√≥jniki wsp√≥≈Çrzƒôdne (niezale≈ºne zdania sk≈Çadowe)
# Po podziale sp√≥jnik jest USUWANY ‚Äî kropka pe≈Çni jego funkcjƒô
SPLIT_POINTS_TIER2 = [
    (r',\s+ale\s+', '. ', 'ale'),
    (r',\s+jednak\s+', '. ', 'jednak'),
    (r',\s+natomiast\s+', '. ', 'natomiast'),
    (r',\s+lecz\s+', '. ', 'lecz'),
    (r',\s+wiƒôc\s+', '. ', 'wiec'),
    (r',\s+dlatego\s+', '. ', 'dlatego'),
    (r',\s+zatem\s+', '. ', 'zatem'),
    (r',\s+tymczasem\s+', '. ', 'tymczasem'),
    (r',\s+z kolei\s+', '. ', 'z_kolei'),
    (r',\s+a tak≈ºe\s+', '. ', 'a_takze'),
    (r',\s+a jednocze≈õnie\s+', '. ', 'a_jednoczesnie'),
]

# TIER 3: Ostro≈ºne ‚Äî sp√≥jniki przyczynowe/wynikowe
# Usuwamy sp√≥jnik i przebudowujemy poczƒÖtek na samodzielne zdanie
SPLIT_POINTS_TIER3 = [
    (r',\s+poniewa≈º\s+', '. Wynika to z tego, ≈ºe ', 'poniewaz'),
    (r',\s+gdy≈º\s+', '. Powodem jest to, ≈ºe ', 'gdyz'),
    (r',\s+bowiem\s+', '. ', 'bowiem'),
    (r',\s+przy czym\s+', '. Warto dodaƒá, ≈ºe ', 'przy_czym'),
    (r',\s+co oznacza,?\s+≈ºe\s+', '. Oznacza to, ≈ºe ', 'co_oznacza'),
    (r',\s+co powoduje,?\s+≈ºe\s+', '. Skutkuje to tym, ≈ºe ', 'co_powoduje'),
    (r',\s+co sprawia,?\s+≈ºe\s+', '. W rezultacie ', 'co_sprawia'),
]

# Minimalna d≈Çugo≈õƒá ka≈ºdej z dw√≥ch czƒô≈õci po podziale (w s≈Çowach)
MIN_HALF_WORDS = 5

# Pr√≥g d≈Çugo≈õci zdania, powy≈ºej kt√≥rego pr√≥bujemy dzieliƒá
LONG_SENTENCE_THRESHOLD = 28  # s≈Ç√≥w


@dataclass
class SplitResult:
    """Wynik podzia≈Çu jednego zdania."""
    original: str
    part1: str
    part2: str
    split_type: str  # np. 'ale', 'semicolon', 'em_dash'
    tier: int  # 1, 2 lub 3


def _count_words(text: str) -> int:
    """Liczy s≈Çowa w tek≈õcie."""
    return len(text.split())


def _find_best_split(sentence: str, threshold: int = LONG_SENTENCE_THRESHOLD) -> Optional[SplitResult]:
    """
    Znajduje najlepszy punkt podzia≈Çu dla d≈Çugiego zdania.
    
    Strategia:
    1. Sprawd≈∫ TIER 1 (≈õredniki, my≈õlniki) ‚Äî zawsze bezpieczne
    2. Sprawd≈∫ TIER 2 (ale, jednak, natomiast) ‚Äî bezpieczne
    3. Sprawd≈∫ TIER 3 (poniewa≈º, gdy≈º) ‚Äî ostro≈ºnie
    4. Je≈õli wiele opcji ‚Äî wybierz tƒô, kt√≥ra daje najbardziej r√≥wny podzia≈Ç
    
    Returns:
        SplitResult lub None je≈õli nie znaleziono bezpiecznego podzia≈Çu
    """
    word_count = _count_words(sentence)
    if word_count < threshold:
        return None
    
    candidates = []
    
    all_tiers = [
        (1, SPLIT_POINTS_TIER1),
        (2, SPLIT_POINTS_TIER2),
        (3, SPLIT_POINTS_TIER3),
    ]
    
    for tier_num, tier_points in all_tiers:
        for pattern, replacement, split_type in tier_points:
            # Znajd≈∫ WSZYSTKIE wystƒÖpienia wzorca w zdaniu
            for match in _re.finditer(pattern, sentence):
                start, end = match.start(), match.end()
                part1 = sentence[:start].strip()
                # Replacement zawiera nowy poczƒÖtek part2 (np. ". Ale ")
                # We≈∫ tylko to co po replacement (kapitalizacja jest w replacement)
                part2_raw = sentence[end:].strip()
                
                # Zbuduj part2 z odpowiednim poczƒÖtkiem
                # replacement = '. Ale ' ‚Üí part1 ko≈Ñczy siƒô kropkƒÖ, part2 zaczyna od 'Ale ...'
                rep_parts = replacement.split('. ', 1)
                if len(rep_parts) == 2 and rep_parts[1]:
                    # np. replacement = '. Ale ' ‚Üí prefix = 'Ale '
                    prefix = rep_parts[1]
                    part2 = prefix + part2_raw
                else:
                    # np. replacement = '. ' ‚Üí po prostu capitalize
                    part2 = part2_raw[0].upper() + part2_raw[1:] if part2_raw else part2_raw
                
                # Zako≈Ñcz part1 kropkƒÖ je≈õli nie ma
                if part1 and part1[-1] not in '.!?':
                    part1 = part1 + '.'
                
                # Sprawd≈∫ czy obie czƒô≈õci majƒÖ minimalnƒÖ d≈Çugo≈õƒá
                if _count_words(part1) >= MIN_HALF_WORDS and _count_words(part2) >= MIN_HALF_WORDS:
                    # Oblicz balans (im bli≈ºej 0.5, tym lepiej)
                    total = _count_words(part1) + _count_words(part2)
                    balance = min(_count_words(part1), _count_words(part2)) / total
                    
                    candidates.append({
                        'result': SplitResult(
                            original=sentence,
                            part1=part1,
                            part2=part2,
                            split_type=split_type,
                            tier=tier_num
                        ),
                        'balance': balance,
                        'tier': tier_num,
                    })
    
    if not candidates:
        return None
    
    # Wybierz: priorytet tier (ni≈ºszy = lepszy), potem balans (wy≈ºszy = lepszy)
    candidates.sort(key=lambda c: (c['tier'], -c['balance']))
    return candidates[0]['result']


def split_long_sentences(
    text: str,
    threshold: int = LONG_SENTENCE_THRESHOLD,
    max_splits: int = 4,
    min_tier: int = 3,
) -> Dict[str, any]:
    """
    Dzieli d≈Çugie zdania w tek≈õcie na kr√≥tsze w naturalnych punktach gramatycznych.
    
    Args:
        text: Tekst do przetworzenia (batch_content)
        threshold: Min. liczba s≈Ç√≥w w zdaniu, ≈ºeby pr√≥bowaƒá dzieliƒá (default 28)
        max_splits: Max liczba zda≈Ñ do podzielenia w jednym batchu (default 4)
        min_tier: Najni≈ºszy akceptowalny tier (1=tylko bezpieczne, 3=wszystkie)
        
    Returns:
        Dict z:
        - modified_text: tekst po podziale
        - splits: lista SplitResult (co zosta≈Ço podzielone)
        - stats: statystyki (ile zda≈Ñ by≈Ço d≈Çugich, ile podzielono)
        - before_after: lista par (before, after) do prezentacji GPT
    """
    # Podziel na akapity (zachowaj strukturƒô)
    paragraphs = text.split('\n')
    
    splits_done = []
    modified_paragraphs = []
    long_count = 0
    
    for paragraph in paragraphs:
        if not paragraph.strip():
            modified_paragraphs.append(paragraph)
            continue
        
        # Podziel akapit na zdania (regex z ai_detection_metrics)
        sentences = _re.split(r'(?<=[.!?])\s+(?=[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª])', paragraph)
        modified_sentences = []
        
        for sentence in sentences:
            word_count = _count_words(sentence)
            
            if word_count >= threshold and len(splits_done) < max_splits:
                long_count += 1
                split = _find_best_split(sentence, threshold)
                
                if split and split.tier <= min_tier:
                    splits_done.append(split)
                    modified_sentences.append(split.part1)
                    modified_sentences.append(split.part2)
                else:
                    modified_sentences.append(sentence)
            else:
                modified_sentences.append(sentence)
        
        modified_paragraphs.append(' '.join(modified_sentences))
    
    modified_text = '\n'.join(modified_paragraphs)
    
    # Wygeneruj before/after do prezentacji
    before_after = []
    for s in splits_done:
        before_after.append({
            "before": s.original,
            "after": f"{s.part1} {s.part2}",
            "split_type": s.split_type,
            "tier": s.tier,
        })
    
    return {
        "modified_text": modified_text,
        "splits": splits_done,
        "split_count": len(splits_done),
        "stats": {
            "long_sentences_found": long_count,
            "sentences_split": len(splits_done),
            "threshold": threshold,
            "max_tier_used": max(s.tier for s in splits_done) if splits_done else 0,
        },
        "before_after": before_after,
    }


def suggest_sentence_splits(
    text: str,
    threshold: int = LONG_SENTENCE_THRESHOLD,
    max_suggestions: int = 4,
) -> List[Dict[str, str]]:
    """
    Zwraca SUGESTIE podzia≈Çu d≈Çugich zda≈Ñ (bez modyfikacji tekstu).
    U≈ºywane w fix_instructions dla GPT ‚Äî pokazuje co i jak podzieliƒá.
    
    Returns:
        Lista dict z: original, suggested_part1, suggested_part2, split_type
    """
    result = split_long_sentences(text, threshold=threshold, max_splits=max_suggestions)
    
    suggestions = []
    for ba in result["before_after"]:
        suggestions.append({
            "original": ba["before"][:120] + ("..." if len(ba["before"]) > 120 else ""),
            "suggested": ba["after"][:140] + ("..." if len(ba["after"]) > 140 else ""),
            "split_type": ba["split_type"],
        })
    
    return suggestions


# ============================================================================
# SYNONIMY DYNAMICZNE - zamiast s≈Çabego SYNONYM_MAP
# ============================================================================

# üÜï v41.0: Synonimy kontekstowe - importowane z contextual_synonyms_v41.py
# 105 s≈Ç√≥w w 7 kategoriach (by≈Ço 25 s≈Ç√≥w)
CONTEXTUAL_SYNONYMS = CONTEXTUAL_SYNONYMS_V41


def get_synonyms_for_word(word: str, context: str = "") -> List[str]:
    """
    üÜï v41.0: Zwraca synonimy dla s≈Çowa - najpierw rozszerzona mapa v41.
    
    Hierarchia ≈∫r√≥de≈Ç:
    1. contextual_synonyms_v41 (105 s≈Ç√≥w - najszybsze)
    2. synonym_service (plWordNet API + Firestore cache + LLM fallback)
    
    Args:
        word: S≈Çowo do znalezienia synonim√≥w
        context: Opcjonalny kontekst (np. "artyku≈Ç prawniczy")
        
    Returns:
        Lista synonim√≥w (max 5)
    """
    word_lower = word.lower().strip()
    
    # 1. NAJPIERW: rozszerzona mapa v41 (105 s≈Ç√≥w)
    v41_synonyms = get_synonyms_v41(word_lower, max_count=5)
    if v41_synonyms:
        return v41_synonyms
    
    # 2. FALLBACK: synonym_service (plWordNet + cache + LLM)
    if SYNONYM_SERVICE_AVAILABLE:
        try:
            result = _get_synonyms_external(word_lower, context=context, use_cache=True)
            external_synonyms = result.get("synonyms", [])
            if external_synonyms:
                source = result.get("source", "unknown")
                print(f"[DYNAMIC_HUMANIZATION] üìö Synonyms for '{word}' from {source}: {external_synonyms[:3]}")
                return external_synonyms[:5]
        except Exception as e:
            print(f"[DYNAMIC_HUMANIZATION] ‚ö†Ô∏è synonym_service error for '{word}': {e}")
    
    return []


def get_synonyms_batch(words: List[str], context: str = "") -> Dict[str, List[str]]:
    """
    üÜï v40.1: Pobiera synonimy dla wielu s≈Ç√≥w naraz.
    
    Optymalizacja - jedno zapytanie zamiast wielu.
    
    Args:
        words: Lista s≈Ç√≥w
        context: Kontekst artyku≈Çu
        
    Returns:
        Dict {s≈Çowo: [synonimy]}
    """
    result = {}
    words_to_fetch_external = []
    
    # 1. Sprawd≈∫ lokalnƒÖ mapƒô
    for word in words:
        word_lower = word.lower().strip()
        local = CONTEXTUAL_SYNONYMS.get(word_lower, [])
        if local:
            result[word] = local[:5]
        else:
            words_to_fetch_external.append(word)
    
    # 2. Pobierz brakujƒÖce z synonym_service
    if words_to_fetch_external and SYNONYM_SERVICE_AVAILABLE:
        try:
            external_results = _get_synonyms_batch_external(words_to_fetch_external, context)
            for word, synonyms in external_results.items():
                if synonyms:
                    result[word] = synonyms[:5]
        except Exception as e:
            print(f"[DYNAMIC_HUMANIZATION] ‚ö†Ô∏è Batch synonym fetch error: {e}")
    
    return result


def get_synonym_instructions(overused_words: List[str] = None, context: str = "") -> Dict[str, any]:
    """
    Generuje instrukcje synonim√≥w dla GPT.
    
    v40.1: U≈ºywa batch fetch dla wydajno≈õci + kontekst dla lepszych wynik√≥w.
    
    Args:
        overused_words: Lista s≈Ç√≥w kt√≥re sƒÖ nadu≈ºywane w artykule
        context: Kontekst artyku≈Çu (np. "prawo", "medycyna")
        
    Returns:
        Dict z instrukcjami i mapƒÖ synonim√≥w
    """
    # Je≈õli podano nadu≈ºywane s≈Çowa, priorytetyzuj je
    if overused_words:
        # v40.1: U≈ºyj batch fetch dla wydajno≈õci
        all_synonyms = get_synonyms_batch(overused_words, context=context)
        
        priority_synonyms = {}
        for word in overused_words:
            syns = all_synonyms.get(word, [])
            if not syns:
                # Fallback do pojedynczego zapytania
                syns = get_synonyms_for_word(word, context=context)
            if syns:
                priority_synonyms[word] = syns[:3]
        
        if priority_synonyms:
            # Informacja o ≈∫r√≥dle
            source_info = "plWordNet + cache" if SYNONYM_SERVICE_AVAILABLE else "local"
            
            return {
                "priority": "HIGH",
                "instruction": "‚ö†Ô∏è TE S≈ÅOWA SƒÑ NADU≈ªYWANE - u≈ºyj synonim√≥w:",
                "synonyms": priority_synonyms,
                "warning": "Nie powtarzaj tego samego s≈Çowa >3x w batchu!",
                "source": source_info
            }
    
    # Domy≈õlne - og√≥lne wskaz√≥wki
    return {
        "priority": "NORMAL",
        "instruction": "Unikaj powt√≥rze≈Ñ - u≈ºywaj synonim√≥w:",
        "synonyms": {
            "mo≈ºna/nale≈ºy": ["trzeba", "warto", "da siƒô"],
            "wa≈ºny/istotny": ["kluczowy", "znaczƒÖcy", "zasadniczy"],
            "w przypadku": ["gdy", "je≈õli", "kiedy"],
        },
        "tip": "Sprawd≈∫ czy nie powtarzasz s≈Ç√≥w >3x",
        "source": "defaults"
    }


# ============================================================================
# BURSTINESS - SPRAWDZANIE I INSTRUKCJE
# ============================================================================

@dataclass
class BurstinessMetrics:
    """Metryki burstiness (zr√≥≈ºnicowania d≈Çugo≈õci zda≈Ñ)."""
    cv: float  # Wsp√≥≈Çczynnik zmienno≈õci (target > 0.40)
    short_pct: float  # % kr√≥tkich zda≈Ñ (3-8 s≈Ç√≥w) - target 20-25%
    medium_pct: float  # % ≈õrednich (10-18 s≈Ç√≥w) - target 50-60%
    long_pct: float  # % d≈Çugich (22-35 s≈Ç√≥w) - target 15-25%
    ai_pattern_pct: float  # % zda≈Ñ 15-22 s≈Ç√≥w (AI pattern) - target <30%
    is_healthy: bool
    issues: List[str]


def analyze_burstiness(text: str) -> BurstinessMetrics:
    """
    Analizuje burstiness tekstu.
    """
    # Podziel na zdania
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if len(sentences) < 3:
        return BurstinessMetrics(
            cv=0.0, short_pct=0, medium_pct=0, long_pct=0,
            ai_pattern_pct=0, is_healthy=False, 
            issues=["Za ma≈Ço zda≈Ñ do analizy"]
        )
    
    # Policz s≈Çowa w ka≈ºdym zdaniu
    lengths = [len(s.split()) for s in sentences]
    
    # Oblicz metryki
    import statistics
    mean_len = statistics.mean(lengths)
    std_len = statistics.stdev(lengths) if len(lengths) > 1 else 0
    cv = std_len / mean_len if mean_len > 0 else 0
    
    total = len(lengths)
    short_count = sum(1 for l in lengths if 3 <= l <= 8)
    medium_count = sum(1 for l in lengths if 10 <= l <= 18)
    long_count = sum(1 for l in lengths if 22 <= l <= 35)
    ai_pattern_count = sum(1 for l in lengths if 15 <= l <= 22)
    
    short_pct = (short_count / total) * 100
    medium_pct = (medium_count / total) * 100
    long_pct = (long_count / total) * 100
    ai_pattern_pct = (ai_pattern_count / total) * 100
    
    # Sprawd≈∫ problemy
    issues = []
    if cv < 0.35:
        issues.append(f"CV={cv:.2f} za niskie (target >0.40) - zdania za podobne!")
    if short_pct < 15:
        issues.append(f"Za ma≈Ço kr√≥tkich zda≈Ñ: {short_pct:.0f}% (target 20-25%)")
    if ai_pattern_pct > 40:
        issues.append(f"Za du≈ºo zda≈Ñ 15-22 s≈Ç√≥w: {ai_pattern_pct:.0f}% (AI pattern!)")
    
    is_healthy = len(issues) == 0
    
    return BurstinessMetrics(
        cv=round(cv, 3),
        short_pct=round(short_pct, 1),
        medium_pct=round(medium_pct, 1),
        long_pct=round(long_pct, 1),
        ai_pattern_pct=round(ai_pattern_pct, 1),
        is_healthy=is_healthy,
        issues=issues
    )


def get_burstiness_instructions(previous_batch_text: str = None) -> Dict[str, any]:
    """
    Generuje instrukcje burstiness dla GPT.
    
    Args:
        previous_batch_text: Tekst poprzedniego batcha (opcjonalnie, do analizy)
        
    Returns:
        Dict z instrukcjami i metrykami
    """
    base_instruction = {
        "critical": True,
        "what": "BURSTINESS = zr√≥≈ºnicowanie d≈Çugo≈õci zda≈Ñ",
        "why": "Monotonne zdania 15-20 s≈Ç√≥w = wykrycie AI!",
        "target_cv": ">0.40",
        "distribution": {
            "short_3_8_words": "20-25%",
            "medium_10_18_words": "50-60%",
            "long_22_35_words": "15-25%"
        },
        "example_sequence": "5, 18, 8, 25, 12, 6, 30, 14 s≈Ç√≥w",
        "avoid": "‚ùå NIE PISZ wszystkich zda≈Ñ 15-22 s≈Ç√≥w!"
    }
    
    # Je≈õli mamy poprzedni batch, analizuj go
    if previous_batch_text:
        metrics = analyze_burstiness(previous_batch_text)
        
        if not metrics.is_healthy:
            base_instruction["previous_batch_analysis"] = {
                "cv": metrics.cv,
                "short_pct": metrics.short_pct,
                "issues": metrics.issues,
                "fix_instruction": "‚ö†Ô∏è Poprzedni batch ma problemy z burstiness - POPRAW!"
            }
    
    return base_instruction


# ============================================================================
# G≈Å√ìWNA FUNKCJA - PE≈ÅNE INSTRUKCJE HUMANIZACJI
# ============================================================================

def get_humanization_instructions(
    main_keyword: str,
    h2_titles: List[str] = None,
    previous_batch_text: str = None,
    overused_words: List[str] = None
) -> Dict[str, any]:
    """
    Generuje kompletne instrukcje humanizacji dla GPT.
    
    Args:
        main_keyword: G≈Ç√≥wna fraza kluczowa
        h2_titles: Lista H2 (opcjonalnie)
        previous_batch_text: Poprzedni batch do analizy (opcjonalnie)
        overused_words: Nadu≈ºywane s≈Çowa (opcjonalnie)
        
    Returns:
        Dict z pe≈Çnymi instrukcjami humanizacji
    """
    return {
        "version": "v40.0",
        
        # Kr√≥tkie zdania
        "short_sentences": get_dynamic_short_sentences(
            main_keyword, h2_titles, count=8
        ),
        
        # Burstiness
        "burstiness": get_burstiness_instructions(previous_batch_text),
        
        # Synonimy
        "synonyms": get_synonym_instructions(overused_words),
        
        # AI patterns do unikania
        "avoid_ai_patterns": {
            "instruction": "‚ùå UNIKAJ tych fraz (typowe AI):",
            "patterns": {
                "warto podkre≈õliƒá": "‚Üí usu≈Ñ lub 'Zwr√≥ƒá uwagƒô:'",
                "nale≈ºy pamiƒôtaƒá": "‚Üí 'Pamiƒôtaj:' lub usu≈Ñ",
                "w kontek≈õcie": "‚Üí 'przy', 'podczas'",
                "istotne jest": "‚Üí 'Wa≈ºne:'",
                "kluczowym aspektem jest": "‚Üí usu≈Ñ ca≈Ço≈õƒá",
                "warto zauwa≈ºyƒá": "‚Üí usu≈Ñ",
                "nie bez znaczenia jest": "‚Üí 'Wa≈ºne:'",
            }
        },
        
        # Styl
        "style_tips": {
            "instruction": "Pisz jak ekspert rozmawiajƒÖcy ze znajomym",
            "tips": [
                "U≈ºywaj pyta≈Ñ retorycznych",
                "Nie ka≈ºde zdanie musi byƒá 'mƒÖdre'",
                "Kr√≥tkie zdania tw√≥rz SAM z kontekstu akapitu ‚Äî nie kopiuj gotowych fraz",
                "Mieszaj zdania proste ze z≈Ço≈ºonymi"
            ]
        }
    }


# ============================================================================
# EXPORTS
# ============================================================================

__all__ = [
    # G≈Ç√≥wne funkcje
    'get_dynamic_short_sentences',
    'get_synonym_instructions',
    'get_burstiness_instructions',
    'get_humanization_instructions',
    
    # Funkcje pomocnicze
    'detect_topic_domain',
    'analyze_burstiness',
    'get_synonyms_for_word',
    'get_synonyms_batch',  # üÜï v40.1
    
    # üÜï v41.0: Sentence Splitter
    'split_long_sentences',
    'suggest_sentence_splits',
    
    # Klasy
    'BurstinessMetrics',
    
    # Sta≈Çe
    'CONTEXTUAL_SYNONYMS',
    'TOPIC_DOMAIN_CONFIG',
    'SHORT_SENTENCE_GRAMMAR_PATTERNS',
    
    # Status integracji
    'SYNONYM_SERVICE_AVAILABLE',  # üÜï v40.1
]


# ============================================================================
# TEST / DEMO
# ============================================================================

if __name__ == "__main__":
    # Test detekcji domeny
    print("=" * 60)
    print("TEST: Detekcja domeny")
    print("=" * 60)
    
    test_cases = [
        ("ubezw≈Çasnowolnienie czƒô≈õciowe", ["Przes≈Çanki prawne", "Procedura sƒÖdowa"]),
        ("terapia integracji sensorycznej", ["ƒÜwiczenia dla dzieci"]),
        ("rozliczenie podatku PIT", ["Ulgi podatkowe"]),
        ("programowanie w Python", ["Podstawy kodu"]),
    ]
    
    for main_kw, h2s in test_cases:
        domain = detect_topic_domain(main_kw, h2s)
        print(f"'{main_kw}' ‚Üí {domain}")
    
    print("\n" + "=" * 60)
    print("TEST: Kr√≥tkie zdania dla tematu prawnego")
    print("=" * 60)
    
    result = get_dynamic_short_sentences(
        "ubezw≈Çasnowolnienie ca≈Çkowite",
        ["Procedura sƒÖdowa", "Skutki prawne"]
    )
    print(f"Domena: {result['domain']}")
    print("Instrukcja (fragment):")
    print(result['instruction'][:300])
    print("...")
    print(f"Wzorce gramatyczne: {len(result['grammar_patterns'])}")
    print(f"Podpowiedzi domenowe: {len(result['context_hints'])}")
    
    print("\n" + "=" * 60)
    print("TEST: Analiza burstiness")
    print("=" * 60)
    
    test_text = """
    Ubezw≈Çasnowolnienie to powa≈ºna decyzja. SƒÖd orzeka. Wymaga to odpowiednich 
    przes≈Çanek prawnych okre≈õlonych w kodeksie cywilnym. To wa≈ºne. Procedura 
    jest skomplikowana i wymaga udzia≈Çu bieg≈Çych psychiatr√≥w oraz psycholog√≥w 
    w celu oceny stanu zdrowia osoby, kt√≥ra ma byƒá ubezw≈Çasnowolniona. 
    Termin mija. Nale≈ºy pamiƒôtaƒá o terminach.
    """
    
    metrics = analyze_burstiness(test_text)
    print(f"CV: {metrics.cv}")
    print(f"Kr√≥tkie: {metrics.short_pct}%")
    print(f"≈örednie: {metrics.medium_pct}%")
    print(f"D≈Çugie: {metrics.long_pct}%")
    print(f"AI pattern: {metrics.ai_pattern_pct}%")
    print(f"Zdrowe: {metrics.is_healthy}")
    if metrics.issues:
        print(f"Problemy: {metrics.issues}")
    
    # üÜï v40.1: Test integracji z synonym_service
    print("\n" + "=" * 60)
    print("TEST: Synonimy (v40.1 - z integracjƒÖ synonym_service)")
    print("=" * 60)
    print(f"SYNONYM_SERVICE_AVAILABLE: {SYNONYM_SERVICE_AVAILABLE}")
    
    test_words = ["mo≈ºna", "wa≈ºny", "procedura", "ubezw≈Çasnowolnienie"]
    for word in test_words:
        syns = get_synonyms_for_word(word, context="prawo")
        source = "local" if word in CONTEXTUAL_SYNONYMS else ("external" if syns else "none")
        print(f"'{word}' ‚Üí {syns[:3] if syns else '(brak)'} [source: {source}]")
    
    print("\n" + "=" * 60)
    print("TEST: Batch synonym fetch")
    print("=" * 60)
    
    batch_result = get_synonyms_batch(["mo≈ºna", "nale≈ºy", "sƒÖd"], context="prawo")
    for word, syns in batch_result.items():
        print(f"'{word}' ‚Üí {syns[:3]}")

    # üÜï v41.0: Test Sentence Splitter
    print("\n" + "=" * 60)
    print("TEST: Sentence Splitter v41.0")
    print("=" * 60)
    
    test_long_text = """Ubezw≈Çasnowolnienie ca≈Çkowite jest instytucjƒÖ prawa cywilnego, kt√≥ra ma na celu ochronƒô os√≥b niezdolnych do samodzielnego kierowania swoim postƒôpowaniem, jednak jej zastosowanie wymaga spe≈Çnienia ≈õci≈õle okre≈õlonych przes≈Çanek ustawowych. SƒÖd okrƒôgowy rozpatruje wniosek o ubezw≈Çasnowolnienie w postƒôpowaniu nieprocesowym, ale przed wydaniem postanowienia konieczne jest przeprowadzenie badania przez bieg≈Çych psychiatr√≥w oraz psycholog√≥w klinicznych. Procedura ta trwa zazwyczaj od kilku miesiƒôcy do nawet roku, poniewa≈º wymaga zgromadzenia obszernej dokumentacji medycznej oraz przeprowadzenia szczeg√≥≈Çowych bada≈Ñ stanu zdrowia psychicznego osoby, kt√≥rej dotyczy wniosek. Kr√≥tkie zdanie. Kolejne d≈Çugie zdanie o prawie rodzinnym i opieku≈Ñczym, kt√≥re reguluje kwestie kurateli nad osobƒÖ ubezw≈ÇasnowolnionƒÖ czƒô≈õciowo; opiekun prawny natomiast jest powo≈Çywany w przypadku ubezw≈Çasnowolnienia ca≈Çkowitego."""
    
    result = split_long_sentences(test_long_text, threshold=25, max_splits=4)
    print(f"Znaleziono d≈Çugich zda≈Ñ: {result['stats']['long_sentences_found']}")
    print(f"Podzielono: {result['stats']['sentences_split']}")
    print()
    for ba in result['before_after']:
        print(f"  TYP: {ba['split_type']} (tier {ba['tier']})")
        print(f"  PRZED: {ba['before'][:100]}...")
        print(f"  PO:    {ba['after'][:100]}...")
        print()
