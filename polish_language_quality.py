"""
===============================================================================
üáµüá± POLISH LANGUAGE QUALITY v23.0 - Kontrola Jako≈õci Jƒôzyka Polskiego
===============================================================================
Modu≈Ç sprawdzajƒÖcy:
1. Kolokacje polskie (naturalne po≈ÇƒÖczenia wyraz√≥w)
2. Powt√≥rzenia leksykalne
3. Sp√≥jno≈õƒá rejestru stylistycznego
4. Szyk zdania (monotonno≈õƒá)
5. Typowe b≈Çƒôdy AI w polskim

Autorzy: Opracowano na podstawie:
- Wielki S≈Çownik Jƒôzyka Polskiego
- Nowy S≈Çownik Poprawnej Polszczyzny PWN
- Praktyczny S≈Çownik Wsp√≥≈Çczesnej Polszczyzny
===============================================================================
"""

import re
from typing import Dict, List, Any, Tuple, Set
from collections import Counter, defaultdict
from dataclasses import dataclass, field
import spacy

# ================================================================
# üß† Wsp√≥≈Çdzielony model spaCy
# ================================================================
try:
    from shared_nlp import get_nlp
    nlp = get_nlp()
except ImportError:
    # Fallback - ≈Çaduj lokalnie
    import spacy
    try:
        nlp = spacy.load("pl_core_news_md")
    except OSError:
        from spacy.cli import download
        download("pl_core_news_md")
        nlp = spacy.load("pl_core_news_md")


# ================================================================
# üìö ROZSZERZONA LISTA TRANSITION WORDS (z kategoryzacjƒÖ funkcjonalnƒÖ)
# ================================================================
TRANSITION_WORDS_CATEGORIZED = {
    "dodawanie": [
        "r√≥wnie≈º", "tak≈ºe", "ponadto", "dodatkowo", "co wiƒôcej",
        "opr√≥cz tego", "poza tym", "a tak≈ºe", "jak r√≥wnie≈º", "przy czym",
        "jednocze≈õnie", "zarazem", "w dodatku", "na dodatek", "i"
    ],
    "kontrast": [
        "jednak", "jednak≈ºe", "natomiast", "ale", "lecz", "aczkolwiek",
        "z drugiej strony", "mimo to", "niemniej", "tymczasem",
        "przeciwnie", "w przeciwie≈Ñstwie do", "choƒá", "chocia≈º", "wprawdzie"
    ],
    "przyczyna": [
        "poniewa≈º", "bowiem", "albowiem", "gdy≈º", "jako ≈ºe",
        "z tego powodu", "z uwagi na", "ze wzglƒôdu na", "dlatego ≈ºe",
        "skoro", "w zwiƒÖzku z"
    ],
    "skutek": [
        "dlatego", "zatem", "wiƒôc", "tote≈º", "stƒÖd", "wobec tego",
        "w efekcie", "w rezultacie", "w konsekwencji", "skutkiem tego",
        "tym samym", "przeto", "w zwiƒÖzku z tym"
    ],
    "czas_sekwencja": [
        "najpierw", "nastƒôpnie", "potem", "p√≥≈∫niej", "wcze≈õniej",
        "uprzednio", "w√≥wczas", "dotychczas", "tymczasem", "na poczƒÖtku",
        "na koniec", "w ko≈Ñcu", "po pierwsze", "po drugie", "po trzecie",
        "finalnie", "ostatecznie", "wreszcie"
    ],
    "przyklady": [
        "na przyk≈Çad", "przyk≈Çadowo", "miƒôdzy innymi", "m.in.", "np.",
        "chocia≈ºby", "choƒáby", "jak choƒáby", "dla przyk≈Çadu",
        "we≈∫my pod uwagƒô", "rozwa≈ºmy", "wyobra≈∫my sobie"
    ],
    "podsumowanie": [
        "podsumowujƒÖc", "reasumujƒÖc", "w skr√≥cie", "kr√≥tko m√≥wiƒÖc",
        "og√≥lnie rzecz biorƒÖc", "jednym s≈Çowem", "w konkluzji",
        "konkludujƒÖc", "zatem", "tak wiƒôc", "s≈Çowem"
    ],
    "emfaza": [
        "przede wszystkim", "szczeg√≥lnie", "zw≈Çaszcza", "w szczeg√≥lno≈õci",
        "g≈Ç√≥wnie", "nade wszystko", "co najwa≈ºniejsze", "kluczowe jest",
        "istotne jest", "warto podkre≈õliƒá", "nale≈ºy zauwa≈ºyƒá"
    ],
    "warunek": [
        "je≈õli", "je≈ºeli", "o ile", "pod warunkiem ≈ºe", "w przypadku gdy",
        "gdyby", "w razie", "chyba ≈ºe", "byleby", "byle"
    ],
    "porownanie": [
        "podobnie", "analogicznie", "tak samo", "w podobny spos√≥b",
        "na podobnej zasadzie", "por√≥wnywalnie", "identycznie",
        "w przeciwie≈Ñstwie", "inaczej ni≈º", "odmiennie"
    ]
}

# P≈Çaska lista dla kompatybilno≈õci wstecznej
ALL_TRANSITION_WORDS = []
for category, words in TRANSITION_WORDS_CATEGORIZED.items():
    ALL_TRANSITION_WORDS.extend(words)
ALL_TRANSITION_WORDS = list(set(ALL_TRANSITION_WORDS))


# ================================================================
# üö´ ROZSZERZONA LISTA BANNED PHRASES
# ================================================================
BANNED_PHRASES_EXTENDED = {
    "puste_intensyfikatory": [
        "niezwykle istotny", "niezmiernie wa≈ºny", "absolutnie kluczowy",
        "fundamentalnie istotny", "szczeg√≥lnie znaczƒÖcy", "wyjƒÖtkowo wa≈ºny",
        "nadzwyczaj istotny", "szalenie wa≈ºny"
    ],
    "pseudo_empatia_ai": [
        "doskonale rozumiemy", "zdajemy sobie sprawƒô",
        "mamy ≈õwiadomo≈õƒá", "jeste≈õmy przekonani",
        "rozumiemy twoje obawy", "wiemy jak siƒô czujesz",
        "doceniamy twoje zainteresowanie"
    ],
    "nadmierna_formalnosc": [
        "niniejszy artyku≈Ç", "przedmiotowe zagadnienie",
        "powy≈ºsze rozwa≈ºania", "poni≈ºsze informacje",
        "niniejszym informujemy", "uprzejmie informujemy",
        "majƒÖc na uwadze powy≈ºsze"
    ],
    "sztuczne_przejscia": [
        "przechodzƒÖc do kolejnego aspektu", "warto w tym miejscu zauwa≈ºyƒá",
        "nie spos√≥b nie wspomnieƒá", "godnym uwagi jest fakt",
        "w tym kontek≈õcie warto", "analizujƒÖc dalej"
    ],
    "redundancja": [
        "bardzo wa≈ºne i istotne", "nowe i nowatorskie",
        "r√≥≈ºne i rozmaite", "pe≈Çny i kompletny",
        "szybki i sprawny", "jasny i czytelny"
    ],
    "pleonazmy": [
        "cofnƒÖƒá siƒô wstecz", "kontynuowaƒá dalej",
        "powr√≥ciƒá z powrotem", "wzajemna wsp√≥≈Çpraca",
        "spadek w d√≥≈Ç", "wzrost w g√≥rƒô",
        "przysz≈Ça przysz≈Ço≈õƒá", "wsp√≥lnie razem"
    ],
    "typowe_ai_openers": [
        "w dzisiejszych czasach", "w obecnych czasach",
        "w dobie", "w erze", "≈ºyjemy w czasach",
        "warto wiedzieƒá", "warto pamiƒôtaƒá",
        "jak wiadomo", "powszechnie wiadomo",
        "ka≈ºdy z nas", "wszyscy wiemy",
        "nie ulega wƒÖtpliwo≈õci", "nie da siƒô ukryƒá",
        "coraz wiƒôcej os√≥b", "coraz czƒô≈õciej",
        "z ca≈ÇƒÖ pewno≈õciƒÖ", "bez wƒÖtpienia"
    ],
    "section_openers": [
        "dlatego", "ponadto", "dodatkowo", "tym samym",
        "warto", "nale≈ºy", "trzeba", "wystarczy"
    ]
}


# ================================================================
# üîó KOLOKACJE POLSKIE (najczƒôstsze b≈Çƒôdne po≈ÇƒÖczenia)
# ================================================================
INCORRECT_COLLOCATIONS = {
    # "b≈Çƒôdna fraza": "poprawna fraza"
    "robiƒá decyzjƒô": "podejmowaƒá decyzjƒô",
    "dawaƒá uwagƒô": "zwracaƒá uwagƒô",
    "braƒá pod rozwa≈ºenie": "braƒá pod uwagƒô",
    "mieƒá opiniƒô": "wyra≈ºaƒá opiniƒô",
    "graƒá rolƒô": "odgrywaƒá rolƒô",
    "silne przekonanie": "g≈Çƒôbokie przekonanie",
    "wysoki stopie≈Ñ": "wysoki poziom",
    "robiƒá wp≈Çyw": "wywieraƒá wp≈Çyw",
    "dawaƒá przyk≈Çad": "stanowiƒá przyk≈Çad",
    "mieƒá miejsce": "odbywaƒá siƒô",  # kontekstowe
    "robiƒá b≈ÇƒÖd": "pope≈Çniaƒá b≈ÇƒÖd",
    "stawiaƒá pytanie": "zadawaƒá pytanie",
    "dawaƒá odpowied≈∫": "udzielaƒá odpowiedzi",
    "robiƒá postƒôp": "czyniƒá postƒôpy",
    "braƒá odpowiedzialno≈õƒá": "ponosiƒá odpowiedzialno≈õƒá",
    "silny argument": "mocny argument",
    "du≈ºy sukces": "wielki sukces",
    "robiƒá wysi≈Çek": "podejmowaƒá wysi≈Çek",
    "wielka ilo≈õƒá": "du≈ºa ilo≈õƒá",
    "ma≈Ça ilo≈õƒá": "niewielka ilo≈õƒá",
    "robiƒá wra≈ºenie": "sprawiaƒá wra≈ºenie",
}

# Poprawne kolokacje (do promocji)
PREFERRED_COLLOCATIONS = [
    "podejmowaƒá decyzjƒô", "zwracaƒá uwagƒô", "braƒá pod uwagƒô",
    "odgrywaƒá rolƒô", "wywieraƒá wp≈Çyw", "pope≈Çniaƒá b≈ÇƒÖd",
    "zadawaƒá pytanie", "udzielaƒá odpowiedzi", "czyniƒá postƒôpy",
    "ponosiƒá odpowiedzialno≈õƒá", "sprawiaƒá wra≈ºenie"
]


# ================================================================
# üìä STRUKTURY DANYCH
# ================================================================
@dataclass
class LanguageQualityResult:
    """Wynik analizy jako≈õci jƒôzykowej."""
    score: float  # 0-100
    issues: List[Dict[str, Any]] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        return {
            "score": round(self.score, 1),
            "issues_count": len(self.issues),
            "issues": self.issues[:10],  # Limit
            "warnings": self.warnings[:5],
            "recommendations": self.recommendations[:5],
            "metrics": self.metrics,
            "status": "GOOD" if self.score >= 70 else ("FAIR" if self.score >= 50 else "POOR")
        }


# ================================================================
# üîç FUNKCJE ANALIZY
# ================================================================

def check_collocations(text: str) -> Tuple[List[Dict], float]:
    """
    Sprawdza b≈Çƒôdne kolokacje w tek≈õcie.
    
    Returns:
        Tuple[lista b≈Çƒôd√≥w, score 0-1]
    """
    text_lower = text.lower()
    errors = []
    
    for incorrect, correct in INCORRECT_COLLOCATIONS.items():
        if incorrect in text_lower:
            # Znajd≈∫ kontekst
            idx = text_lower.find(incorrect)
            context = text[max(0, idx-20):min(len(text), idx+len(incorrect)+20)]
            
            errors.append({
                "type": "COLLOCATION_ERROR",
                "found": incorrect,
                "suggested": correct,
                "context": f"...{context}..."
            })
    
    # Score: 1.0 je≈õli brak b≈Çƒôd√≥w, maleje z ka≈ºdym b≈Çƒôdem
    score = max(0, 1 - len(errors) * 0.15)
    
    return errors, score


def check_lexical_repetitions(text: str, window_size: int = 3) -> Tuple[List[Dict], float]:
    """
    Sprawdza nadmierne powt√≥rzenia leksykalne.
    
    Args:
        text: Tekst do analizy
        window_size: Ile zda≈Ñ wstecz sprawdzaƒá
    
    Returns:
        Tuple[lista powt√≥rze≈Ñ, score 0-1]
    """
    doc = nlp(text[:20000])
    
    # Zbierz rzeczowniki i czasowniki (content words)
    sentences = list(doc.sents)
    repetitions = []
    
    for i, sent in enumerate(sentences):
        if i < window_size:
            continue
        
        # S≈Çowa w bie≈ºƒÖcym zdaniu
        current_words = set(
            token.lemma_.lower() for token in sent 
            if token.pos_ in ["NOUN", "VERB"] and len(token.text) > 3
        )
        
        # S≈Çowa w poprzednich zdaniach
        prev_words = Counter()
        for j in range(max(0, i - window_size), i):
            for token in sentences[j]:
                if token.pos_ in ["NOUN", "VERB"] and len(token.text) > 3:
                    prev_words[token.lemma_.lower()] += 1
        
        # Znajd≈∫ powt√≥rzenia
        for word in current_words:
            if prev_words[word] >= 2:  # S≈Çowo by≈Ço 2+ razy w ostatnich zdaniach
                repetitions.append({
                    "type": "LEXICAL_REPETITION",
                    "word": word,
                    "count_in_window": prev_words[word] + 1,
                    "sentence_index": i
                })
    
    # Score
    score = max(0, 1 - len(repetitions) * 0.1)
    
    return repetitions, score


def check_register_consistency(text: str) -> Tuple[List[Dict], float, str]:
    """
    Sprawdza sp√≥jno≈õƒá rejestru stylistycznego.
    
    Returns:
        Tuple[lista problem√≥w, score 0-1, wykryty rejestr]
    """
    # Markery rejestr√≥w
    FORMAL_MARKERS = [
        "niniejszy", "przedmiotowy", "powy≈ºszy", "uprzejmie",
        "w zwiƒÖzku z powy≈ºszym", "majƒÖc na uwadze"
    ]
    
    COLLOQUIAL_MARKERS = [
        "fajny", "super", "mega", "w sumie", "og√≥lnie",
        "no i", "tak naprawdƒô", "jakby", "w og√≥le"
    ]
    
    SCIENTIFIC_MARKERS = [
        "hipoteza", "metodologia", "empiryczny", "teoretyczny",
        "analiza wskazuje", "badania dowodzƒÖ", "korelacja"
    ]
    
    text_lower = text.lower()
    
    formal_count = sum(1 for m in FORMAL_MARKERS if m in text_lower)
    colloquial_count = sum(1 for m in COLLOQUIAL_MARKERS if m in text_lower)
    scientific_count = sum(1 for m in SCIENTIFIC_MARKERS if m in text_lower)
    
    # Okre≈õl dominujƒÖcy rejestr
    counts = {
        "formalny": formal_count,
        "potoczny": colloquial_count,
        "naukowy": scientific_count
    }
    dominant = max(counts, key=counts.get) if max(counts.values()) > 0 else "neutralny"
    
    # Wykryj mieszanie rejestr√≥w
    issues = []
    
    if formal_count > 0 and colloquial_count > 0:
        issues.append({
            "type": "REGISTER_MIXING",
            "message": "Mieszanie rejestru formalnego z potocznym",
            "formal_markers": formal_count,
            "colloquial_markers": colloquial_count
        })
    
    if scientific_count > 0 and colloquial_count > 0:
        issues.append({
            "type": "REGISTER_MIXING",
            "message": "Mieszanie rejestru naukowego z potocznym",
            "scientific_markers": scientific_count,
            "colloquial_markers": colloquial_count
        })
    
    # Score
    if len(issues) == 0:
        score = 1.0
    elif len(issues) == 1:
        score = 0.7
    else:
        score = 0.4
    
    return issues, score, dominant


def check_sentence_variety(text: str) -> Tuple[List[Dict], float]:
    """
    Sprawdza r√≥≈ºnorodno≈õƒá struktur sk≈Çadniowych.
    
    Returns:
        Tuple[lista problem√≥w, score 0-1]
    """
    doc = nlp(text[:15000])
    sentences = list(doc.sents)
    
    if len(sentences) < 5:
        return [], 1.0
    
    issues = []
    
    # 1. Sprawd≈∫ monotoniƒô poczƒÖtk√≥w zda≈Ñ
    starters = [sent[0].text.lower() if len(sent) > 0 else "" for sent in sentences]
    starter_counts = Counter(starters)
    
    for starter, count in starter_counts.items():
        ratio = count / len(sentences)
        if ratio > 0.2 and count > 2:  # >20% zda≈Ñ zaczyna siƒô tak samo
            issues.append({
                "type": "MONOTONOUS_STARTERS",
                "starter": starter,
                "count": count,
                "percentage": round(ratio * 100, 1)
            })
    
    # 2. Sprawd≈∫ monotoniƒô szyku (SVO)
    svo_count = 0
    for sent in sentences:
        tokens = list(sent)
        if len(tokens) >= 3:
            # Uproszczona heurystyka: NOUN + VERB + NOUN
            pos_pattern = [t.pos_ for t in tokens[:5]]
            if "NOUN" in pos_pattern[:2] and "VERB" in pos_pattern[1:4]:
                svo_count += 1
    
    svo_ratio = svo_count / len(sentences)
    if svo_ratio > 0.7:  # >70% zda≈Ñ to SVO
        issues.append({
            "type": "MONOTONOUS_WORD_ORDER",
            "svo_percentage": round(svo_ratio * 100, 1),
            "recommendation": "Urozmaiƒá szyk zdania - u≈ºyj inwersji, konstrukcji z emfazƒÖ"
        })
    
    # 3. Sprawd≈∫ r√≥≈ºnorodno≈õƒá d≈Çugo≈õci zda≈Ñ
    lengths = [len(list(sent)) for sent in sentences]
    if lengths:
        avg_len = sum(lengths) / len(lengths)
        variance = sum((l - avg_len) ** 2 for l in lengths) / len(lengths)
        
        if variance < 10:  # Zbyt podobne d≈Çugo≈õci
            issues.append({
                "type": "MONOTONOUS_SENTENCE_LENGTH",
                "variance": round(variance, 2),
                "recommendation": "Mieszaj zdania kr√≥tkie (5-10 s≈Ç√≥w) z d≈Çu≈ºszymi (15-25 s≈Ç√≥w)"
            })
    
    # Score
    score = max(0, 1 - len(issues) * 0.2)
    
    return issues, score


def check_banned_phrases(text: str) -> Tuple[List[Dict], float]:
    """
    Sprawdza obecno≈õƒá zakazanych fraz AI.
    
    Returns:
        Tuple[lista znalezionych fraz, score 0-1]
    """
    text_lower = text.lower()
    found = []
    
    for category, phrases in BANNED_PHRASES_EXTENDED.items():
        for phrase in phrases:
            if phrase in text_lower:
                idx = text_lower.find(phrase)
                context = text[max(0, idx-10):min(len(text), idx+len(phrase)+10)]
                
                found.append({
                    "type": "BANNED_PHRASE",
                    "category": category,
                    "phrase": phrase,
                    "context": f"...{context}..."
                })
    
    # Score
    score = max(0, 1 - len(found) * 0.1)
    
    return found, score


def check_transition_words_usage(text: str) -> Tuple[Dict, float]:
    """
    Analizuje u≈ºycie s≈Ç√≥w ≈ÇƒÖczƒÖcych z podzia≈Çem na kategorie.
    
    Returns:
        Tuple[analiza, score 0-1]
    """
    text_lower = text.lower()
    
    usage = {}
    total_found = 0
    
    for category, words in TRANSITION_WORDS_CATEGORIZED.items():
        count = sum(1 for word in words if word in text_lower)
        usage[category] = count
        total_found += count
    
    # Sprawd≈∫ balans kategorii
    issues = []
    
    # Za du≈ºo "dodawanie" w stosunku do innych
    if usage.get("dodawanie", 0) > total_found * 0.4 and total_found > 5:
        issues.append("Zbyt wiele s≈Ç√≥w ≈ÇƒÖczƒÖcych typu 'dodawanie' (r√≥wnie≈º, tak≈ºe, ponadto)")
    
    # Brak kontrastu
    if usage.get("kontrast", 0) == 0 and total_found > 5:
        issues.append("Brak s≈Ç√≥w kontrastujƒÖcych (jednak, natomiast, ale) - tekst mo≈ºe byƒá monotonny")
    
    # Za du≈ºo skutek/przyczyna razem
    if usage.get("skutek", 0) > total_found * 0.3:
        issues.append("Nadmierne u≈ºycie s≈Ç√≥w wyra≈ºajƒÖcych skutek (dlatego, zatem, wiƒôc)")
    
    analysis = {
        "by_category": usage,
        "total": total_found,
        "issues": issues,
        "balance": "OK" if len(issues) == 0 else "UNBALANCED"
    }
    
    # Score
    score = max(0.5, 1 - len(issues) * 0.15)
    
    return analysis, score


# ================================================================
# üéØ G≈Å√ìWNA FUNKCJA ANALIZY
# ================================================================
def analyze_polish_quality(text: str) -> LanguageQualityResult:
    """
    G≈Ç√≥wna funkcja - kompleksowa analiza jako≈õci jƒôzyka polskiego.
    
    Args:
        text: Tekst do analizy
    
    Returns:
        LanguageQualityResult z pe≈ÇnƒÖ analizƒÖ
    """
    if not text or len(text.strip()) < 100:
        return LanguageQualityResult(
            score=0,
            issues=[{"type": "TEXT_TOO_SHORT", "message": "Tekst zbyt kr√≥tki do analizy"}],
            warnings=["Tekst musi mieƒá minimum 100 znak√≥w"],
            recommendations=[],
            metrics={}
        )
    
    all_issues = []
    all_warnings = []
    all_recommendations = []
    scores = []
    
    # 1. Kolokacje
    collocation_issues, collocation_score = check_collocations(text)
    all_issues.extend(collocation_issues)
    scores.append(collocation_score * 0.25)  # Waga 25%
    
    if collocation_issues:
        all_recommendations.append(
            f"Popraw kolokacje: {collocation_issues[0]['found']} ‚Üí {collocation_issues[0]['suggested']}"
        )
    
    # 2. Powt√≥rzenia leksykalne
    repetition_issues, repetition_score = check_lexical_repetitions(text)
    all_issues.extend(repetition_issues)
    scores.append(repetition_score * 0.20)  # Waga 20%
    
    if len(repetition_issues) > 3:
        all_warnings.append("Nadmierne powt√≥rzenia leksykalne - u≈ºyj synonim√≥w lub zaimk√≥w")
    
    # 3. Sp√≥jno≈õƒá rejestru
    register_issues, register_score, dominant_register = check_register_consistency(text)
    all_issues.extend(register_issues)
    scores.append(register_score * 0.15)  # Waga 15%
    
    if register_issues:
        all_recommendations.append(
            f"Ujednoliƒá rejestr stylistyczny - wykryto mieszanie rejestr√≥w"
        )
    
    # 4. R√≥≈ºnorodno≈õƒá sk≈Çadniowa
    variety_issues, variety_score = check_sentence_variety(text)
    all_issues.extend(variety_issues)
    scores.append(variety_score * 0.15)  # Waga 15%
    
    if variety_issues:
        for issue in variety_issues:
            if issue["type"] == "MONOTONOUS_STARTERS":
                all_recommendations.append(
                    f"Urozmaiƒá poczƒÖtki zda≈Ñ - {issue['percentage']}% zaczyna siƒô od '{issue['starter']}'"
                )
    
    # 5. Banned phrases
    banned_issues, banned_score = check_banned_phrases(text)
    all_issues.extend(banned_issues)
    scores.append(banned_score * 0.15)  # Waga 15%
    
    if banned_issues:
        categories = set(i["category"] for i in banned_issues)
        all_warnings.append(f"Znaleziono typowe frazy AI: {', '.join(categories)}")
    
    # 6. Transition words
    transition_analysis, transition_score = check_transition_words_usage(text)
    scores.append(transition_score * 0.10)  # Waga 10%
    
    if transition_analysis["issues"]:
        all_recommendations.extend(transition_analysis["issues"])
    
    # Oblicz ko≈Ñcowy score
    final_score = sum(scores) * 100
    
    # Metryki
    metrics = {
        "collocation_score": round(collocation_score, 2),
        "repetition_score": round(repetition_score, 2),
        "register_score": round(register_score, 2),
        "variety_score": round(variety_score, 2),
        "banned_phrases_score": round(banned_score, 2),
        "transition_score": round(transition_score, 2),
        "dominant_register": dominant_register,
        "transition_analysis": transition_analysis
    }
    
    return LanguageQualityResult(
        score=final_score,
        issues=all_issues,
        warnings=all_warnings,
        recommendations=all_recommendations,
        metrics=metrics
    )


# ================================================================
# üîß HELPER: Szybka walidacja
# ================================================================
def quick_polish_check(text: str) -> Dict[str, Any]:
    """
    Szybka walidacja - tylko najwa≈ºniejsze elementy.
    """
    result = {
        "status": "OK",
        "issues_count": 0,
        "critical": []
    }
    
    # Tylko banned phrases i kolokacje
    banned, _ = check_banned_phrases(text)
    collocations, _ = check_collocations(text)
    
    result["issues_count"] = len(banned) + len(collocations)
    
    if banned:
        result["status"] = "WARN"
        result["critical"].append(f"Frazy AI: {banned[0]['phrase']}")
    
    if collocations:
        result["status"] = "WARN"
        result["critical"].append(f"B≈Çƒôdna kolokacja: {collocations[0]['found']}")
    
    return result


# ================================================================
# üîß HELPER: Sugestie poprawy
# ================================================================
def generate_improvement_suggestions(issues: List[Dict]) -> List[str]:
    """
    Generuje konkretne sugestie poprawy na podstawie wykrytych problem√≥w.
    """
    suggestions = []
    
    for issue in issues[:5]:
        issue_type = issue.get("type", "")
        
        if issue_type == "COLLOCATION_ERROR":
            suggestions.append(
                f"Zamie≈Ñ '{issue['found']}' na '{issue['suggested']}' - poprawna kolokacja polska"
            )
        
        elif issue_type == "LEXICAL_REPETITION":
            word = issue.get("word", "")
            suggestions.append(
                f"S≈Çowo '{word}' powtarza siƒô zbyt czƒôsto - u≈ºyj synonimu lub zaimka"
            )
        
        elif issue_type == "BANNED_PHRASE":
            phrase = issue.get("phrase", "")
            category = issue.get("category", "")
            
            if category == "typowe_ai_openers":
                suggestions.append(
                    f"Usu≈Ñ '{phrase}' - to typowy marker tekstu AI. Zacznij od konkretnej informacji."
                )
            elif category == "pleonazmy":
                suggestions.append(
                    f"Usu≈Ñ pleonazm '{phrase}' - wyra≈ºenie redundantne"
                )
            else:
                suggestions.append(
                    f"Rozwa≈º usuniƒôcie '{phrase}' - mo≈ºe brzmieƒá sztucznie"
                )
        
        elif issue_type == "MONOTONOUS_STARTERS":
            suggestions.append(
                f"Urozmaiƒá poczƒÖtki zda≈Ñ - {issue.get('percentage', 0)}% zaczyna siƒô od '{issue.get('starter', '')}'"
            )
        
        elif issue_type == "REGISTER_MIXING":
            suggestions.append(
                "Ujednoliciƒá rejestr stylistyczny - nie mieszaj jƒôzyka formalnego z potocznym"
            )
    
    return suggestions
