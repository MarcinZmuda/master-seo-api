# local_court_scraper.py
# BRAJEN Legal Module - Scraper lokalnych portali orzecze≈Ñ v3.5
# Uzupe≈Çnia SAOS o orzeczenia z portali indywidualnych sƒÖd√≥w

"""
===============================================================================
üèõÔ∏è LOCAL COURT SCRAPER v3.5
===============================================================================

Scraper dla lokalnych portali orzecze≈Ñ sƒÖd√≥w powszechnych.
Portale te NIE sƒÖ zindeksowane w SAOS!

Przyk≈Çad: https://orzeczenia.warszawa.so.gov.pl

v3.5 ZMIANY:
- Prawid≈Çowy format URL wyszukiwania: /search/advanced/{keyword}/$N/{court_code}/...
- Prawid≈Çowy format URL tre≈õci: /content/{keyword}/{id}
- Kodowanie polskich znak√≥w: √≥ ‚Üí $00f3, ≈Ç ‚Üí $0142

Struktura URL:
- Wyszukiwanie: /search/advanced/{keyword}/$N/{court_code}/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/score/descending/1
- Szczeg√≥≈Çy: /details/{keyword}/{ID}
- Tre≈õƒá: /content/{keyword}/{ID}

===============================================================================
"""

import requests
from bs4 import BeautifulSoup
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime
import re
import urllib.parse


# ============================================================================
# KONFIGURACJA LOKALNYCH PORTALI
# ============================================================================

@dataclass
class LocalCourtConfig:
    """Konfiguracja scrapera lokalnych portali."""
    
    TIMEOUT: int = 15
    MAX_RESULTS: int = 20
    
    # G≈Ç√≥wne portale sƒÖd√≥w okrƒôgowych
    COURT_PORTALS: Dict[str, Dict[str, str]] = field(default_factory=lambda: {
        "warszawa.so": {
            "name": "SƒÖd Okrƒôgowy w Warszawie",
            "code": "15450500",
            "base_url": "https://orzeczenia.warszawa.so.gov.pl"
        },
        "warszawapraga.so": {
            "name": "SƒÖd Okrƒôgowy Warszawa-Praga",
            "code": "15451000",
            "base_url": "https://orzeczenia.warszawapraga.so.gov.pl"
        },
        "krakow.so": {
            "name": "SƒÖd Okrƒôgowy w Krakowie",
            "code": "15201000",
            "base_url": "https://orzeczenia.krakow.so.gov.pl"
        },
        "gdansk.so": {
            "name": "SƒÖd Okrƒôgowy w Gda≈Ñsku",
            "code": "15101500",
            "base_url": "https://orzeczenia.gdansk.so.gov.pl"
        },
        "wroclaw.so": {
            "name": "SƒÖd Okrƒôgowy we Wroc≈Çawiu",
            "code": "15502500",
            "base_url": "https://orzeczenia.wroclaw.so.gov.pl"
        },
        "poznan.so": {
            "name": "SƒÖd Okrƒôgowy w Poznaniu",
            "code": "15351000",
            "base_url": "https://orzeczenia.poznan.so.gov.pl"
        },
        "lodz.so": {
            "name": "SƒÖd Okrƒôgowy w ≈Åodzi",
            "code": "15251000",
            "base_url": "https://orzeczenia.lodz.so.gov.pl"
        },
        "katowice.so": {
            "name": "SƒÖd Okrƒôgowy w Katowicach",
            "code": "15152000",
            "base_url": "https://orzeczenia.katowice.so.gov.pl"
        },
        "lublin.so": {
            "name": "SƒÖd Okrƒôgowy w Lublinie",
            "code": "15300500",
            "base_url": "https://orzeczenia.lublin.so.gov.pl"
        },
        "szczecin.so": {
            "name": "SƒÖd Okrƒôgowy w Szczecinie",
            "code": "15551500",
            "base_url": "https://orzeczenia.szczecin.so.gov.pl"
        },
    })
    
    # v3.5: Mapa polskich znak√≥w ‚Üí kody URL
    POLISH_CHARS: Dict[str, str] = field(default_factory=lambda: {
        'ƒÖ': '$0105', 'ƒá': '$0107', 'ƒô': '$0119', '≈Ç': '$0142',
        '≈Ñ': '$0144', '√≥': '$00f3', '≈õ': '$015b', '≈∫': '$017a', '≈º': '$017c',
        'ƒÑ': '$0104', 'ƒÜ': '$0106', 'ƒò': '$0118', '≈Å': '$0141',
        '≈É': '$0143', '√ì': '$00d3', '≈ö': '$015a', '≈π': '$0179', '≈ª': '$017b'
    })


CONFIG = LocalCourtConfig()


# ============================================================================
# SCRAPER LOKALNYCH PORTALI
# ============================================================================

class LocalCourtScraper:
    """Scraper dla lokalnych portali orzecze≈Ñ v3.5."""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "pl-PL,pl;q=0.9,en;q=0.8",
        })
    
    def _encode_keyword_for_url(self, keyword: str) -> str:
        """
        v3.5: Koduje polskie znaki do formatu URL portali.
        
        Przyk≈Çady:
        - "rozw√≥d" ‚Üí "rozw$00f3d"
        - "ubezw≈Çasnowolnienie" ‚Üí "ubezw$0142asnowolnienie"
        """
        result = keyword
        for char, code in CONFIG.POLISH_CHARS.items():
            result = result.replace(char, code)
        result = result.replace(' ', '+')
        return result
    
    def search_all_portals(
        self,
        keyword: str,
        max_results: int = CONFIG.MAX_RESULTS,
        theme_phrase: Optional[str] = None
    ) -> Dict[str, Any]:
        """Przeszukuje wszystkie lokalne portale."""
        all_results = []
        errors = []
        
        results_per_portal = max(3, max_results // len(CONFIG.COURT_PORTALS))
        
        for portal_key, portal_info in CONFIG.COURT_PORTALS.items():
            try:
                portal_results = self.search_portal(
                    base_url=portal_info["base_url"],
                    keyword=keyword,
                    max_results=results_per_portal,
                    court_code=portal_info.get("code")
                )
                
                for result in portal_results.get("judgments", []):
                    result["source_portal"] = portal_info["name"]
                    result["source_url"] = portal_info["base_url"]
                
                all_results.extend(portal_results.get("judgments", []))
                
            except Exception as e:
                errors.append({"portal": portal_info["name"], "error": str(e)})
        
        all_results.sort(key=lambda x: x.get("date", ""), reverse=True)
        
        return {
            "status": "success" if all_results else "no_results",
            "keyword": keyword,
            "total_found": len(all_results),
            "judgments": all_results[:max_results],
            "errors": errors if errors else None
        }
    
    def search_portal(
        self,
        base_url: str,
        keyword: str,
        max_results: int = 10,
        theme_phrase: Optional[str] = None,
        court_code: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Przeszukuje pojedynczy portal.
        
        Przyk≈Çad URL:
            https://orzeczenia.warszawa.so.gov.pl/search/advanced/alimenty/$N/15450500/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/score/descending/1
        """
        encoded_keyword = self._encode_keyword_for_url(keyword)
        court_id = court_code or "$N"
        
        search_url = f"{base_url}/search/advanced/{encoded_keyword}/$N/{court_id}/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/score/descending/1"
        
        print(f"[LOCAL_SCRAPER] üîç URL: {search_url}")
        
        try:
            response = self.session.get(search_url, timeout=CONFIG.TIMEOUT)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            judgments = self._parse_search_results(soup, base_url, keyword)
            
            return {
                "status": "success",
                "search_url": search_url,
                "judgments": judgments[:max_results]
            }
            
        except requests.RequestException as e:
            print(f"[LOCAL_SCRAPER] ‚ùå Error: {e}")
            return {"status": "error", "error": str(e), "judgments": []}
    
    def _parse_search_results(self, soup: BeautifulSoup, base_url: str, keyword: str) -> List[Dict]:
        """Parsuje wyniki wyszukiwania z HTML."""
        results = []
        encoded_keyword = self._encode_keyword_for_url(keyword)
        
        result_links = (
            soup.select("a[href*='/details/']") or 
            soup.select("a[href*='/content/']") or
            soup.select("table.searchResults a")
        )
        
        seen_ids = set()
        
        for link in result_links[:30]:
            try:
                href = link.get("href", "")
                if not href:
                    continue
                
                judgment_id = self._extract_judgment_id(href)
                if not judgment_id or judgment_id in seen_ids:
                    continue
                
                seen_ids.add(judgment_id)
                text = link.get_text(strip=True)
                signature = self._extract_signature(text)
                
                # v3.5: Prawid≈Çowy format URL
                content_url = f"{base_url}/content/{encoded_keyword}/{judgment_id}"
                details_url = f"{base_url}/details/{encoded_keyword}/{judgment_id}"
                
                results.append({
                    "id": judgment_id,
                    "signature": signature or text[:50],
                    "date": self._extract_date_from_text(text),
                    "formatted_date": "",
                    "court": "",
                    "excerpt": "",
                    "url": content_url,
                    "details_url": details_url,
                })
                
            except Exception as e:
                print(f"[LOCAL_SCRAPER] Parse error: {e}")
                continue
        
        # Pobierz szczeg√≥≈Çy
        for result in results[:10]:
            try:
                details = self._fetch_judgment_details(base_url, result["id"], keyword)
                result.update(details)
            except Exception as e:
                print(f"[LOCAL_SCRAPER] Details error: {e}")
        
        return results
    
    def _fetch_judgment_details(self, base_url: str, judgment_id: str, keyword: str) -> Dict:
        """Pobiera szczeg√≥≈Çy orzeczenia."""
        encoded_keyword = self._encode_keyword_for_url(keyword)
        details_url = f"{base_url}/details/{encoded_keyword}/{judgment_id}"
        
        try:
            response = self.session.get(details_url, timeout=CONFIG.TIMEOUT)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            date = ""
            court = ""
            signature = ""
            
            meta_table = soup.select_one("table.metaTable") or soup.select_one(".details") or soup.select_one("table")
            if meta_table:
                for row in meta_table.select("tr"):
                    cells = row.select("td, th")
                    if len(cells) >= 2:
                        label = cells[0].get_text(strip=True).lower()
                        value = cells[1].get_text(strip=True)
                        
                        if "data" in label and ("orzeczenia" in label or "wydania" in label):
                            date = value
                        elif "sƒÖd" in label:
                            court = value
                        elif "sygnatura" in label:
                            signature = value
            
            content_url = f"{base_url}/content/{encoded_keyword}/{judgment_id}"
            excerpt = self._fetch_excerpt(content_url, keyword)
            
            return {
                "date": self._normalize_date(date),
                "formatted_date": date,
                "court": court,
                "signature": signature,
                "excerpt": excerpt,
            }
            
        except Exception as e:
            return {}
    
    def _fetch_excerpt(self, content_url: str, keyword: str, max_len: int = 300) -> str:
        """Pobiera fragment tre≈õci orzeczenia."""
        try:
            response = self.session.get(content_url, timeout=CONFIG.TIMEOUT)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            for tag in soup(["script", "style", "nav", "header", "footer"]):
                tag.decompose()
            
            text = soup.get_text(separator=" ", strip=True)
            keyword_lower = keyword.lower()
            text_lower = text.lower()
            
            pos = text_lower.find(keyword_lower)
            if pos == -1:
                return text[:max_len] + "..." if len(text) > max_len else text
            
            start = max(0, pos - max_len // 2)
            end = min(len(text), pos + max_len // 2)
            
            excerpt = text[start:end].strip()
            if start > 0:
                excerpt = "..." + excerpt
            if end < len(text):
                excerpt = excerpt + "..."
            
            return excerpt
            
        except Exception:
            return ""
    
    def _extract_judgment_id(self, href: str) -> Optional[str]:
        """WyciƒÖga ID orzeczenia z URL."""
        patterns = [
            r'/(?:details|content)/[^/]+/([^\s/]+)$',
            r'/(?:details|content)/\$N/([^\s/]+)',
            r'/([0-9]{15}_[^/\s]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, href)
            if match:
                return match.group(1)
        return None
    
    def _extract_signature(self, text: str) -> Optional[str]:
        """WyciƒÖga sygnaturƒô z tekstu."""
        patterns = [
            r'\b([IVX]+\s+[A-Za-z]{1,4}\s+\d+/\d{2,4})\b',
            r'(?:sygn\.?\s*(?:akt\s*)?:?\s*)([IVX]+\s+[A-Za-z]+\s+\d+/\d+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        return None
    
    def _extract_date_from_text(self, text: str) -> str:
        """WyciƒÖga datƒô z tekstu."""
        patterns = [
            r'(\d{4}-\d{2}-\d{2})',
            r'(\d{1,2})[.\-/](\d{1,2})[.\-/](\d{4})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                if len(match.groups()) == 1:
                    return match.group(1)
                elif len(match.groups()) == 3:
                    g = match.groups()
                    if g[0].isdigit() and len(g[0]) == 4:
                        return f"{g[0]}-{g[1].zfill(2)}-{g[2].zfill(2)}"
                    else:
                        return f"{g[2]}-{g[1].zfill(2)}-{g[0].zfill(2)}"
        return ""
    
    def _normalize_date(self, date_str: str) -> str:
        """Normalizuje datƒô do formatu YYYY-MM-DD."""
        if not date_str:
            return ""
        
        if re.match(r'\d{4}-\d{2}-\d{2}', date_str):
            return date_str
        
        months = {
            'stycznia': '01', 'lutego': '02', 'marca': '03', 'kwietnia': '04',
            'maja': '05', 'czerwca': '06', 'lipca': '07', 'sierpnia': '08',
            'wrze≈õnia': '09', 'pa≈∫dziernika': '10', 'listopada': '11', 'grudnia': '12'
        }
        
        pattern = r'(\d{1,2})\s+(\w+)\s+(\d{4})'
        match = re.search(pattern, date_str)
        if match:
            day = match.group(1).zfill(2)
            month = months.get(match.group(2).lower(), '01')
            year = match.group(3)
            return f"{year}-{month}-{day}"
        
        return date_str


# ============================================================================
# INTEGRACJA Z SAOS
# ============================================================================

def search_judgments_combined(
    keyword: str,
    include_saos: bool = True,
    include_local: bool = True,
    max_results: int = 20,
    **kwargs
) -> Dict[str, Any]:
    """Przeszukuje zar√≥wno SAOS jak i lokalne portale."""
    all_judgments = []
    sources = []
    
    if include_saos:
        try:
            from saos_client import search_judgments as saos_search
            saos_results = saos_search(keyword, max_results=max_results // 2, **kwargs)
            
            if saos_results.get("status") == "success":
                for j in saos_results.get("judgments", []):
                    j["source"] = "SAOS"
                all_judgments.extend(saos_results.get("judgments", []))
                sources.append("SAOS")
        except ImportError:
            print("[COMBINED] SAOS client not available")
        except Exception as e:
            print(f"[COMBINED] SAOS error: {e}")
    
    if include_local:
        try:
            scraper = LocalCourtScraper()
            local_results = scraper.search_all_portals(keyword, max_results=max_results // 2)
            
            if local_results.get("judgments"):
                for j in local_results.get("judgments", []):
                    j["source"] = f"Portal: {j.get('source_portal', 'lokalny')}"
                all_judgments.extend(local_results.get("judgments", []))
                sources.append("Lokalne portale")
        except Exception as e:
            print(f"[COMBINED] Local scraper error: {e}")
    
    # Deduplikacja
    seen_signatures = set()
    unique_judgments = []
    for j in all_judgments:
        sig = j.get("signature", "")
        if sig and sig not in seen_signatures:
            seen_signatures.add(sig)
            unique_judgments.append(j)
        elif not sig:
            unique_judgments.append(j)
    
    unique_judgments.sort(key=lambda x: x.get("date", ""), reverse=True)
    
    return {
        "status": "success" if unique_judgments else "no_results",
        "keyword": keyword,
        "total_found": len(unique_judgments),
        "judgments": unique_judgments[:max_results],
        "sources": sources
    }


# ============================================================================
# SINGLETON & HELPERS
# ============================================================================

_scraper = None

def get_local_scraper() -> LocalCourtScraper:
    """Zwraca singleton scrapera."""
    global _scraper
    if _scraper is None:
        _scraper = LocalCourtScraper()
    return _scraper


def search_local_courts(keyword: str, **kwargs) -> Dict[str, Any]:
    """Skr√≥t do wyszukiwania w lokalnych portalach."""
    return get_local_scraper().search_all_portals(keyword, **kwargs)


# ============================================================================
# TEST
# ============================================================================

if __name__ == "__main__":
    print("üèõÔ∏è Local Court Scraper v3.5 Test\n")
    
    scraper = LocalCourtScraper()
    
    print("Test kodowania polskich znak√≥w:")
    print(f"  'rozw√≥d' ‚Üí '{scraper._encode_keyword_for_url('rozw√≥d')}'")
    print(f"  'ubezw≈Çasnowolnienie' ‚Üí '{scraper._encode_keyword_for_url('ubezw≈Çasnowolnienie')}'")
    print()
    
    print("Przyk≈Çadowy URL wyszukiwania:")
    base = "https://orzeczenia.warszawa.so.gov.pl"
    keyword = "alimenty"
    encoded = scraper._encode_keyword_for_url(keyword)
    url = f"{base}/search/advanced/{encoded}/$N/15450500/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/$N/score/descending/1"
    print(f"  {url}")
