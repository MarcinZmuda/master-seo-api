# claude_reviewer.py
# v28.2 - Claude jako Reviewer/Editor batchy
# v33.3 - + mandatory_entities, diff output
# v33.4 - + LanguageTool integration, semantic_diversity
#
# System sprawdzania i poprawiania batchy przez Claude API.
# Sprawdza: SEO, d≈Çugo≈õƒá, powt√≥rzenia, gramatykƒô, AI patterns, halucynacje

import os
import json
import re
import time
import difflib
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict, field
from collections import Counter
import math

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# v33.4: LanguageTool integration
try:
    from grammar_middleware import validate_batch_grammar, validate_batch_full
    LANGUAGETOOL_AVAILABLE = True
    print("[CLAUDE_REVIEWER] ‚úÖ LanguageTool integration enabled")
except ImportError:
    LANGUAGETOOL_AVAILABLE = False
    print("[CLAUDE_REVIEWER] ‚ö†Ô∏è LanguageTool not available, using Claude-only grammar check")


@dataclass
class ReviewIssue:
    type: str  # seo, length, repetition, grammar, ai_pattern, hallucination, coherence
    severity: str  # critical, warning, suggestion
    description: str
    location: str = ""
    fix_applied: bool = False


@dataclass
class DiffChange:
    """v33.3: Pojedyncza zmiana w tek≈õcie"""
    type: str  # "removed", "added", "context"
    text: str
    line_num: int = 0


@dataclass
class DiffSummary:
    """v33.3: Podsumowanie zmian"""
    lines_changed: int = 0
    words_removed: int = 0
    words_added: int = 0
    changes: List[DiffChange] = field(default_factory=list)


@dataclass
class ReviewResult:
    status: str  # APPROVED, CORRECTED, REJECTED, QUICK_CHECK_FAILED
    original_text: str
    corrected_text: Optional[str]
    issues: List[ReviewIssue]
    summary: str
    word_count: int = 0
    paragraph_count: int = 0
    diff: Optional[DiffSummary] = None  # v33.3: diff output
    semantic_diversity: Optional[Dict] = None  # v33.4: semantic diversity score
    grammar_lt: Optional[Dict] = None  # v33.4: LanguageTool results


# ================================================================
# v33.4: SEMANTIC DIVERSITY - Wykrywanie powt√≥rze≈Ñ semantycznych
# ================================================================

def extract_key_phrases(text: str) -> List[str]:
    """
    WyciƒÖga kluczowe frazy informacyjne z tekstu.
    Ignoruje s≈Çowa funkcyjne, zostawia tylko frazy niosƒÖce informacjƒô.
    """
    # S≈Çowa funkcyjne do ignorowania
    stop_words = {
        'jest', 'sƒÖ', 'byƒá', 'to', 'oraz', 'i', 'lub', 'ale', 'jednak', 'kt√≥ry', 'kt√≥ra', 'kt√≥re',
        'ten', 'ta', 'te', 'tym', 'tej', 'tego', 'na', 'w', 'z', 'do', 'od', 'dla', 'po', 'przez',
        'jako', 'te≈º', 'tak≈ºe', 'r√≥wnie≈º', 'wiƒôc', 'dlatego', 'poniewa≈º', 'gdy≈º', 'je≈õli', 'gdy',
        'bardzo', 'bardziej', 'najbardziej', 'mo≈ºe', 'mo≈ºna', 'powinien', 'warto', 'nale≈ºy',
        'co', 'jak', 'gdzie', 'kiedy', 'dlaczego', 'ile', 'ka≈ºdy', 'ka≈ºda', 'wszystko', 'nic',
        'tylko', 'ju≈º', 'jeszcze', 'w≈Ça≈õnie', 'natomiast', 'zatem', 'bowiem', 'czyli'
    }
    
    # WyciƒÖgnij zdania
    sentences = re.split(r'[.!?]+', text)
    
    key_phrases = []
    for sent in sentences:
        sent = sent.strip().lower()
        if len(sent) < 10:
            continue
        
        # WyciƒÖgnij 3-5 gramowe frazy (bez s≈Ç√≥w funkcyjnych na poczƒÖtku/ko≈Ñcu)
        words = [w for w in re.findall(r'\b[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]{3,}\b', sent) if w not in stop_words]
        
        # Bigramy i trigramy
        for n in [2, 3]:
            for i in range(len(words) - n + 1):
                phrase = ' '.join(words[i:i+n])
                key_phrases.append(phrase)
    
    return key_phrases


def calculate_semantic_diversity(text: str, sections: List[str] = None) -> Dict:
    """
    v33.4: Oblicza r√≥≈ºnorodno≈õƒá semantycznƒÖ tekstu.
    
    Wykrywa:
    - Powt√≥rzenia tych samych informacji r√≥≈ºnymi s≈Çowami
    - Sekcje m√≥wiƒÖce o tym samym
    - Brak nowych informacji w kolejnych sekcjach
    
    Returns:
        {
            "score": 0-100 (wy≈ºszy = bardziej r√≥≈ºnorodny),
            "status": "OK" | "WARNING" | "CRITICAL",
            "repeated_concepts": ["witamina C synteza kolagenu", ...],
            "section_similarity": [{"sections": [1,3], "similarity": 0.8}],
            "suggestions": ["Sekcja 3 powtarza informacje z sekcji 1"]
        }
    """
    if not text or len(text) < 200:
        return {
            "score": 100,
            "status": "OK",
            "repeated_concepts": [],
            "section_similarity": [],
            "suggestions": []
        }
    
    # Podziel na sekcje (po h2: lub podw√≥jnym newline)
    if sections is None:
        sections = re.split(r'\n\n+|(?=h2:)', text)
        sections = [s.strip() for s in sections if s.strip() and len(s.strip()) > 50]
    
    if len(sections) < 2:
        return {
            "score": 100,
            "status": "OK", 
            "repeated_concepts": [],
            "section_similarity": [],
            "suggestions": []
        }
    
    # WyciƒÖgnij kluczowe frazy z ka≈ºdej sekcji
    section_phrases = [set(extract_key_phrases(s)) for s in sections]
    
    # Znajd≈∫ powt√≥rzenia miƒôdzy sekcjami
    repeated_concepts = []
    section_similarities = []
    suggestions = []
    
    for i in range(len(sections)):
        for j in range(i + 1, len(sections)):
            common = section_phrases[i] & section_phrases[j]
            
            if len(common) == 0:
                continue
            
            # Oblicz Jaccard similarity
            union = section_phrases[i] | section_phrases[j]
            similarity = len(common) / len(union) if union else 0
            
            if similarity > 0.3:  # >30% podobie≈Ñstwa = warning
                section_similarities.append({
                    "sections": [i + 1, j + 1],
                    "similarity": round(similarity, 2),
                    "common_phrases": list(common)[:5]
                })
                
                if similarity > 0.5:
                    suggestions.append(f"Sekcja {j + 1} powtarza ~{int(similarity * 100)}% informacji z sekcji {i + 1}")
            
            # Zbierz powt√≥rzone frazy (te kt√≥re wystƒôpujƒÖ w >2 sekcjach)
            for phrase in common:
                if phrase not in repeated_concepts:
                    repeated_concepts.append(phrase)
    
    # Oblicz ko≈Ñcowy score
    # Penalizuj za: podobne sekcje, powt√≥rzone koncepty
    penalty = 0
    penalty += len([s for s in section_similarities if s["similarity"] > 0.5]) * 15
    penalty += len([s for s in section_similarities if 0.3 < s["similarity"] <= 0.5]) * 8
    penalty += min(len(repeated_concepts), 10) * 3
    
    score = max(0, 100 - penalty)
    
    if score >= 70:
        status = "OK"
    elif score >= 40:
        status = "WARNING"
    else:
        status = "CRITICAL"
    
    return {
        "score": score,
        "status": status,
        "repeated_concepts": repeated_concepts[:10],  # Max 10
        "section_similarity": section_similarities[:5],  # Max 5
        "suggestions": suggestions[:3]  # Max 3
    }


def check_grammar_with_languagetool(text: str) -> Dict:
    """
    v33.4: Sprawdza gramatykƒô przez LanguageTool.
    Zwraca wyniki w formacie kompatybilnym z ReviewIssue.
    """
    if not LANGUAGETOOL_AVAILABLE:
        return {
            "enabled": False,
            "errors": [],
            "error_count": 0
        }
    
    try:
        result = validate_batch_grammar(text)
        
        return {
            "enabled": True,
            "is_valid": result.is_valid,
            "errors": [
                {
                    "message": err.get("message", ""),
                    "context": err.get("context", {}).get("text", "")[:50] if isinstance(err.get("context"), dict) else str(err.get("context", ""))[:50],
                    "suggestions": [r.get("value", r) if isinstance(r, dict) else r for r in err.get("replacements", [])[:2]]
                }
                for err in result.errors[:5]  # Max 5 b≈Çƒôd√≥w
            ],
            "error_count": result.error_count,
            "backend": result.backend,
            "correction_prompt": result.correction_prompt
        }
    except Exception as e:
        print(f"[CLAUDE_REVIEWER] ‚ö†Ô∏è LanguageTool error: {e}")
        return {
            "enabled": True,
            "errors": [],
            "error_count": 0,
            "error_message": str(e)
        }


# ================================================================
# QUICK CHECKS (Python, bez API)
# ================================================================
# v29.0: NOWE PRIORYTETY
# 1. JAKO≈öƒÜ TEKSTU (tautologie, gramatyka) ‚Üí CRITICAL
# 2. ENCJE + N-GRAMY w odpowiednich miejscach ‚Üí WARNING  
# 3. S≈ÅOWA KLUCZOWE: min 1√ó, NIE stuffing ‚Üí tylko stuffing blokuje
# ================================================================

# Import lemmatyzacji dla quick checks
try:
    from polish_lemmatizer import count_phrase_occurrences
    _LEMMATIZER_OK = True
except ImportError:
    _LEMMATIZER_OK = False
    print("[CLAUDE_REVIEWER] ‚ö†Ô∏è polish_lemmatizer not available, using exact match")


def quick_check_keywords(text: str, required: List[Dict]) -> Tuple[List[str], List[str], Dict]:
    """
    v29.2: NOWA LOGIKA - TYLKO STUFFING BLOKUJE!
    
    - Fraza 0√ó ‚Üí WARNING (Claude uzupe≈Çni na ko≈Ñcu)
    - Fraza <target ‚Üí OK
    - Fraza >max ‚Üí CRITICAL (stuffing) - JEDYNY BLOKER!
    
    Zwraca: (critical_errors, warnings, missing_info)
    - critical = TYLKO stuffing
    - warnings = missing (0√ó) + suggestions
    - missing_info = {"basic": [...], "extended": [...]} - do przekazania Claude'owi
    """
    text_lower = text.lower()
    missing_basic = []        # 0 wystƒÖpie≈Ñ BASIC - do uzupe≈Çnienia
    missing_extended = []     # 0 wystƒÖpie≈Ñ EXTENDED - do uzupe≈Çnienia
    stuffing_errors = []      # za du≈ºo - CRITICAL (blokuje!)
    warnings = []             # info o brakujƒÖcych
    
    for kw in required:
        keyword = kw.get("keyword", "")
        count_req = kw.get("count", 1)
        count_max = kw.get("max", count_req * 3)  # max = 3√ó wymagane
        kw_type = kw.get("type", "BASIC").upper()
        if not keyword:
            continue
        
        # Licz z lemmatyzacjƒÖ
        if _LEMMATIZER_OK:
            result = count_phrase_occurrences(text, keyword)
            count_found = result.get("count", 0)
        else:
            count_found = text_lower.count(keyword.lower())
        
        # LOGIKA v29.2:
        if count_found == 0:
            # WARNING: fraza brakuje - Claude uzupe≈Çni na ko≈Ñcu
            warnings.append(f'"{keyword}" (0/{count_req}) - brak, do uzupe≈Çnienia')
            # Dodaj do listy dla Claude
            if kw_type == "EXTENDED":
                missing_extended.append(keyword)
            else:
                missing_basic.append(keyword)
        elif count_found > count_max:
            # CRITICAL: stuffing - JEDYNY BLOKER!
            stuffing_errors.append(f'"{keyword}" ({count_found}√ó) - STUFFING! Max {count_max}√ó')
        elif count_found < count_req:
            # OK: mog≈Çoby byƒá wiƒôcej - tylko info, nie dodajemy do missing
            warnings.append(f'"{keyword}" ({count_found}/{count_req}) - OK')
    
    # v29.2: TYLKO stuffing blokuje!
    critical = stuffing_errors
    
    # Info o brakujƒÖcych frazach do przekazania Claude'owi
    missing_info = {
        "basic": missing_basic,
        "extended": missing_extended
    }
    
    return critical, warnings, missing_info


def quick_check_text_quality(text: str) -> Tuple[List[str], List[str]]:
    """
    v29.0: NOWY CHECK - Jako≈õƒá tekstu (PRIORYTET 1!)
    
    Sprawdza:
    - Tautologie (s≈Çowo powt√≥rzone w jednym zdaniu)
    - Pleonazmy ("przedszkole...w przedszkolu")
    - Zbyt d≈Çugie zdania (>35 s≈Ç√≥w)
    - Strona bierna nadu≈ºywana
    """
    import re
    
    critical = []
    warnings = []
    
    # Podziel na zdania
    sentences = re.split(r'[.!?]+', text)
    
    for i, sentence in enumerate(sentences, 1):
        sentence = sentence.strip()
        if not sentence:
            continue
        
        words = sentence.lower().split()
        
        # 1. TAUTOLOGIE - to samo s≈Çowo 2+ razy w zdaniu (min 4 litery)
        word_counts = {}
        for w in words:
            w_clean = re.sub(r'[^\w]', '', w)
            if len(w_clean) >= 4:
                word_counts[w_clean] = word_counts.get(w_clean, 0) + 1
        
        for word, count in word_counts.items():
            if count >= 2 and word not in ['jest', 'oraz', 'kt√≥re', 'kt√≥ry', 'kt√≥ra', 'tak≈ºe', 'bardzo']:
                # Sprawd≈∫ czy to nie odmiana
                if word in ['przedszkole', 'przedszkolu', 'przedszkolnym', 'przedszkolnych']:
                    critical.append(f'Zdanie {i}: tautologia "przedszkol*" powt√≥rzone {count}√ó - POPRAW!')
                elif word in ['sensoryczny', 'sensoryczna', 'sensoryczne', 'sensorycznych', 'sensorycznym']:
                    if count >= 3:
                        warnings.append(f'Zdanie {i}: "sensoryczn*" u≈ºyte {count}√ó - rozwa≈º synonim')
        
        # 2. ZBYT D≈ÅUGIE ZDANIE
        if len(words) > 35:
            warnings.append(f'Zdanie {i}: {len(words)} s≈Ç√≥w - rozwa≈º podzia≈Ç')
    
    # 3. PLEONAZMY GLOBALNE
    text_lower = text.lower()
    
    # "przedszkole...w przedszkolu" w tym samym akapicie
    paragraphs = text.split('\n\n')
    for p_idx, para in enumerate(paragraphs, 1):
        para_lower = para.lower()
        if 'przedszkole' in para_lower and 'w przedszkolu' in para_lower:
            # Sprawd≈∫ czy to nie jest "pomoce sensoryczne w przedszkolu" (fraza kluczowa)
            if 'pomoce sensoryczne w przedszkolu' not in para_lower:
                critical.append(f'Akapit {p_idx}: pleonazm "przedszkole...w przedszkolu" - zamie≈Ñ jedno na "plac√≥wka/obiekt"')
    
    return critical, warnings


def quick_check_length(text: str, min_w: int, max_w: int) -> Tuple[Optional[str], int]:
    words = len(text.split())
    if words < min_w * 0.8:
        return f"Za kr√≥tki: {words} s≈Ç√≥w (min: {min_w})", words
    elif words > max_w * 1.3:
        return f"Za d≈Çugi: {words} s≈Ç√≥w (max: {max_w})", words
    return None, words


def quick_check_forbidden(text: str, forbidden: List[str]) -> List[str]:
    text_lower = text.lower()
    return [f for f in forbidden if f and f.lower() in text_lower]


def quick_check_ai_patterns(text: str) -> List[str]:
    patterns = [
        "w dzisiejszych czasach", "warto wiedzieƒá", "nie jest tajemnicƒÖ",
        "podsumowujƒÖc", "w niniejszym artykule", "jak wiadomo",
        "przyk≈Çad:", "na przyk≈Çad,", "wyobra≈∫my sobie", "za≈Ç√≥≈ºmy, ≈ºe"
    ]
    text_lower = text.lower()
    return [p for p in patterns if p in text_lower]


def run_quick_checks(text: str, context: Dict) -> Dict:
    """
    v29.0: NOWE PRIORYTETY
    
    PRIORYTET 1: Jako≈õƒá tekstu (tautologie, pleonazmy) ‚Üí CRITICAL
    PRIORYTET 2: Encje/n-gramy w odpowiednich miejscach ‚Üí WARNING
    PRIORYTET 3: S≈Çowa kluczowe (min 1√ó, nie stuffing) ‚Üí tylko stuffing/brak blokuje
    """
    critical_errors = []  # BlokujƒÖ zapis
    warnings = []         # Tylko info, nie blokujƒÖ
    suggestions = []      # Sugestie optymalizacji
    
    # ============================================
    # PRIORYTET 1: JAKO≈öƒÜ TEKSTU (CRITICAL!)
    # ============================================
    quality_critical, quality_warnings = quick_check_text_quality(text)
    for err in quality_critical:
        critical_errors.append({"type": "quality", "severity": "critical", "msg": err})
    for warn in quality_warnings:
        warnings.append({"type": "quality", "severity": "warning", "msg": warn})
    
    # AI patterns - te≈º jako≈õƒá
    ai = quick_check_ai_patterns(text)
    if ai:
        warnings.append({"type": "ai_pattern", "severity": "warning", "msg": f"AI patterns: {', '.join(ai)}"})
    
    # ============================================
    # PRIORYTET 2: D≈ÅUGO≈öƒÜ (ale elastyczna)
    # ============================================
    len_err, words = quick_check_length(
        text, 
        context.get("target_words_min", 150),
        context.get("target_words_max", 500)
    )
    if len_err:
        # Za kr√≥tki = critical, za d≈Çugi = warning (mo≈ºna skr√≥ciƒá)
        if "za kr√≥tki" in len_err.lower():
            critical_errors.append({"type": "length", "severity": "critical", "msg": len_err})
        else:
            warnings.append({"type": "length", "severity": "warning", "msg": len_err})
    
    # ============================================
    # PRIORYTET 3: KEYWORDS (nowa logika v29.2!)
    # ============================================
    # critical = TYLKO stuffing
    # warnings = info o brakujƒÖcych i niedostatecznych
    # missing_info = lista fraz do uzupe≈Çnienia przez Claude
    kw_critical, kw_warnings, missing_info = quick_check_keywords(text, context.get("keywords_required", []))
    
    for err in kw_critical:
        critical_errors.append({"type": "seo", "severity": "critical", "msg": err})
    for warn in kw_warnings:
        warnings.append({"type": "seo", "severity": "warning", "msg": warn})
    
    # Forbidden keywords - zawsze critical
    forbidden = quick_check_forbidden(text, context.get("keywords_forbidden", []))
    if forbidden:
        critical_errors.append({"type": "seo", "severity": "critical", "msg": f"Zabronione frazy: {', '.join(forbidden)}"})
    
    # ============================================
    # STATS
    # ============================================
    paras = len([p for p in text.split('\n\n') if p.strip() and len(p) > 30])
    
    return {
        "passed": len(critical_errors) == 0,  # Tylko CRITICAL (stuffing) blokuje!
        "errors": critical_errors,
        "warnings": warnings,
        "suggestions": suggestions,
        "word_count": words,
        "paragraph_count": paras,
        "missing_phrases": missing_info,  # v29.2: do uzupe≈Çnienia przez Claude
        "priority_summary": {
            "quality_issues": len([e for e in critical_errors if e["type"] == "quality"]),
            "seo_issues": len([e for e in critical_errors if e["type"] == "seo"]),
            "length_issues": len([e for e in critical_errors if e["type"] == "length"])
        }
    }


# ================================================================
# CLAUDE REVIEW
# ================================================================

def build_review_prompt(text: str, ctx: Dict) -> str:
    """v29.2: Prompt z listƒÖ BRAKUJƒÑCYCH FRAZ do uzupe≈Çnienia przez Claude
       v33.3: + mandatory_entities kt√≥rych Claude NIE MO≈ªE usunƒÖƒá
    """
    
    required = "\n".join([f'  ‚Ä¢ "{k["keyword"]}" (min 1√ó, zalecane {k.get("count",1)}√ó)' 
                          for k in ctx.get("keywords_required", []) if k.get("keyword")])
    forbidden = ", ".join(ctx.get("keywords_forbidden", [])) or "brak"
    h2_list = "\n".join([f"  ‚Ä¢ {h}" for h in ctx.get("h2_current", [])]) or "  (brak)"
    
    # G≈Ç√≥wna fraza
    main_kw = ctx.get("main_keyword", "")
    main_kw_count = ctx.get("main_keyword_count", 2)
    
    # Snippet info
    snippet_info = "TAK (40-60 s≈Ç√≥w na poczƒÖtku)" if ctx.get("snippet_required") else "NIE"
    
    # v29.2: BRAKUJƒÑCE FRAZY DO UZUPE≈ÅNIENIA
    missing_basic = ctx.get("missing_basic", [])
    missing_extended = ctx.get("missing_extended", [])
    
    # v33.3: MANDATORY ENTITIES - Claude NIE MO≈ªE ich usunƒÖƒá/zmieniƒá
    mandatory_entities = ctx.get("mandatory_entities", [])
    mandatory_section = ""
    if mandatory_entities:
        entities_list = "\n".join([f'  ‚õî "{e}"' for e in mandatory_entities])
        mandatory_section = f"""
### ‚õî OBOWIƒÑZKOWE ENCJE (NIE USUWAJ, NIE ZMIENIAJ!)
{entities_list}

Te encje/frazy MUSZƒÑ pozostaƒá w tek≈õcie BEZ ZMIAN. Mo≈ºesz tylko poprawiƒá ich otoczenie gramatycznie.
Je≈õli usuniesz lub zmienisz kt√≥rƒÖkolwiek z nich - twoja odpowied≈∫ zostanie odrzucona!

"""
    
    missing_section = ""
    if missing_basic or missing_extended:
        missing_section = """
### üî¥ BRAKUJƒÑCE FRAZY - MUSISZ UZUPE≈ÅNIƒÜ!

**Te frazy NIE wystƒôpujƒÖ w tek≈õcie - wpleƒá je naturalnie:**
"""
        if missing_basic:
            missing_section += "\nBASIC (wa≈ºniejsze):\n"
            for phrase in missing_basic:
                missing_section += f'  ‚Ä¢ "{phrase}"\n'
        
        if missing_extended:
            missing_section += "\nEXTENDED:\n"
            for phrase in missing_extended:
                missing_section += f'  ‚Ä¢ "{phrase}"\n'
        
        missing_section += """
‚ö†Ô∏è WPLEƒÜ KA≈ªDƒÑ BRAKUJƒÑCƒÑ FRAZƒò min 1√ó w naturalny spos√≥b!
   Mo≈ºesz u≈ºyƒá odmiany (np. "≈õcie≈ºkƒÖ sensorycznƒÖ" zamiast "≈õcie≈ºka sensoryczna")
"""
    
    return f"""Jeste≈õ redaktorem i stylistƒÖ jƒôzyka polskiego. Sprawd≈∫ i POPRAW tekst.

## PRIORYTETY (w tej kolejno≈õci!):
{mandatory_section}
### üî¥ PRIORYTET 1: JAKO≈öƒÜ TEKSTU (NAJWA≈ªNIEJSZE!)
Tekst musi byƒá poprawny, naturalny i przyjemny w czytaniu.

SPRAWD≈π I POPRAW:
- **TAUTOLOGIE**: "przedszkole... w przedszkolu" ‚Üí zamie≈Ñ jedno na "plac√≥wka/obiekt/sala"
- **PLEONAZMY**: "nowoczesne przedszkole wyposa≈ºone jest w pomoce w przedszkolu" ‚Üí DRAMAT!
- **POWT√ìRZENIA**: To samo s≈Çowo 2√ó w zdaniu (poza sp√≥jnikami) ‚Üí u≈ºyj synonimu
- **STRONA BIERNA**: "jest wyposa≈ºone w" ‚Üí "posiada", "oferuje", "zawiera"
- **D≈ÅUGIE ZDANIA**: >30 s≈Ç√≥w ‚Üí podziel na 2
- **GRAMATYKA**: B≈Çƒôdy, kolokacje, naturalno≈õƒá jƒôzyka polskiego
- **AI PATTERNS**: "W dzisiejszych czasach", "Warto wiedzieƒá", "Nie jest tajemnicƒÖ" ‚Üí USU≈É
- **HALUCYNACJE**: Wymy≈õlone statystyki, badania, fakty bez ≈∫r√≥d≈Ça ‚Üí USU≈É

### üî¥ PRIORYTET 1b: SEMANTIC DIVERSITY (v33.4 NOWE!)
KA≈ªDA SEKCJA MUSI WNOSIƒÜ NOWE INFORMACJE!

‚õî NIEDOPUSZCZALNE:
- Powtarzanie tej samej informacji w r√≥≈ºnych sekcjach (np. "witamina C wspomaga kolagen" ‚Üí "wit. C produkuje kolagen" ‚Üí "C odpowiada za kolagen")
- M√≥wienie o tym samym innymi s≈Çowami
- Sekcje kt√≥re mo≈ºna usunƒÖƒá bez utraty informacji

‚úÖ WYMAGANE:
- Ka≈ºda sekcja = NOWA wiedza, nowy aspekt, nowy kontekst
- Je≈õli witamina C by≈Ça w sekcji 1, w sekcji 3 napisz o czym≈õ INNYM (dawkowanie, ≈∫r√≥d≈Ça, interakcje)
- Konkrety zamiast powt√≥rze≈Ñ: nazwy substancji, warto≈õci, przyk≈Çady

### üü° PRIORYTET 2: ENCJE I N-GRAMY
Upewnij siƒô, ≈ºe kluczowe pojƒôcia sƒÖ zdefiniowane/wyja≈õnione przy pierwszym u≈ºyciu.
{missing_section}
### üü¢ PRIORYTET 3: ISTNIEJƒÑCE FRAZY (sprawd≈∫ ilo≈õci)
Frazy powinny wystƒôpowaƒá NATURALNIE. Lepiej 1√ó naturalnie ni≈º 3√ó sztucznie!

G≈Å√ìWNA FRAZA: "{main_kw}" (min {main_kw_count}√ó)

POZOSTA≈ÅE FRAZY (min 1√ó, zalecane ilo≈õci to cel, nie wym√≥g):
{required}

‚ùå NIE R√ìB: wstawiania fraz "na si≈Çƒô" kt√≥re psujƒÖ naturalno≈õƒá
‚úÖ TAK R√ìB: wplataj frazy gdzie pasujƒÖ do kontekstu

ZABRONIONE: {forbidden}

---

## KONTEKST
- Temat: {ctx.get("topic", "")}
- Batch: #{ctx.get("batch_number", 1)}
- H2: {h2_list}
- S≈Çowa: {ctx.get("target_words_min", 200)}-{ctx.get("target_words_max", 500)}
- Akapity: {ctx.get("target_paragraphs_min", 2)}-{ctx.get("target_paragraphs_max", 5)}
- Snippet: {snippet_info}

## TEKST DO SPRAWDZENIA:
{text}

---

## ODPOWIED≈π (tylko JSON):
```json
{{
  "status": "APPROVED | CORRECTED | REJECTED",
  "quality_score": 1-10,
  "issues": [
    {{"priority": 1, "type": "tautologia|pleonazm|gramatyka|halucynacja|ai_pattern|semantic_repetition", "description": "...", "fix_applied": true}},
    {{"priority": 2, "type": "encja_brak", "description": "...", "fix_applied": false}},
    {{"priority": 3, "type": "fraza_dodana|fraza_brak", "description": "...", "fix_applied": true}}
  ],
  "phrases_added": ["lista fraz kt√≥re wplot≈Çe≈õ"],
  "corrected_text": "pe≈Çny poprawiony tekst (tylko je≈õli CORRECTED)",
  "summary": "co poprawiono"
}}
```

ZASADY:
- APPROVED = jako≈õƒá OK, wszystkie brakujƒÖce frazy uzupe≈Çnione
- CORRECTED = poprawi≈Çe≈õ b≈Çƒôdy jako≈õciowe LUB uzupe≈Çni≈Çe≈õ frazy, zwr√≥ƒá pe≈Çny tekst
- REJECTED = tekst nie do uratowania (za kr√≥tki, same b≈Çƒôdy, halucynacje)
- Zachowaj format h2: / h3:
- JE≈öLI SƒÑ BRAKUJƒÑCE FRAZY ‚Üí MUSISZ zwr√≥ciƒá CORRECTED z uzupe≈Çnionym tekstem!
- NIE dopisuj tekstu je≈õli za kr√≥tki ‚Üí zwr√≥ƒá REJECTED
- PRIORYTET 1 (jako≈õƒá) wa≈ºniejszy ni≈º dok≈Çadne ilo≈õci fraz!"""


def generate_diff(original: str, corrected: str) -> DiffSummary:
    """
    v33.3: Generuje diff miƒôdzy orygina≈Çem a poprawionym tekstem.
    """
    if not corrected or original == corrected:
        return DiffSummary()
    
    original_lines = original.splitlines()
    corrected_lines = corrected.splitlines()
    
    differ = difflib.unified_diff(
        original_lines,
        corrected_lines,
        lineterm='',
        n=0  # bez kontekstu
    )
    
    changes = []
    words_removed = 0
    words_added = 0
    line_num = 0
    
    for line in differ:
        if line.startswith('@@'):
            # Parse line number from @@ -X,Y +A,B @@
            match = re.search(r'\+(\d+)', line)
            if match:
                line_num = int(match.group(1))
            continue
        elif line.startswith('---') or line.startswith('+++'):
            continue
        elif line.startswith('-'):
            text = line[1:]
            if text.strip():
                changes.append(DiffChange(type="removed", text=text, line_num=line_num))
                words_removed += len(text.split())
        elif line.startswith('+'):
            text = line[1:]
            if text.strip():
                changes.append(DiffChange(type="added", text=text, line_num=line_num))
                words_added += len(text.split())
            line_num += 1
    
    return DiffSummary(
        lines_changed=len([c for c in changes if c.type in ["removed", "added"]]),
        words_removed=words_removed,
        words_added=words_added,
        changes=changes[:20]  # Limit do 20 zmian
    )


def validate_mandatory_entities(original: str, corrected: str, mandatory: List[str]) -> List[str]:
    """
    v33.3: Sprawdza czy mandatory_entities zosta≈Çy zachowane w tek≈õcie.
    Zwraca listƒô brakujƒÖcych encji.
    """
    if not mandatory or not corrected:
        return []
    
    missing = []
    original_lower = original.lower()
    corrected_lower = corrected.lower()
    
    for entity in mandatory:
        entity_lower = entity.lower()
        # Sprawd≈∫ czy encja by≈Ça w oryginale i zniknƒô≈Ça z poprawionego
        if entity_lower in original_lower and entity_lower not in corrected_lower:
            missing.append(entity)
    
    return missing


def review_with_claude(text: str, ctx: Dict) -> ReviewResult:
    if not ANTHROPIC_AVAILABLE or not os.environ.get("ANTHROPIC_API_KEY"):
        return ReviewResult("APPROVED", text, None, [], "Claude niedostƒôpny", len(text.split()))
    
    try:
        client = anthropic.Anthropic()
        start = time.time()
        
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=4000,
            messages=[{"role": "user", "content": build_review_prompt(text, ctx)}]
        )
        
        resp_text = response.content[0].text
        json_match = re.search(r'\{[\s\S]*\}', resp_text)
        
        if not json_match:
            return ReviewResult("APPROVED", text, None, [], "Brak JSON w odpowiedzi", len(text.split()))
        
        data = json.loads(json_match.group())
        
        issues = [ReviewIssue(
            type=i.get("type", ""),
            severity=i.get("severity", "warning"),
            description=i.get("description", ""),
            fix_applied=i.get("fix_applied", False)
        ) for i in data.get("issues", [])]
        
        status = data.get("status", "APPROVED")
        corrected = data.get("corrected_text")
        
        if status == "CORRECTED" and (not corrected or len(corrected) < 50):
            status = "APPROVED"
            corrected = None
        
        # v33.3: Walidacja mandatory_entities
        mandatory_entities = ctx.get("mandatory_entities", [])
        if corrected and mandatory_entities:
            missing_entities = validate_mandatory_entities(text, corrected, mandatory_entities)
            if missing_entities:
                # Claude usunƒÖ≈Ç obowiƒÖzkowe encje - odrzuƒá korektƒô!
                issues.append(ReviewIssue(
                    type="mandatory_entity_removed",
                    severity="critical",
                    description=f"Claude usunƒÖ≈Ç obowiƒÖzkowe encje: {', '.join(missing_entities)}",
                    fix_applied=False
                ))
                # Przywr√≥ƒá oryginalny tekst
                print(f"[CLAUDE_REVIEWER] ‚ö†Ô∏è Mandatory entities removed: {missing_entities} - reverting to original")
                corrected = None
                status = "APPROVED"  # Zachowaj oryginalny tekst
        
        # v33.3: Generuj diff je≈õli sƒÖ zmiany
        diff_summary = None
        if corrected and corrected != text:
            diff_summary = generate_diff(text, corrected)
            print(f"[CLAUDE_REVIEWER] üìù Diff: {diff_summary.lines_changed} lines, -{diff_summary.words_removed}/+{diff_summary.words_added} words")
        
        final = corrected if corrected else text
        
        # v33.4: Semantic diversity check
        semantic_div = calculate_semantic_diversity(final)
        if semantic_div["status"] == "CRITICAL":
            issues.append(ReviewIssue(
                type="semantic_repetition",
                severity="critical",
                description=f"Tekst zawiera zbyt du≈ºo powt√≥rze≈Ñ semantycznych. {'; '.join(semantic_div['suggestions'][:2])}",
                fix_applied=False
            ))
            print(f"[CLAUDE_REVIEWER] ‚ö†Ô∏è Semantic diversity CRITICAL: {semantic_div['score']}")
        elif semantic_div["status"] == "WARNING":
            issues.append(ReviewIssue(
                type="semantic_repetition",
                severity="warning",
                description=f"Niekt√≥re sekcje powtarzajƒÖ podobne informacje. {'; '.join(semantic_div['suggestions'][:1])}",
                fix_applied=False
            ))
        
        # v33.4: LanguageTool grammar check
        grammar_lt = check_grammar_with_languagetool(final)
        if grammar_lt.get("enabled") and grammar_lt.get("error_count", 0) > 0:
            for err in grammar_lt.get("errors", [])[:3]:
                issues.append(ReviewIssue(
                    type="grammar_lt",
                    severity="warning" if grammar_lt["error_count"] <= 2 else "critical",
                    description=f"B≈ÇƒÖd gramatyczny: {err.get('message', '')}. Kontekst: ...{err.get('context', '')}...",
                    fix_applied=False
                ))
            print(f"[CLAUDE_REVIEWER] ‚ö†Ô∏è LanguageTool found {grammar_lt['error_count']} errors")
        
        return ReviewResult(
            status=status,
            original_text=text,
            corrected_text=corrected,
            issues=issues,
            summary=data.get("summary", ""),
            word_count=len(final.split()),
            paragraph_count=len([p for p in final.split('\n\n') if p.strip()]),
            diff=diff_summary,  # v33.3
            semantic_diversity=semantic_div,  # v33.4
            grammar_lt=grammar_lt  # v33.4
        )
        
    except Exception as e:
        print(f"[CLAUDE_REVIEWER] Error: {e}")
        return ReviewResult("APPROVED", text, None, [], f"B≈ÇƒÖd: {e}", len(text.split()))


# ================================================================
# G≈Å√ìWNA FUNKCJA
# ================================================================

def review_batch(text: str, context: Dict, skip_claude: bool = False) -> ReviewResult:
    """
    v29.2: Pe≈Çny review: Quick Checks + Claude.
    
    Je≈õli sƒÖ brakujƒÖce frazy (0√ó), przekazuje je do Claude do uzupe≈Çnienia.
    """
    # Quick checks
    qc = run_quick_checks(text, context)
    
    if not qc["passed"]:
        # v29.2: Jedyny bloker to stuffing
        issues = [ReviewIssue(e["type"], "critical", e["msg"]) for e in qc["errors"]]
        issues += [ReviewIssue(w["type"], "warning", w["msg"]) for w in qc["warnings"]]
        return ReviewResult(
            "QUICK_CHECK_FAILED", text, None, issues,
            "Popraw b≈Çƒôdy krytyczne (stuffing)",
            qc["word_count"], qc["paragraph_count"]
        )
    
    if skip_claude:
        issues = [ReviewIssue(w["type"], "warning", w["msg"]) for w in qc["warnings"]]
        return ReviewResult(
            "APPROVED", text, None, issues,
            "Quick check OK",
            qc["word_count"], qc["paragraph_count"]
        )
    
    # v29.2: Przeka≈º brakujƒÖce frazy do kontekstu Claude
    missing = qc.get("missing_phrases", {})
    context["missing_basic"] = missing.get("basic", [])
    context["missing_extended"] = missing.get("extended", [])
    
    # Claude review - uzupe≈Çni brakujƒÖce frazy
    result = review_with_claude(text, context)
    
    # Dodaj warnings z quick check
    for w in qc["warnings"]:
        if not any(i.fix_applied and i.type == w["type"] for i in result.issues):
            result.issues.append(ReviewIssue(w["type"], "warning", w["msg"]))
    
    return result


def build_context_from_pre_batch(pre_batch: Dict, project: Dict = None) -> Dict:
    """Helper: buduje context z getPreBatchInfo."""
    keywords_required = []
    
    main_kw = pre_batch.get("main_keyword", {})
    if main_kw.get("keyword"):
        keywords_required.append({
            "keyword": main_kw["keyword"],
            "count": main_kw.get("info", {}).get("use_this_batch", 2)
        })
    
    kw = pre_batch.get("keywords", {})
    for k in kw.get("basic_must_use", [])[:8]:
        if k.get("keyword"):
            keywords_required.append({"keyword": k["keyword"], "count": 1})
    for k in kw.get("extended_this_batch", [])[:4]:
        if k.get("keyword"):
            keywords_required.append({"keyword": k["keyword"], "count": 1})
    
    forbidden = [k.get("keyword") for k in kw.get("locked_exceeded", []) if k.get("keyword")]
    forbidden += kw.get("extended_done", [])
    
    bl = pre_batch.get("batch_length", {})
    
    last = ""
    if project:
        content = project.get("article_content", "")
        if content:
            last = content[-200:]
    
    return {
        "topic": pre_batch.get("topic", ""),
        "h2_current": pre_batch.get("h2_remaining", [])[:2],
        "keywords_required": keywords_required,
        "keywords_forbidden": [f for f in forbidden if f],
        "last_sentences": last,
        "target_words_min": bl.get("suggested_min", 200),
        "target_words_max": bl.get("suggested_max", 500),
        "target_paragraphs_min": bl.get("paragraphs_min", 2),
        "target_paragraphs_max": bl.get("paragraphs_max", 5),
        "main_keyword": main_kw.get("keyword", ""),
        "main_keyword_count": main_kw.get("info", {}).get("use_this_batch", 2),
        "batch_number": pre_batch.get("batch_number", 1),
        "snippet_required": bl.get("snippet_required", True),
        "complexity_score": bl.get("complexity_score", 50)
    }
