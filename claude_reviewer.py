"""
===============================================================================
CLAUDE REVIEWER v33.5 - OPTIMIZED
===============================================================================
ZMIANY OPTYMALIZACYJNE:
- ðŸ†• AUTO-FIX STUFFINGU: automatyczna zamiana na synonimy przed odrzuceniem
- ðŸ†• Mniej restrykcyjne quick checks
- ðŸ†• Zmniejszona liczba critical errors
- ðŸ†• Inteligentne retry z kontekstem bÅ‚Ä™du

EFEKT: -30% iteracji, auto-naprawa prostych problemÃ³w
===============================================================================
"""

import os
import json
import re
import time
import difflib
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict, field
from collections import Counter
import math

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# v33.4: LanguageTool integration
try:
    from grammar_middleware import validate_batch_grammar, validate_batch_full
    LANGUAGETOOL_AVAILABLE = True
    print("[CLAUDE_REVIEWER] âœ… LanguageTool integration enabled")
except ImportError:
    LANGUAGETOOL_AVAILABLE = False
    print("[CLAUDE_REVIEWER] âš ï¸ LanguageTool not available, using Claude-only grammar check")


# ================================================================
# ðŸ†• v33.5: SÅOWNIK SYNONIMÃ“W DO AUTO-FIX
# ================================================================
SYNONYM_MAP = {
    # Prawne
    "ubezwÅ‚asnowolnienie": ["ograniczenie zdolnoÅ›ci do czynnoÅ›ci prawnych", "pozbawienie peÅ‚nej zdolnoÅ›ci", "orzeczenie o niezdolnoÅ›ci"],
    "sÄ…d": ["organ sÄ…dowy", "instytucja", "trybunaÅ‚"],
    "sÄ…d okrÄ™gowy": ["wÅ‚aÅ›ciwy sÄ…d", "organ orzekajÄ…cy", "sÄ…d"],
    "kurator": ["opiekun prawny", "przedstawiciel", "osoba sprawujÄ…ca pieczÄ™"],
    "postÄ™powanie": ["procedura", "proces", "sprawa"],
    "wniosek": ["pismo", "podanie", "Å¼Ä…danie"],
    
    # Medyczne
    "demencja": ["otÄ™pienie", "zaburzenia poznawcze", "choroba otÄ™pienna"],
    "choroba Alzheimera": ["Alzheimer", "choroba neurodegeneracyjna", "otÄ™pienie typu Alzheimera"],
    
    # Rodzinne
    "osoba starsza": ["senior", "osoba w podeszÅ‚ym wieku", "osoba starza wiekiem"],
    "rodzina": ["bliscy", "krewni", "czÅ‚onkowie rodziny"],
    
    # OgÃ³lne
    "pomoc": ["wsparcie", "asystencja", "opieka"],
    "waÅ¼ne": ["istotne", "kluczowe", "znaczÄ…ce"],
    "czÄ™sto": ["nierzadko", "wielokrotnie", "regularnie"],
    "bardzo": ["niezwykle", "szczegÃ³lnie", "wyjÄ…tkowo"],
}


def get_synonym(phrase: str) -> Optional[str]:
    """
    ðŸ†• v33.5: Zwraca synonim dla frazy.
    """
    phrase_lower = phrase.lower().strip()
    
    # DokÅ‚adne dopasowanie
    if phrase_lower in SYNONYM_MAP:
        synonyms = SYNONYM_MAP[phrase_lower]
        if synonyms:
            return synonyms[0]  # ZwrÃ³Ä‡ pierwszy synonim
    
    # CzÄ™Å›ciowe dopasowanie
    for key, synonyms in SYNONYM_MAP.items():
        if key in phrase_lower or phrase_lower in key:
            if synonyms:
                return synonyms[0]
    
    return None


def auto_fix_stuffing(text: str, stuffed_keywords: List[Dict]) -> Tuple[str, List[str]]:
    """
    ðŸ†• v33.5: Automatycznie naprawia stuffing zamieniajÄ…c nadmiarowe wystÄ…pienia na synonimy.
    
    Args:
        text: Tekst do naprawy
        stuffed_keywords: Lista {keyword, count, limit} z check_batch_stuffing
    
    Returns:
        Tuple[fixed_text, applied_fixes]
    """
    fixed_text = text
    applied_fixes = []
    
    for stuffed in stuffed_keywords:
        keyword = stuffed.get("keyword", "")
        count = stuffed.get("count", 0)
        limit = stuffed.get("limit", 2)
        
        if not keyword or count <= limit:
            continue
        
        excess = count - limit
        synonym = get_synonym(keyword)
        
        if not synonym:
            # Brak synonimu - sprÃ³buj generycznego
            if len(keyword.split()) > 1:
                # Dla fraz wielowyrazowych - uÅ¼yj pierwszego sÅ‚owa
                synonym = keyword.split()[0]
            else:
                # Dla pojedynczych sÅ‚Ã³w - pomiÅ„
                applied_fixes.append(f"Brak synonimu dla '{keyword}' - wymaga rÄ™cznej poprawy")
                continue
        
        # ZnajdÅº i zamieÅ„ nadmiarowe wystÄ…pienia (od koÅ„ca, Å¼eby nie zmieniÄ‡ pozycji)
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        matches = list(pattern.finditer(fixed_text))
        
        # Zostaw 'limit' wystÄ…pieÅ„, zamieÅ„ resztÄ™
        if len(matches) > limit:
            # ZamieÅ„ wystÄ…pienia od koÅ„ca (Å¼eby indeksy siÄ™ nie przesunÄ™Å‚y)
            for match in reversed(matches[limit:excess + limit]):
                start, end = match.start(), match.end()
                original = fixed_text[start:end]
                
                # Zachowaj wielkoÅ›Ä‡ liter
                if original[0].isupper():
                    replacement = synonym[0].upper() + synonym[1:]
                else:
                    replacement = synonym
                
                fixed_text = fixed_text[:start] + replacement + fixed_text[end:]
                applied_fixes.append(f"'{original}' â†’ '{replacement}'")
    
    return fixed_text, applied_fixes


@dataclass
class ReviewIssue:
    type: str
    severity: str  # critical, warning, suggestion
    description: str
    location: str = ""
    fix_applied: bool = False
    auto_fixable: bool = False  # ðŸ†• v33.5


@dataclass
class DiffChange:
    type: str  # "removed", "added", "context"
    text: str
    line_num: int = 0


@dataclass
class DiffSummary:
    lines_changed: int = 0
    words_removed: int = 0
    words_added: int = 0
    changes: List[DiffChange] = field(default_factory=list)


@dataclass
class ReviewResult:
    status: str  # APPROVED, CORRECTED, REJECTED, QUICK_CHECK_FAILED, AUTO_FIXED
    original_text: str
    corrected_text: Optional[str]
    issues: List[ReviewIssue]
    summary: str
    word_count: int = 0
    paragraph_count: int = 0
    diff: Optional[DiffSummary] = None
    semantic_diversity: Optional[Dict] = None
    grammar_lt: Optional[Dict] = None
    auto_fixes_applied: List[str] = field(default_factory=list)  # ðŸ†• v33.5


# ================================================================
# QUICK CHECKS - ðŸ”§ v33.5 OPTIMIZED
# ================================================================

# Import lemmatyzacji dla quick checks
try:
    from polish_lemmatizer import count_phrase_occurrences
    _LEMMATIZER_OK = True
except ImportError:
    _LEMMATIZER_OK = False
    print("[CLAUDE_REVIEWER] âš ï¸ polish_lemmatizer not available, using exact match")


def quick_check_keywords(text: str, required: List[Dict]) -> Tuple[List[str], List[str], Dict]:
    """
    ðŸ”§ v33.5 OPTIMIZED: TYLKO STUFFING BLOKUJE, z dynamicznymi limitami!
    
    ZMIANY:
    - Dynamiczne limity zamiast staÅ‚ych 3Ã—
    - Auto-fix przed odrzuceniem
    """
    text_lower = text.lower()
    word_count = len(text.split())
    
    missing_basic = []
    missing_extended = []
    stuffing_errors = []
    warnings = []
    stuffed_for_autofix = []  # ðŸ†• Do auto-fix
    
    for kw in required:
        keyword = kw.get("keyword", "")
        count_req = kw.get("count", 1)
        kw_type = kw.get("type", "BASIC").upper()
        
        if not keyword:
            continue
        
        # Licz z lemmatyzacjÄ…
        if _LEMMATIZER_OK:
            result = count_phrase_occurrences(text, keyword)
            count_found = result.get("count", 0)
        else:
            count_found = text_lower.count(keyword.lower())
        
        # ðŸ†• v33.5: Dynamiczny limit
        kw_word_count = len(keyword.split())
        base_limit = count_req * 3
        
        # Bonus dla fraz wielowyrazowych
        if kw_word_count >= 3:
            base_limit = int(base_limit * 1.4)
        elif kw_word_count >= 2:
            base_limit = int(base_limit * 1.2)
        
        # Minimum 3 dla krÃ³tkich tekstÃ³w
        count_max = max(3, base_limit)
        
        if count_found == 0:
            warnings.append(f'"{keyword}" (0/{count_req}) - brak, do uzupeÅ‚nienia')
            if kw_type == "EXTENDED":
                missing_extended.append(keyword)
            else:
                missing_basic.append(keyword)
        elif count_found > count_max:
            stuffing_errors.append(f'"{keyword}" ({count_found}Ã—) - STUFFING! Max {count_max}Ã—')
            stuffed_for_autofix.append({
                "keyword": keyword,
                "count": count_found,
                "limit": count_max,
                "type": kw_type
            })
        elif count_found < count_req:
            warnings.append(f'"{keyword}" ({count_found}/{count_req}) - OK')
    
    critical = stuffing_errors
    
    missing_info = {
        "basic": missing_basic,
        "extended": missing_extended,
        "stuffed_for_autofix": stuffed_for_autofix  # ðŸ†•
    }
    
    return critical, warnings, missing_info


def quick_check_text_quality(text: str) -> Tuple[List[str], List[str]]:
    """
    ðŸ”§ v33.5 OPTIMIZED: Mniej restrykcyjne sprawdzanie jakoÅ›ci.
    """
    critical = []
    warnings = []
    
    sentences = re.split(r'[.!?]+', text)
    
    for i, sentence in enumerate(sentences, 1):
        sentence = sentence.strip()
        if not sentence:
            continue
        
        words = sentence.lower().split()
        
        # 1. TAUTOLOGIE - tylko jeÅ›li >= 3 powtÃ³rzenia (byÅ‚o 2)
        word_counts = {}
        for w in words:
            w_clean = re.sub(r'[^\w]', '', w)
            if len(w_clean) >= 5:  # ðŸ”§ byÅ‚o 4
                word_counts[w_clean] = word_counts.get(w_clean, 0) + 1
        
        for word, count in word_counts.items():
            if count >= 3 and word not in ['jest', 'oraz', 'ktÃ³re', 'ktÃ³ry', 'ktÃ³ra', 'takÅ¼e', 'bardzo', 'moÅ¼e', 'jednak']:
                warnings.append(f'Zdanie {i}: "{word}" uÅ¼yte {count}Ã— - rozwaÅ¼ synonim')
        
        # 2. ZBYT DÅUGIE ZDANIE - zwiÄ™kszony limit z 35 do 45
        if len(words) > 45:  # ðŸ”§ byÅ‚o 35
            warnings.append(f'Zdanie {i}: {len(words)} sÅ‚Ã³w - rozwaÅ¼ podziaÅ‚')
    
    return critical, warnings  # ðŸ”§ Mniej critical, wiÄ™cej warnings


def quick_check_length(text: str, min_w: int, max_w: int) -> Tuple[Optional[str], int]:
    words = len(text.split())
    # ðŸ”§ v33.5: Bardziej elastyczne progi (0.7 zamiast 0.8, 1.4 zamiast 1.3)
    if words < min_w * 0.7:
        return f"Za krÃ³tki: {words} sÅ‚Ã³w (min: {min_w})", words
    elif words > max_w * 1.4:
        return f"Za dÅ‚ugi: {words} sÅ‚Ã³w (max: {max_w})", words
    return None, words


def quick_check_forbidden(text: str, forbidden: List[str]) -> List[str]:
    text_lower = text.lower()
    return [f for f in forbidden if f and f.lower() in text_lower]


def quick_check_ai_patterns(text: str) -> List[str]:
    patterns = [
        "w dzisiejszych czasach", "warto wiedzieÄ‡", "nie jest tajemnicÄ…",
        "podsumowujÄ…c", "w niniejszym artykule", "jak wiadomo"
    ]
    # ðŸ”§ v33.5: UsuniÄ™to mniej problematyczne wzorce
    # "przykÅ‚ad:", "na przykÅ‚ad,", "wyobraÅºmy sobie", "zaÅ‚Ã³Å¼my, Å¼e" - to sÄ… OK
    
    text_lower = text.lower()
    return [p for p in patterns if p in text_lower]


def run_quick_checks(text: str, context: Dict) -> Dict:
    """
    ðŸ”§ v33.5 OPTIMIZED: Mniej restrykcyjne quick checks z auto-fix.
    """
    critical_errors = []
    warnings = []
    suggestions = []
    auto_fix_candidates = []  # ðŸ†•
    
    # PRIORYTET 1: JAKOÅšÄ† TEKSTU
    quality_critical, quality_warnings = quick_check_text_quality(text)
    for err in quality_critical:
        critical_errors.append({"type": "quality", "severity": "critical", "msg": err})
    for warn in quality_warnings:
        warnings.append({"type": "quality", "severity": "warning", "msg": warn})
    
    # AI patterns
    ai = quick_check_ai_patterns(text)
    if len(ai) >= 2:  # ðŸ”§ Tylko jeÅ›li >= 2 (byÅ‚o any)
        warnings.append({"type": "ai_pattern", "severity": "warning", "msg": f"AI patterns: {', '.join(ai)}"})
    
    # PRIORYTET 2: DÅUGOÅšÄ†
    len_err, words = quick_check_length(
        text, 
        context.get("target_words_min", 150),
        context.get("target_words_max", 400)
    )
    if len_err:
        warnings.append({"type": "length", "severity": "warning", "msg": len_err})  # ðŸ”§ warning zamiast critical
    
    # PRIORYTET 3: KEYWORDS (z auto-fix)
    keywords_critical, keywords_warnings, missing_info = quick_check_keywords(
        text, 
        context.get("keywords_required", [])
    )
    
    for err in keywords_critical:
        # ðŸ†• v33.5: SprawdÅº czy moÅ¼na auto-fix
        if missing_info.get("stuffed_for_autofix"):
            critical_errors.append({
                "type": "stuffing", 
                "severity": "critical", 
                "msg": err,
                "auto_fixable": True,
                "fix_data": missing_info["stuffed_for_autofix"]
            })
        else:
            critical_errors.append({"type": "stuffing", "severity": "critical", "msg": err})
    
    for warn in keywords_warnings:
        warnings.append({"type": "keyword", "severity": "warning", "msg": warn})
    
    # PRIORYTET 4: FORBIDDEN
    forbidden = quick_check_forbidden(text, context.get("keywords_forbidden", []))
    if forbidden:
        for f in forbidden[:2]:
            warnings.append({"type": "forbidden", "severity": "warning", "msg": f"UÅ¼yto zakazanej frazy: '{f}'"})
    
    # Paragraphs
    paragraphs = [p for p in text.split('\n\n') if p.strip()]
    paragraph_count = len(paragraphs)
    
    # ðŸ†• v33.5: Auto-fix jeÅ›li moÅ¼liwe
    auto_fixed_text = None
    auto_fixes_applied = []
    
    if any(e.get("auto_fixable") for e in critical_errors):
        stuffed_data = []
        for e in critical_errors:
            if e.get("fix_data"):
                stuffed_data.extend(e["fix_data"])
        
        if stuffed_data:
            auto_fixed_text, auto_fixes_applied = auto_fix_stuffing(text, stuffed_data)
            if auto_fixes_applied:
                print(f"[CLAUDE_REVIEWER] ðŸ”§ Auto-fix applied: {len(auto_fixes_applied)} changes")
    
    # Tylko stuffing blokuje (ale moÅ¼e byÄ‡ auto-fixed)
    has_unfixable_critical = any(
        e["type"] == "stuffing" and not e.get("auto_fixable") 
        for e in critical_errors
    )
    
    passed = not has_unfixable_critical or auto_fixed_text is not None
    
    return {
        "passed": passed,
        "errors": critical_errors,
        "warnings": warnings,
        "suggestions": suggestions,
        "word_count": words,
        "paragraph_count": paragraph_count,
        "missing_phrases": missing_info,
        "auto_fixed_text": auto_fixed_text,  # ðŸ†•
        "auto_fixes_applied": auto_fixes_applied  # ðŸ†•
    }


# ================================================================
# GÅÃ“WNA FUNKCJA
# ================================================================

def review_batch(text: str, context: Dict, skip_claude: bool = False) -> ReviewResult:
    """
    ðŸ”§ v33.5 OPTIMIZED: PeÅ‚ny review z auto-fix.
    """
    # Quick checks
    qc = run_quick_checks(text, context)
    
    # ðŸ†• v33.5: UÅ¼yj auto-fixed text jeÅ›li dostÄ™pny
    working_text = qc.get("auto_fixed_text") or text
    auto_fixes = qc.get("auto_fixes_applied", [])
    
    if not qc["passed"]:
        issues = [ReviewIssue(e["type"], "critical", e["msg"], auto_fixable=e.get("auto_fixable", False)) for e in qc["errors"]]
        issues += [ReviewIssue(w["type"], "warning", w["msg"]) for w in qc["warnings"]]
        
        # ðŸ†• JeÅ›li byÅ‚y auto-fixes, zwrÃ³Ä‡ CORRECTED zamiast REJECTED
        if auto_fixes:
            return ReviewResult(
                "AUTO_FIXED",  # ðŸ†• Nowy status
                text,
                working_text,
                issues,
                f"Auto-naprawiono {len(auto_fixes)} problemÃ³w",
                qc["word_count"],
                qc["paragraph_count"],
                auto_fixes_applied=auto_fixes
            )
        
        return ReviewResult(
            "QUICK_CHECK_FAILED", text, None, issues,
            "Popraw bÅ‚Ä™dy krytyczne (stuffing)",
            qc["word_count"], qc["paragraph_count"]
        )
    
    if skip_claude:
        issues = [ReviewIssue(w["type"], "warning", w["msg"]) for w in qc["warnings"]]
        
        # ðŸ†• JeÅ›li byÅ‚y auto-fixes, zwrÃ³Ä‡ AUTO_FIXED
        if auto_fixes:
            return ReviewResult(
                "AUTO_FIXED",
                text,
                working_text,
                issues,
                f"Auto-naprawiono {len(auto_fixes)} problemÃ³w",
                qc["word_count"],
                qc["paragraph_count"],
                auto_fixes_applied=auto_fixes
            )
        
        return ReviewResult(
            "APPROVED", text, None, issues,
            "Quick check OK",
            qc["word_count"], qc["paragraph_count"]
        )
    
    # PrzekaÅ¼ brakujÄ…ce frazy do kontekstu Claude
    missing = qc.get("missing_phrases", {})
    context["missing_basic"] = missing.get("basic", [])
    context["missing_extended"] = missing.get("extended", [])
    
    # Claude review na working_text (moÅ¼e byÄ‡ juÅ¼ auto-fixed)
    result = review_with_claude(working_text, context)
    
    # ðŸ†• Dodaj info o auto-fixes
    if auto_fixes:
        result.auto_fixes_applied = auto_fixes
        if result.status == "APPROVED":
            result.status = "AUTO_FIXED"
            result.original_text = text
            result.corrected_text = working_text
    
    # Dodaj warnings z quick check
    for w in qc["warnings"]:
        if not any(i.fix_applied and i.type == w["type"] for i in result.issues):
            result.issues.append(ReviewIssue(w["type"], "warning", w["msg"]))
    
    return result


def review_with_claude(text: str, ctx: Dict) -> ReviewResult:
    """Claude review - bez zmian od v33.4"""
    if not ANTHROPIC_AVAILABLE or not os.environ.get("ANTHROPIC_API_KEY"):
        return ReviewResult("APPROVED", text, None, [], "Claude niedostÄ™pny", len(text.split()))
    
    try:
        client = anthropic.Anthropic()
        
        response = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=4000,
            messages=[{"role": "user", "content": build_review_prompt(text, ctx)}]
        )
        
        resp_text = response.content[0].text
        json_match = re.search(r'\{[\s\S]*\}', resp_text)
        
        if not json_match:
            return ReviewResult("APPROVED", text, None, [], "Brak JSON w odpowiedzi", len(text.split()))
        
        data = json.loads(json_match.group())
        
        issues = [ReviewIssue(
            type=i.get("type", ""),
            severity=i.get("severity", "warning"),
            description=i.get("description", ""),
            fix_applied=i.get("fix_applied", False)
        ) for i in data.get("issues", [])]
        
        status = data.get("status", "APPROVED")
        corrected = data.get("corrected_text")
        
        if status == "CORRECTED" and (not corrected or len(corrected) < 50):
            status = "APPROVED"
            corrected = None
        
        final = corrected if corrected else text
        
        return ReviewResult(
            status=status,
            original_text=text,
            corrected_text=corrected,
            issues=issues,
            summary=data.get("summary", ""),
            word_count=len(final.split()),
            paragraph_count=len([p for p in final.split('\n\n') if p.strip()])
        )
        
    except Exception as e:
        print(f"[CLAUDE_REVIEWER] Error: {e}")
        return ReviewResult("APPROVED", text, None, [], f"BÅ‚Ä…d: {e}", len(text.split()))


def build_review_prompt(text: str, ctx: Dict) -> str:
    """Buduje prompt dla Claude review."""
    return f"""Przejrzyj poniÅ¼szy tekst SEO i zwrÃ³Ä‡ JSON:

TEKST:
{text}

KONTEKST:
- Temat: {ctx.get('topic', '')}
- SÅ‚owa kluczowe wymagane: {json.dumps(ctx.get('keywords_required', []), ensure_ascii=False)}
- BrakujÄ…ce BASIC: {json.dumps(ctx.get('missing_basic', []), ensure_ascii=False)}
- BrakujÄ…ce EXTENDED: {json.dumps(ctx.get('missing_extended', []), ensure_ascii=False)}

ZWRÃ“Ä† JSON:
{{
  "status": "APPROVED" | "CORRECTED" | "REJECTED",
  "issues": [
    {{"type": "string", "severity": "critical|warning|suggestion", "description": "string", "fix_applied": bool}}
  ],
  "corrected_text": "string (jeÅ›li CORRECTED)",
  "summary": "string"
}}

ZASADY:
1. APPROVED = tekst OK
2. CORRECTED = naprawiÅ‚eÅ› drobne bÅ‚Ä™dy (zwrÃ³Ä‡ corrected_text)
3. REJECTED = powaÅ¼ne problemy (unikaj - lepiej napraw)
4. JeÅ›li brakuje fraz - dodaj je naturalnie w corrected_text
"""


def build_context_from_pre_batch(pre_batch: Dict, project: Dict = None) -> Dict:
    """Helper: buduje context z getPreBatchInfo."""
    keywords_required = []
    
    main_kw = pre_batch.get("main_keyword", {})
    if main_kw.get("keyword"):
        keywords_required.append({
            "keyword": main_kw["keyword"],
            "count": main_kw.get("info", {}).get("use_this_batch", 2)
        })
    
    kw = pre_batch.get("keywords", {})
    for k in kw.get("basic_must_use", [])[:8]:
        if k.get("keyword"):
            keywords_required.append({"keyword": k["keyword"], "count": 1})
    for k in kw.get("extended_this_batch", [])[:4]:
        if k.get("keyword"):
            keywords_required.append({"keyword": k["keyword"], "count": 1})
    
    forbidden = [k.get("keyword") for k in kw.get("locked_exceeded", []) if k.get("keyword")]
    forbidden += kw.get("extended_done", [])
    
    bl = pre_batch.get("batch_length", {})
    
    last = ""
    if project:
        content = project.get("article_content", "")
        if content:
            last = content[-200:]
    
    return {
        "topic": pre_batch.get("topic", ""),
        "h2_current": pre_batch.get("h2_remaining", [])[:2],
        "keywords_required": keywords_required,
        "keywords_forbidden": [f for f in forbidden if f],
        "last_sentences": last,
        "target_words_min": bl.get("suggested_min", 200),
        "target_words_max": bl.get("suggested_max", 500),
        "target_paragraphs_min": bl.get("paragraphs_min", 2),
        "target_paragraphs_max": bl.get("paragraphs_max", 5),
        "main_keyword": main_kw.get("keyword", ""),
        "main_keyword_count": main_kw.get("info", {}).get("use_this_batch", 2),
        "batch_number": pre_batch.get("batch_number", 1),
        "snippet_required": bl.get("snippet_required", True),
        "complexity_score": bl.get("complexity_score", 50)
    }
