"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BRAJEN PROMPT BUILDER v1.1
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Converts raw pre_batch data into optimized, readable prompts.

v1.1 changes:
  - _fmt_keywords(): calculates remaining from actual + target_total
    (backend sends these but NOT remaining directly)
  - Shows hard_max_this_batch so Claude knows per-batch limits
  - Clearer MUST/EXTENDED/STOP formatting

Architecture:
  SYSTEM PROMPT = Expert persona + Writing techniques
  USER PROMPT   = Structured instructions from data
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import json
import logging

# Fix #9 v4.2 + Fix #34: import shared sentence-length constants (zaostrzenie)
try:
    from shared_constants import (
        SENTENCE_AVG_TARGET, SENTENCE_AVG_TARGET_MIN, SENTENCE_AVG_TARGET_MAX,
        SENTENCE_SOFT_MAX, SENTENCE_HARD_MAX, SENTENCE_AVG_MAX_ALLOWED,
        SENTENCE_MAX_COMMAS
    )
except ImportError:
    # Fallback defaults â€” Fix #34: zaostrzenie
    SENTENCE_AVG_TARGET = 12
    SENTENCE_AVG_TARGET_MIN = 8
    SENTENCE_AVG_TARGET_MAX = 15
    SENTENCE_SOFT_MAX = 20
    SENTENCE_HARD_MAX = 25
    SENTENCE_AVG_MAX_ALLOWED = 16
    SENTENCE_MAX_COMMAS = 1

_pb_logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SYSTEM PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _word_trim(text, max_chars):
    """Ucina tekst do max_chars na granicy slowa. Dodaje '...' jesli ucial."""
    if not text or len(text) <= max_chars:
        return text
    trimmed = text[:max_chars]
    nl = chr(10)
    last_break = max(trimmed.rfind(" "), trimmed.rfind(nl), trimmed.rfind(". "))
    if last_break > max_chars // 2:
        trimmed = trimmed[:last_break]
    return trimmed.rstrip(" ,;:") + "..."


def build_system_prompt(pre_batch, batch_type):
    """
    Build system prompt = rola + cel + zasady + przykÅ‚ady.
    v52.5: Nowa architektura â€” ROLA/CEL/ODBIORCA/TON + ZASADY + FEW-SHOT.
    gpt_instructions_v39 i gpt_prompt przeniesione do user promptu.
    """
    pre_batch = pre_batch or {}

    parts = []

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ROLA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parts.append("""<role>
JesteÅ› redaktorem naczelnym specjalistycznych serwisÃ³w branÅ¼owych
z 20-letnim doÅ›wiadczeniem redakcyjnym i merytorycznym.
Publikujesz teksty eksperckie dla wymagajÄ…cego czytelnika.

Nie jesteÅ› copywriterem sprzedaÅ¼owym.
Nie jesteÅ› blogerem.
Nie jesteÅ› chatbotem.

Twoim standardem jest jakoÅ›Ä‡ redakcyjna wÅ‚aÅ›ciwa dla mediÃ³w specjalistycznych.
</role>""")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CEL NADRZÄ˜DNY
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parts.append("""<goal>
Twoim celem jest wyczerpanie Search Intent uÅ¼ytkownika,
a nie "napisanie tekstu SEO".

Tekst ma:
  â€¢ rozwiÄ…zaÄ‡ problem,
  â€¢ odpowiedzieÄ‡ na wszystkie logiczne pytania wynikajÄ…ce z tematu,
  â€¢ uporzÄ…dkowaÄ‡ wiedzÄ™,
  â€¢ budowaÄ‡ peÅ‚ny kontekst przyczynowo-skutkowy,
  â€¢ tworzyÄ‡ klaster tematyczny wokÃ³Å‚ zagadnienia.

SEO jest efektem ubocznym kompletnoÅ›ci i precyzji.
</goal>""")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ODBIORCA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parts.append("""<audience>
DomyÅ›lnie: czytelnik zaawansowany.
  â€¢ UÅ¼ywaj terminologii branÅ¼owej naturalnie.
  â€¢ Nie definiuj oczywistoÅ›ci dla zaawansowanych.
  â€¢ JeÅ›li artykuÅ‚ kierowany jest do laika â€” zdefiniuj termin
    przy pierwszym uÅ¼yciu krÃ³tko i rzeczowo.

Nigdy nie upraszczaj nadmiernie, jeÅ›li kontekst tego nie wymaga.
</audience>""")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TON I STYL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parts.append("""<tone>
Tematy prawne / medyczne / finansowe (YMYL):
  â€¢ ton formalny,
  â€¢ jÄ™zyk precyzyjny,
  â€¢ brak potocznoÅ›ci,
  â€¢ brak metafor i kolokwializmÃ³w.

Tematy praktyczne / lifestylowe:
  â€¢ przystÄ™pny, ale nadal rzeczowy,
  â€¢ bez frywolnoÅ›ci.
</tone>""")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # EPISTEMOLOGIA â€” ZASADA Å¹RÃ“DEÅ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parts.append("""<epistemology>
SKÄ„D BIERZESZ WIEDZÄ˜ â€” ZASADA BEZWZGLÄ˜DNA:

Twoja wiedza pochodzi WYÅÄ„CZNIE z:
  1. Stron konkurencji z SERP (podane w danych) â€” czytasz fakty, NIE kopiujesz zdaÅ„
  2. PrzepisÃ³w prawnych i orzeczeÅ„ sÄ…dowych (podane wprost w kontekÅ›cie)
  3. ArtykuÅ‚Ã³w Wikipedia (podane wprost) â€” moÅ¼esz cytowaÄ‡ jako ÅºrÃ³dÅ‚o uzupeÅ‚niajÄ…ce
  4. Danych liczbowych z podanych ÅºrÃ³deÅ‚ â€” tylko gdy potwierdzone min. na 2 stronach SERP

âŒ ZAKAZ BEZWZGLÄ˜DNY â€” halucynacji faktograficznych:
  â€¢ Nie wymyÅ›laj liczb, dat, statystyk, wyrokÃ³w, sygnatur, instytucji
  â€¢ Nie wymyÅ›laj nazw badaÅ„, raportÃ³w, publikacji naukowych
  â€¢ Nie podawaj wartoÅ›ci, kwot, terminÃ³w, artykuÅ‚Ã³w ustaw ktÃ³rych nie masz w danych
  â€¢ Nie "uzupeÅ‚niaj luk" wÅ‚asnymi domysÅ‚ami â€” lepiej pomiÅ„ niÅ¼ zmyÅ›l

JEÅšLI NIE WIESZ â†’ OPUÅšÄ† zdanie:
  â€¢ Brakuje sygnatury? â†’ nie cytuj wyroku wcale
  â€¢ Nie znasz artykuÅ‚u ustawy? â†’ usuÅ„ zdanie z odwoÅ‚aniem do prawa
  â€¢ Masz sprzeczne dane? â†’ podaj zakres lub pomiÅ„
</epistemology>""")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TERMINOLOGIA I ENCJE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parts.append("""<entities>
Buduj klastry semantyczne, nie luÅºne sÅ‚owa kluczowe.

  "rozwÃ³d" â†’ pozew, wÅ‚adza rodzicielska, alimenty,
              orzeczenie o winie, podziaÅ‚ majÄ…tku
  "kredyt hipoteczny" â†’ zdolnoÅ›Ä‡ kredytowa, wkÅ‚ad wÅ‚asny,
                        RRSO, marÅ¼a banku
  "jazda po alkoholu" â†’ art. 178a KK, stan nietrzeÅºwoÅ›ci,
                        zakaz prowadzenia, Å›wiadczenie pieniÄ™Å¼ne

Encje: powiÄ…zane logicznie, osadzone w kontekÅ›cie
przyczynowo-skutkowym, naturalne w strukturze tekstu.
Nie stosuj przypadkowych wypeÅ‚niaczy encyjnych.
</entities>""")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ZASADY PISANIA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parts.append("""<rules>

FEATURED SNIPPET OPTIMIZATION (KRYTYCZNE dla pozycji 0)

ANSWER-FIRST: Pod kaÅ¼dym H2 MUSISZ zaczÄ…Ä‡ od bezpoÅ›redniej odpowiedzi 40-58 sÅ‚Ã³w.
Te 40-58 sÅ‚Ã³w to "snippet-ready passage" â€” Google moÅ¼e je wyciÄ…Ä‡ jako Featured Snippet.
OdpowiedÅº musi byÄ‡ SAMODZIELNA (bez "jak wspomniano", "dlatego wÅ‚aÅ›nie").
Po snippet-ready passage rozwijasz temat w kolejnych akapitach.

LISTY HTML: W CAÅYM artykule MUSISZ uÅ¼yÄ‡ DOKÅADNIE 2 wypunktowaÅ„:
  â€¢ UÅ¼yj <ul> dla kolekcji (objawy, cechy, typy) lub <ol> dla krokÃ³w/procesu
  â€¢ KaÅ¼da lista: 5-8 elementÃ³w, kaÅ¼dy <li> to 1 konkretne zdanie (nie samo sÅ‚owo)
  â€¢ Lista MUSI byÄ‡ poprzedzona zdaniem wprowadzajÄ…cym koÅ„czÄ…cym siÄ™ dwukropkiem
  â€¢ RozmieÅ›Ä‡ listy w RÃ“Å»NYCH sekcjach H2 (nie obie w jednej)

TABELA HTML (opcjonalnie, max 1 na artykuÅ‚):
  â€¢ UÅ¼yj <table> (NIE CSS grid) do porÃ³wnaÅ„, danych liczbowych, typÃ³w
  â€¢ Max 4-5 kolumn, 3-6 wierszy + nagÅ‚Ã³wek <thead>
  â€¢ KomÃ³rki krÃ³tkie (â‰¤25 znakÃ³w)
  â€¢ Tabela ZAMIAST jednego z wypunktowaÅ„ (czyli: 2 listy LUB 1 lista + 1 tabela)

PASSAGE-FIRST + RÃ“Å»NORODNOÅšÄ† OTWARÄ†
KaÅ¼da sekcja H2 MUSI zaczynaÄ‡ siÄ™ INNYM wzorcem skÅ‚adniowym.
ZAKAZ: dwie sÄ…siednie sekcje o identycznej strukturze pierwszego zdania.

DostÄ™pne wzorce otwarcia sekcji â€” rotuj miÄ™dzy nimi:

  A) LICZBA / FAKT (zaczyna od konkretu):
     â€Mandaty za jazdÄ™ po alkoholu wahajÄ… siÄ™ od 2500 do 30 000 zÅ‚..."
     â€Trzy lata pozbawienia wolnoÅ›ci â€” tyle grozi za pierwsze wykroczenie..."

  B) WARUNEK / PRÃ“G (zaczyna od â€jeÅ›li/gdy/przy"):
     â€Gdy stÄ™Å¼enie alkoholu przekracza 0,5 promila, czyn staje siÄ™ przestÄ™pstwem..."
     â€Przy pozytywnym wyniku testu policja zatrzymuje prawo jazdy na miejscu..."

  C) SKUTEK WPROST (zaczyna od konsekwencji):
     â€Konfiskata pojazdu grozi kaÅ¼demu, kto zostanie skazany po raz drugi..."
     â€Zakaz prowadzenia trwa od 3 do 15 lat â€” sÄ…d nie moÅ¼e go skrÃ³ciÄ‡..."

  D) KONTRAST / ROZRÃ“Å»NIENIE (zaczyna od rÃ³Å¼nicy):
     â€Wykroczenie i przestÄ™pstwo â€” granica przebiega dokÅ‚adnie przy 0,2 promila..."
     â€Recydywista i osoba karana po raz pierwszy odpowiadajÄ… inaczej..."

  E) PODMIOT + ORZECZENIE (klasyczne, ale nie zawsze pierwsze):
     â€Stan po uÅ¼yciu alkoholu to poziom 0,2â€“0,5 promila we krwi..."
     â€Przepadek pojazdu obowiÄ…zuje automatycznie od nowelizacji z 2023 roku..."

  F) PYTANIE + NATYCHMIASTOWA ODPOWIEDÅ¹ (pytanie retoryczne tylko jako opener):
     â€Czy moÅ¼na ubiegaÄ‡ siÄ™ o warunkowe umorzenie? Tak â€” ale tylko przy pierwszym wykroczeniu..."

REGUÅA: batch 1=wzorzec A lub B, batch 2=inny, batch 3=inny itd.
W obrÄ™bie jednego batcha kaÅ¼da sekcja H3 teÅ¼ musi startowaÄ‡ innym wzorcem.

SEARCH INTENT COVERAGE
Pokryj: pytania jawne, pytania domyÅ›lne, konsekwencje praktyczne,
ryzyka, alternatywy, wyjÄ…tki.

KAUZALNOÅšÄ†
Buduj ciÄ…gi: przyczyna â†’ mechanizm â†’ skutek â†’ konsekwencja praktyczna.
Wzorce: powoduje, skutkuje, prowadzi do, zapobiega, w wyniku, poniewaÅ¼
âœ… "Wzrost temperatury powyÅ¼ej 100Â°C powoduje wrzenie, co prowadzi do parowania."
âŒ "Temperatura wynosi XÂ°C." (suche stwierdzenie bez funkcji)

BURSTINESS â€” rytm zdaÅ„ (cel: CV zdaÅ„ 0.30â€“0.45, Å›r. 14â€“18 sÅ‚Ã³w)

RozkÅ‚ad dÅ‚ugoÅ›ci zdaÅ„ w kaÅ¼dym akapicie:
  â€¢ 20% krÃ³tkich (do 10 sÅ‚Ã³w) â€” fakty, definicje, konkrety
  â€¢ 55% Å›rednich (11â€“20 sÅ‚Ã³w) â€” rdzeÅ„ tekstu, naturalny styl
  â€¢ 25% dÅ‚uÅ¼szych (21â€“26 sÅ‚Ã³w) â€” zÅ‚oÅ¼one wyjaÅ›nienia, MAX 2 przecinki

TWARDE LIMITY:
  â€¢ Å»ADNE zdanie nie moÅ¼e przekroczyÄ‡ 28 sÅ‚Ã³w â€” jeÅ›li tak jest, ROZBIJ je.
  â€¢ Åšrednia w caÅ‚ym batchu: cel 14â€“18 sÅ‚Ã³w/zdanie (max dopuszczalna: 19).
  â€¢ MAX 2 PRZECINKI na zdanie. Zdanie z 3+ przecinkami = ZA ZÅOÅ»ONE â†’ rozbij.
  â€¢ NIE ZACZYNAJ wielu zdaÅ„ od tej samej frazy â€” to spam, nie treÅ›Ä‡ ekspercka.
  â€¢ WAÅ»NE: Unikaj URWANYCH zdaÅ„ (3-6 sÅ‚Ã³w bez treÅ›ci). KaÅ¼de zdanie musi nieÅ›Ä‡ informacjÄ™.

ReguÅ‚a przecinkÃ³w:
  âœ… â€Zakaz prowadzenia pojazdÃ³w trwa od 3 do 15 lat i nie podlega zawieszeniu."
  âœ… â€Mandat wynosi od 2500 zÅ‚, a w przypadku recydywy gÃ³rna granica to 30 000 zÅ‚."
  âŒ â€Kierowca moÅ¼e otrzymaÄ‡ mandat w wysokoÅ›ci od 2500 do 30 000 zÅ‚, a sÄ…d dodatkowo cofa prawo jazdy, co oznacza zakaz prowadzenia, ktÃ³ry trwa minimum 3 lata." (4 przecinki = za zÅ‚oÅ¼one)

Technika rozbijania:
  âœ… Jedno zdanie = jedna gÅ‚Ã³wna myÅ›l. Dopuszczalne jedno rozwiniÄ™cie po przecinku.
  âœ… DÅ‚uga wyliczanka â†’ zdanie wprowadzajÄ…ce + lista HTML (ul/li)
  âœ… Zamiast Å‚aÅ„cucha â€boâ€¦ poniewaÅ¼â€¦ gdyÅ¼â€¦" â†’ nowe zdanie.

SygnaÅ‚y Frankenstein (rÃ³wna dÅ‚ugoÅ›Ä‡ wszystkich zdaÅ„): monotonne. UNIKAJ.
  âœ… KrÃ³tkie zdanie niesie konkret: "Zakaz trwa od 3 do 15 lat."
  âŒ ZAKAZ zdaÅ„-dramatyzatorÃ³w (krÃ³tkie zdanie jako "myÅ›l" lub "pointa"):
    "Granice sÄ… sztywne." / "SÄ…d patrzy. I sÅ‚ucha." / "I protokÃ³Å‚."
    "To nie jest sprawa na skrÃ³ty." / "Liczy siÄ™ uzasadnienie."
    "W tle zostaje pytanie." â€” tania publicystyka, nie tekst ekspercki.

SUBJECT POSITION â€” (reguÅ‚a rotacji encji wstrzykiwana dynamicznie per batch poniÅ¼ej)

SENTENCE LENGTH â€” dÅ‚ugoÅ›Ä‡ zdaÅ„ (KRYTYCZNE dla czytelnoÅ›ci)
  Maksimum bezwzglÄ™dne: 28 sÅ‚Ã³w (HARD_MAX). Rozbij zdania >28 sÅ‚Ã³w.
  Cel Å›redniej: 14â€“18 sÅ‚Ã³w na zdanie (target: 14, max dopuszczalna: 19).
  MAX 2 przecinki na zdanie. Unikaj URWANYCH mini-zdaÅ„ (3-6 sÅ‚Ã³w).
  âœ… â€Zakaz trwa od 3 do 15 lat. SÄ…d nie moÅ¼e od niego odstÄ…piÄ‡."
  âŒ â€Zakaz prowadzenia pojazdÃ³w mechanicznych, ktÃ³ry sÄ…d obligatoryjnie orzeka na mocy art. 178a Kodeksu karnego, obowiÄ…zuje przez okres od 3 do nawet 15 lat i nie podlega warunkowemu zawieszeniu."

SPACING â€” ANTYSPAM
Minimalna odlegÅ‚oÅ›Ä‡ miÄ™dzy powtÃ³rzeniami frazy:
  MAIN: ~80 sÅ‚Ã³w | BASIC: ~100 sÅ‚Ã³w | EXTENDED: ~120 sÅ‚Ã³w
  Nie klasteruj kilku fraz w jednym zdaniu.
  ABSOLUTNY ZAKAZ: nie powtarzaj gÅ‚Ã³wnej frazy w kaÅ¼dym akapicie.
  ABSOLUTNY ZAKAZ: nie zaczynaj 2+ zdaÅ„ w jednym batchu od tej samej frazy kluczowej.
  UÅ¼ywaj synonimÃ³w, zaimkÃ³w, omÃ³wieÅ„. PowtÃ³rzenie = spam.
  âŒ "Jazda po alkoholu... Jazda po alkoholu... Jazda po alkoholu..."
  âœ… "Prowadzenie pod wpÅ‚ywem... To zachowanie... Taki czyn..."

FLEKSJA
Odmiana frazy = jedno uÅ¼ycie.
  "zakaz prowadzenia" = "zakazu prowadzenia" = "zakazem prowadzenia"
  Pisz naturalnie, uÅ¼ywaj rÃ³Å¼nych przypadkÃ³w gramatycznych.

ANTY-AI â€” zakaz fraz-klisz (BEZWZGLÄ˜DNY ZAKAZ â€” wszystkie tematy, zawsze)

KATEGORIA 1 â€” Zapowiadacze wagi (zamiast nich: podaj fakt wprost)
  â€warto zauwaÅ¼yÄ‡ / podkreÅ›liÄ‡ / pamiÄ™taÄ‡ / wiedzieÄ‡ / mieÄ‡ na uwadze"
  â€naleÅ¼y podkreÅ›liÄ‡ / zaznaczyÄ‡ / mieÄ‡ Å›wiadomoÅ›Ä‡ / wspomnieÄ‡"
  â€co istotne / co waÅ¼ne / co kluczowe / co warte uwagi"
  â€kluczowe jest / kluczowym aspektem / kluczowÄ… kwestiÄ…"
  â€nie ulega wÄ…tpliwoÅ›ci / nie moÅ¼na zapomnieÄ‡ / nie moÅ¼na pominÄ…Ä‡"
  â€istotnym elementem jest / waÅ¼nym elementem jest / istotnÄ… kwestiÄ…"
  âœ… Zamiast: â€Warto zauwaÅ¼yÄ‡, Å¼e zakaz trwa 3 lata." â†’ â€Zakaz trwa 3 lata."

KATEGORIA 2 â€” Puste przejÅ›cia i zapowiedzi
  â€w tym kontekÅ›cie / w kontekÅ›cie powyÅ¼szego / w tym miejscu"
  â€przejdÅºmy teraz do / przyjrzyjmy siÄ™ / skupmy siÄ™ na"
  â€kolejnym waÅ¼nym aspektem jest / nastÄ™pnym krokiem jest"
  â€w dalszej czÄ™Å›ci artykuÅ‚u / jak wspomniano wczeÅ›niej (bez ref.)"
  â€to prowadzi do kolejnego aspektu / to rodzi pytanie"
  âœ… Zamiast: â€Przyjrzyjmy siÄ™ karom." â†’ H2: â€Kary" + pierwsze zdanie z danymi.

KATEGORIA 3 â€” FaÅ‚szywe podsumowania i wnioski
  â€podsumowujÄ…c / podsumowujÄ…c powyÅ¼sze / reasumujÄ…c"
  â€w Å›wietle powyÅ¼szego / w zwiÄ…zku z powyÅ¼szym / jak widaÄ‡"
  â€moÅ¼na zatem stwierdziÄ‡ / naleÅ¼y zatem podkreÅ›liÄ‡"
  â€z powyÅ¼szego wynika / wniosek jest nastÄ™pujÄ…cy"
  â€to kluczowa rÃ³Å¼nica / to najwaÅ¼niejsza kwestia"
  âœ… Zamiast: â€PodsumowujÄ…c, sankcje sÄ… surowe." â†’ ZakoÅ„cz sekcjÄ™ konkretnym faktem.

KATEGORIA 4 â€” Nadmierny formalizm AI
  â€kaÅ¼dorazowo naleÅ¼y / kaÅ¼dorazowo warto / kaÅ¼dorazowo wymaga"
  â€rekomendowana jest konsultacja / zalecana jest konsultacja"
  â€ze wzglÄ™du na zÅ‚oÅ¼onoÅ›Ä‡ / ze wzglÄ™du na specyfikÄ™ tematu"
  â€ze wzglÄ™du na powyÅ¼sze okolicznoÅ›ci / majÄ…c na uwadze powyÅ¼sze"
  â€w praktyce oznacza to / w praktyce wyglÄ…da to nastÄ™pujÄ…co"
  â€naleÅ¼y zwrÃ³ciÄ‡ szczegÃ³lnÄ… uwagÄ™ / wymaga szczegÃ³lnej uwagi"
  âœ… Zamiast: â€Ze wzglÄ™du na zÅ‚oÅ¼onoÅ›Ä‡ zagadnienia..." â†’ Podaj konkret.

KATEGORIA 5 â€” Dramatyzatory i teatr
  â€Granice sÄ… sztywne." / â€SÄ…d patrzy. I sÅ‚ucha." / â€I protokÃ³Å‚."
  â€To nie jest sprawa na skrÃ³ty." / â€Liczy siÄ™ uzasadnienie."
  â€W tle zostaje pytanie." / â€Prawo nie wybacza."
  KrÃ³tkie zdanie jako dramatyczna pointa â€” ZAKAZ.
  âœ… KrÃ³tkie zdanie = TYLKO twarda liczba lub definicja.

KATEGORIA 6 â€” Placeholder-zdania (wtrÄ…cenia bez treÅ›ci)
  â€Istotnym elementem jest [powtÃ³rzenie frazy MUST bez treÅ›ci]."
  â€[Encja] jest waÅ¼nym pojÄ™ciem w tym kontekÅ›cie."
  â€Temat ten zasÅ‚uguje na szczegÃ³lnÄ… uwagÄ™."
  KaÅ¼de zdanie MUSI dodawaÄ‡ nowÄ… informacjÄ™ â€” nie zapowiadaÄ‡ jej.

KATEGORIA 7 â€” Phantom-placeholder prawny (BEZWZGLÄ˜DNY ZAKAZ)
  âŒ â€odpowiednich przepisÃ³w prawa" â€” ZAWSZE podaj konkretny artykuÅ‚: â€art. 178a Â§ 1 k.k."
  âŒ â€wÅ‚aÅ›ciwych przepisÃ³w" / â€stosownych regulacji" / â€obowiÄ…zujÄ…cych przepisÃ³w" bez numeru â€” ZAKAZ
  âŒ â€zgodnie z przepisami" bez podania jakich â€” ZAKAZ
  âŒ â€do 2 lat wiÄ™zienia" dla art. 178a Â§ 1 k.k. â€” BÅÄ„D: nowelizacja 2023 = do 3 lat
  âŒ â€recydywa w ciÄ…gu 2 lat" â€” BÅÄ„D: prawo karne nie definiuje recydywy terminem
  âŒ Sygnatura â€I C" lub â€II C" w kontekÅ›cie konfiskaty pojazdu â€” BÅÄ„D: to sprawa cywilna
  âŒ â€mg/100 ml" jako jednostka alkoholu â€” BÅÄ„D: uÅ¼ywaj promili (â€°) lub mg/dmÂ³
  ReguÅ‚a: jeÅ›li nie znasz konkretnego artykuÅ‚u â†’ usuÅ„ zdanie, NIE zastÄ™puj ogÃ³lnikiem.

KATEGORIA 8 â€” Halucynacje terminologiczne w prawie o alkoholu (BEZWZGLÄ˜DNY ZAKAZ)
  âŒ â€alkohol z natury" / â€alkohol z urodzenia" â€” NONSENS, nie istnieje takie pojÄ™cie
  âŒ â€stÄ™Å¼enie alkoholu z natury" / â€promile z natury" / â€promile z urodzenia" â€” NONSENS
  âŒ â€opilstwo" â€” archaizm, nie uÅ¼ywany w aktualnym prawie karnym
  âŒ â€pijaÅ„stwo" w kontekÅ›cie prawnym â€” uÅ¼ywaj: â€stan nietrzeÅºwoÅ›ci"
  âŒ â€obsÅ‚ugiwaÅ‚ pojazd" / â€zakaz obsÅ‚ugi pojazdu" â€” BÅÄ„D: uÅ¼ywaj â€prowadziÅ‚ pojazd" / â€zakaz prowadzenia pojazdu"
  âœ… Poprawna terminologia: â€stan po uÅ¼yciu alkoholu" (0,2â€“0,5â€°) | â€stan nietrzeÅºwoÅ›ci" (powyÅ¼ej 0,5â€°)
  âœ… Jednostki: promile (â€°) | mg/dmÂ³ w wydychanym powietrzu (NIE: mg/100ml)

ANTY-POWTÃ“RZENIA
ZdefiniowaÅ‚eÅ› pojÄ™cie raz â€” nie definiuj ponownie.
OdwoÅ‚uj siÄ™: "wspomniany wczeÅ›niej X".
Brak powtÃ³rzeÅ„ leksykalnych w sÄ…siednich akapitach.
Brak powielania tej samej konstrukcji skÅ‚adniowej.

ANTY-MYÅšLNIKI
MyÅ›lniki (â€”) stosuj MAX 1 na 3 akapity.
âœ… UÅ¼ywaj przecinkÃ³w, dwukropkÃ³w, nawiasÃ³w, Å›rednikÃ³w.
âŒ "Wyrok â€” choÄ‡ kontrowersyjny â€” zostaÅ‚ utrzymany." (co zdanie)
Nadmiar myÅ›lnikÃ³w = sygnaÅ‚ tekstu AI.

ANTY-PYTANIA-RETORYCZNE
MAX 1 pytanie retoryczne na sekcjÄ™ H2.
âŒ "Jak to wyglÄ…da w praktyce?", "Co to oznacza?", "Czy zawsze?"
âœ… PrzejdÅº bezpoÅ›rednio do informacji.

ANTY-FILLER
KaÅ¼de zdanie MUSI dodawaÄ‡ nowÄ… informacjÄ™.
âŒ Truizmy: "Przewodnik elektryczny przewodzi prÄ…d."
âŒ Puste przejÅ›cia: "To prowadzi do kolejnego aspektu."
âŒ Zapowiedzi: "Kolejna czÄ™Å›Ä‡ artykuÅ‚u wyjaÅ›nia..."
âŒ Puste podsumowania: "To kluczowa rÃ³Å¼nica technologiczna."
âœ… "MiedÅº przewodzi prÄ…d 6Ã— lepiej niÅ¼ Å¼elazo, dlatego stanowi
   60% okablowania domowego."

ANTY-BRAND-STUFFING
Nazwy firm/marek: MAX 2Ã— w caÅ‚ym artykule.

CYTOWANIE Å¹RÃ“DEÅ (YMYL)
âœ… Ustawy, artykuÅ‚y KK/KC/KW, badania, instytucje oficjalne.
âŒ Encje jako ÅºrÃ³dÅ‚a: "Wikipedia podaje...", "WedÅ‚ug [encji]..."
Podawaj fakty bezpoÅ›rednio. Å¹rÃ³dÅ‚o z nazwy â€” MAX 1 raz na artykuÅ‚.

ANTY-HALUCYNACJA
JeÅ›li brak pewnych danych â€” pomiÅ„ lub opisz zasadÄ™ ogÃ³lnie.
âŒ WymyÅ›lone statystyki, rozporzÄ…dzenia, daty, ceny.
âœ… Zasada ogÃ³lna bez numerÃ³w ustaw gdy nie masz pewnoÅ›ci.

POLSZCZYZNA (NKJP, 1,8 mld segmentÃ³w)
â†’ PRZECINKI: obowiÄ…zkowe przed: Å¼e, ktÃ³ry/a/e, poniewaÅ¼, gdyÅ¼,
  aby, Å¼eby, jednak, lecz, ale.
  Brak przecinka przed "Å¼e" = natychmiastowy sygnaÅ‚ AI.
â†’ KOLOKACJE â€” uÅ¼ywaj poprawnych poÅ‚Ä…czeÅ„:
  podjÄ…Ä‡ decyzjÄ™ (NIE: zrobiÄ‡), odnieÅ›Ä‡ sukces (NIE: mieÄ‡),
  popeÅ‚niÄ‡ bÅ‚Ä…d (NIE: zrobiÄ‡), ponieÅ›Ä‡ konsekwencje (NIE: mieÄ‡),
  wysoki poziom (NIE: duÅ¼y), wysokie ryzyko (NIE: duÅ¼e),
  odgrywaÄ‡ rolÄ™ (NIE: peÅ‚niÄ‡), silny bÃ³l (NIE: duÅ¼y),
  rzÄ™sisty deszcz (NIE: duÅ¼y), wysunÄ…Ä‡ propozycjÄ™ (NIE: daÄ‡).
â†’ DÅUGOÅšÄ† ZDAÅƒ: Å›rednio 10â€“15 sÅ‚Ã³w (styl publicystyczny).
  NIE pisz wszystkich zdaÅ„ jednej dÅ‚ugoÅ›ci â€” to sygnaÅ‚ AI.
â†’ ÅšREDNIA DÅUGOÅšÄ† WYRAZU: 6 znakÃ³w (Â±0,5).
  Nie naduÅ¼ywaj nominalizacji.
â†’ DIAKRYTYKI: naturalny tekst ma ~7% Ä…,Ä™,Ä‡,Å‚,Å„,Ã³,Å›,Åº,Å¼.
â†’ Unikaj pleonazmÃ³w: "wzajemna wspÃ³Å‚praca",
  "aktualna sytuacja na dziÅ›", "krÃ³tkie streszczenie".
â†’ Mieszaj przypadki gramatyczne â€” nie powtarzaj frazy w mianowniku.

FORMAT
h2:/h3: dla nagÅ‚Ã³wkÃ³w. Zero markdown, HTML, gwiazdek.

</rules>""")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DYNAMIC: SUBJECT POSITION â€” per-batch entity rotation
    # Injected HERE (not in static <rules>) so encja rotates per H2
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    section_lead = pre_batch.get("_section_lead_entity", "")
    main_kw = (pre_batch.get("main_keyword") or {}).get("keyword", "") if isinstance(pre_batch.get("main_keyword"), dict) else str(pre_batch.get("main_keyword") or "")
    if not section_lead:
        section_lead = main_kw

    if section_lead:
        # Build rotation list: lead entity first, then other MUST entities from pre_batch
        must_ents_raw = pre_batch.get("_must_cover_concepts") or pre_batch.get("enhanced", {}).get("must_cover_entities") or []
        must_names = []
        for e in must_ents_raw:
            name = (e.get("text", e.get("entity", "")) if isinstance(e, dict) else str(e)).strip()
            if name and name != section_lead and name not in must_names:
                must_names.append(name)

        # Build rotation instruction
        rotation_entities = [section_lead] + must_names[:3]
        if len(rotation_entities) == 1:
            rotation_str = '"' + section_lead + '"'
        else:
            rotation_str = " | ".join(
                f"akapit {i+1}: \"{e}\"" for i, e in enumerate(rotation_entities)
            )

        fallback_ent = must_names[0] if must_names else "Sad/Sprawca"
        sp_note = "" if section_lead == main_kw else (
            f"\n  (Encja glowna \"{main_kw}\" moze sie pojawiac, ale nie jest podmiotem tej sekcji.)"
        )
        rule_body = (
            "<subject_position_rule>\n"
            f"TEMAT TEJ SEKCJI: \"{section_lead}\"\n"
            f"W tej sekcji H2 kazdy akapit musi miec INNA encje jako podmiot otwierajacy.{sp_note}\n"
            "\n"
            f"ROTACJA PODMIOTOW - kolejnosc akapitow:\n"
            f"  {rotation_str}\n"
            "\n"
            "ZASADA: kazdy kolejny akapit otwiera INNA encja z powyzszej listy jako podmiot gramatyczny.\n"
            "Jesli sekcja ma 4 akapity -> 4 rozne encje jako podmiot pierwszego zdania.\n"
            "\n"
            "Przyklad rotacji (3 akapity):\n"
            f"  Akapit 1: \"{section_lead} [orzeczenie]...\"\n"
            f"  Akapit 2: \"{fallback_ent} [orzeczenie]...\"\n"
            "  Akapit 3: Liczba/fakt na poczatku lub kolejna encja MUST\n"
            "\n"
            "ZAKAZ: dwa akapity z rzedu otwarte ta sama encja.\n"
            "ZAKAZ: 'Istotnym aspektem jest [encja]...' - to orzecznik, nie podmiot.\n"
            "ZAKAZ: 'Zgodnie z przepisami o [encja]...' - to dopelnienie, nie podmiot.\n"
            "\n"
            "Google salience: podmiot x pozycja = 3-6x wyzszy wynik niz encja w dopelnieniu.\n"
            "Rotacja podmiotow = naturalne pokrycie wszystkich kluczowych encji tematu.\n"
            "</subject_position_rule>"
        )
        parts.append(rule_body)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Fix #57: SEMANTIC KEYPHRASES â€” natural compound phrases
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    sem_kp = pre_batch.get("_semantic_keyphrases") or []
    if sem_kp:
        kp_lines = []
        for kp in sem_kp[:8]:
            phrase = kp.get("phrase", kp) if isinstance(kp, dict) else str(kp)
            if phrase:
                kp_lines.append(f"  â€¢ {phrase}")
        if kp_lines:
            parts.append(
                "<semantic_keyphrases>\n"
                "FRAZY SEMANTYCZNE â€” uÅ¼yj minimum 3 z poniÅ¼szych jako KOMPLETNE FRAZY (nie rozbijaj na osobne sÅ‚owa):\n"
                + "\n".join(kp_lines) + "\n"
                "KaÅ¼da fraza powinna pojawiÄ‡ siÄ™ jako spÃ³jny ciÄ…g sÅ‚Ã³w w jednym zdaniu.\n"
                "PrzykÅ‚ad: zamiast 'diagnostyka sÅ‚uchu. Dziecka dotyczy...' â†’ 'diagnostyka sÅ‚uchu dziecka obejmuje...'\n"
                "</semantic_keyphrases>"
            )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FEW-SHOT EXAMPLES
    # (Anthropic/OpenAI: przykÅ‚ady skuteczniejsze niÅ¼ instrukcje)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    parts.append("""<examples>

PRZYKÅAD ZÅY â€” czego NIE pisaÄ‡:
<example_bad>
Jazda po alkoholu to powaÅ¼ne przestÄ™pstwo w Polsce. SÄ…d patrzy. I sÅ‚ucha.
Granice sÄ… sztywne. Kancelaria posiada duÅ¼e doÅ›wiadczenie w sprawach
karnych ruchu drogowego. Kancelaria posiada duÅ¼e doÅ›wiadczenie w sprawach
karnych ruchu drogowego. Ta instytucja daje sÄ…dowi moÅ¼liwoÅ›Ä‡ odstÄ…pienia
od wymierzenia Å›rodka, co warto zauwaÅ¼yÄ‡ i naleÅ¼y podkreÅ›liÄ‡.
</example_bad>
BÅ‚Ä™dy: dramatyzatory ("SÄ…d patrzy. I sÅ‚ucha."), powtÃ³rzenie zdania 2Ã—,
frazy AI ("warto zauwaÅ¼yÄ‡"), brak liczb, puste stwierdzenia.

PRZYKÅAD DOBRY â€” tak pisz:
<example_good>
Skazanie z art. 178a Â§ 1 KK grozi pozbawieniem wolnoÅ›ci do 3 lat
oraz obligatoryjnym zakazem prowadzenia pojazdÃ³w od 3 do 15 lat.
SÄ…d nie ma tu uznaniowoÅ›ci â€” zakaz jest obowiÄ…zkowy przy kaÅ¼dym
wyroku skazujÄ…cym, niezaleÅ¼nie od okolicznoÅ›ci Å‚agodzÄ…cych.
JedynÄ… zmiennÄ… pozostaje jego wymiar, ktÃ³ry sÄ…d ustala biorÄ…c pod
uwagÄ™ stopieÅ„ zawinienia i dotychczasowÄ… karalnoÅ›Ä‡ sprawcy.
</example_good>
Zalety: konkretny artykuÅ‚ KK, konkretne liczby (3 lata, 3â€“15 lat),
kauzalnoÅ›Ä‡ (obligatoryjny â†’ brak uznaniowoÅ›ci â†’ jedyna zmienna),
zero fraz AI, zero powtÃ³rzeÅ„.

</examples>""")

    return "\n\n".join(parts)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Schema guard â€” field validation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_CRITICAL_FIELDS = [
    "keywords",             # keyword list: without this, article has no SEO
    "main_keyword",         # primary keyword
    "batch_number",         # batch sequencing
]
_IMPORTANT_FIELDS = [
    "gpt_instructions_v39", # backend writing instructions
    "enhanced",             # enhanced_pre_batch AI data
    "h2_remaining",         # H2 structure
    "article_memory",       # context from previous batches
    "keyword_limits",       # STOP/EXCEEDED rules
    "coverage",             # keyword coverage state
]

def _schema_guard(pre_batch):
    """Validate pre_batch has critical fields. Log warnings for missing."""
    missing_critical = [f for f in _CRITICAL_FIELDS if f not in pre_batch or pre_batch[f] is None]
    missing_important = [f for f in _IMPORTANT_FIELDS if f not in pre_batch or pre_batch[f] is None]

    if missing_critical:
        _pb_logger.warning(
            f"âš ï¸ SCHEMA GUARD: Missing CRITICAL fields: {missing_critical}. "
            f"Backend may have changed API. Article quality will be degraded."
        )
    if missing_important:
        _pb_logger.info(
            f"â„¹ï¸ Schema guard: Missing optional fields: {missing_important} "
            f"(batch {pre_batch.get('batch_number', '?')})"
        )

    # Validate enhanced sub-fields if enhanced exists
    enhanced = pre_batch.get("enhanced") or {}
    if enhanced:
        expected_enhanced = [
            "smart_instructions_formatted", "causal_context",
            "information_gain", "relations_to_establish"
        ]
        missing_enh = [f for f in expected_enhanced if not enhanced.get(f)]
        if missing_enh:
            _pb_logger.info(f"â„¹ï¸ Enhanced missing: {missing_enh}")


def build_user_prompt(pre_batch, h2, batch_type, article_memory=None):
    """
    Main user prompt builder.
    Converts ALL pre_batch fields into readable, actionable instructions.
    Each section is wrapped in try/except so one bad field won't crash generation.
    """
    pre_batch = pre_batch or {}
    sections = []

    # â”€â”€ RE-ANCHOR: krÃ³tkie przypomnienie roli (dokumentacja Anthropic: re-anchor w user prompcie) â”€â”€
    # Dla YMYL dodaje ostrzeÅ¼enie o weryfikacji wyrokÃ³w
    detected_category = pre_batch.get("detected_category", "")
    if detected_category == "prawo":
        sections.append(
            "Piszesz jako redaktor naczelny â€” ton formalny, zero frywolnoÅ›ci. "
            "Wyroki cytuj TYLKO jeÅ›li sygnatura pasuje do gaÅ‚Ä™zi prawa artykuÅ‚u "
            "(II K/AKa = karne, I C/ACa = cywilne). SzczegÃ³Å‚owe zasady w system prompcie."
        )
    elif detected_category in ("medycyna", "finanse"):
        sections.append(
            "Piszesz jako redaktor naczelny â€” ton formalny, precyzyjny. "
            "Cytuj TYLKO pewne dane. SzczegÃ³Å‚owe zasady w system prompcie."
        )
    else:
        sections.append(
            "Piszesz jako redaktor naczelny â€” rzeczowo, bez frywolnoÅ›ci. "
            "SzczegÃ³Å‚owe zasady w system prompcie."
        )

    # â”€â”€ OPENING PATTERN â€” per-batch rotation (zapobiega identycznym otwarciom sekcji) â”€â”€
    _OPENING_PATTERNS = [
        ("A", "LICZBA/FAKT",
         "Zacznij sekcje od konkretnej liczby, daty lub wartosci. Np: '3 lata - tyle wynosi...', 'Od 2500 do 30 000 zl...'"),
        ("B", "WARUNEK",
         "Zacznij sekcje od warunku lub progu. Np: 'Gdy stezenie przekracza...', 'Jesli kierowca...', 'Przy kazdym kolejnym...'"),
        ("C", "SKUTEK WPROST",
         "Zacznij sekcje od konsekwencji. Np: 'Konfiskata grozi kazdemu...', 'Zakaz trwa od 3 do 15 lat - sad nie moze...'"),
        ("D", "KONTRAST",
         "Zacznij sekcje od rozroznienia. Np: 'Wykroczenie i przestepstwo - granica przebiega...', 'Recydywista odpowiada inaczej...'"),
        ("E", "PODMIOT+ORZECZENIE",
         "Zacznij sekcje klasycznie: podmiot + orzeczenie z konkretem. Np: 'Stan po uzyciu alkoholu to poziom 0,2-0,5 promila...'"),
        ("F", "PYTANIE+ODPOWIEDZ",
         "Zacznij sekcje pytaniem z natychmiastowa odpowiedzia. Np: 'Czy mozna unikac zakazu? Tak, ale tylko gdy...'"),
    ]
    batch_num = pre_batch.get("batch_number", 1) or 1
    if batch_type in ("INTRO", "intro"):
        pattern_idx = 0  # INTRO: zawsze liczba/fakt dla silnego otwarcia
    else:
        pattern_idx = (batch_num - 1) % len(_OPENING_PATTERNS)
    p_letter, p_name, p_desc = _OPENING_PATTERNS[pattern_idx]
    sections.append(
        f"OTWARCIE TEJ SEKCJI â€” wzorzec {p_letter} ({p_name}):\n"
        f"{p_desc}\n"
        f"ZAKAZ: nie zaczynaj od encji jako podmiotu w stylu '[X] jest/to/oznacza' â€” to wzorzec juÅ¼ uÅ¼yty w poprzednich sekcjach."
    )

    # â”€â”€ SCHEMA GUARD: validate critical fields from backend â”€â”€
    _schema_guard(pre_batch)

    formatters = [
        # â”€â”€ TIER 1: NON-NEGOTIABLE (backend hard rules) â”€â”€
        lambda: _fmt_batch_header(pre_batch, h2, batch_type),
        lambda: _fmt_keywords(pre_batch),           # MUST/STOP/EXCEEDED: hardest constraints
        lambda: _fmt_smart_instructions(pre_batch),  # enhanced_pre_batch AI instructions
        lambda: _fmt_legal_medical(pre_batch),        # YMYL: legal compliance, non-negotiable

        # â”€â”€ TIER 2: BACKEND WRITE INSTRUCTIONS (gpt_instructions_v39 etc.) â”€â”€
        lambda: _fmt_semantic_plan(pre_batch, h2),
        lambda: _fmt_coverage_density(pre_batch),
        lambda: _fmt_phrase_hierarchy(pre_batch),
        lambda: _fmt_continuation(pre_batch),
        lambda: _fmt_article_memory(article_memory),
        lambda: _fmt_h2_remaining(pre_batch),

        # â”€â”€ TIER 3: CONTENT CONTEXT (enrichment data) â”€â”€
        lambda: _fmt_entity_salience(pre_batch),     # entity positioning rules (salience only)
        # _fmt_entities REMOVED v45.4.1: gpt_instructions_v39 already contains
        # curated "ğŸ§  ENCJE:" section (max 3/batch, importanceâ‰¥0.7, with HOW hints).
        # Our version duplicated it with dirtier, unfiltered data from S1.
        # _fmt_ngrams REMOVED v45.4.1: raw statistical n-grams from competitor
        # pages often contain CSS/JS artifacts ("button button", "block embed").
        # Custom GPT never sees these and produces better text without them.
        lambda: _fmt_serp_enrichment(pre_batch),
        lambda: _fmt_causal_context(pre_batch),
        lambda: _fmt_depth_signals(pre_batch),       # depth signals when previous batch scored low
        lambda: _fmt_experience_markers(pre_batch),
        lambda: _fmt_natural_polish(pre_batch),      # v50: fleksja, spacing, anti-stuffing

        # â”€â”€ TIER 4: SOFT GUIDELINES (format, style, intro) â”€â”€
        lambda: _fmt_intro_guidance(pre_batch, batch_type),
        lambda: _fmt_style(pre_batch),
        lambda: _fmt_output_format(h2, batch_type),
    ]

    for fmt in formatters:
        try:
            result = fmt()
            if result:
                sections.append(result)
        except Exception:
            pass

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECTION FORMATTERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _fmt_batch_header(pre_batch, h2, batch_type):
    batch_number = pre_batch.get("batch_number", 1)
    total_batches = pre_batch.get("total_planned_batches", 1)
    batch_length = pre_batch.get("batch_length") or {}

    min_w = batch_length.get("min_words", 350)
    max_w = batch_length.get("max_words", 500)

    section_length = pre_batch.get("section_length_guidance") or {}
    length_hint = ""
    if section_length:
        suggested = section_length.get("suggested_words") or section_length.get("target_words")
        if suggested:
            length_hint = f"\nSugerowana dÅ‚ugoÅ›Ä‡ tej sekcji: ~{suggested} sÅ‚Ã³w."

    h2_instruction = ""
    if batch_type not in ("INTRO", "intro"):
        h2_instruction = f"\nZaczynaj DOKÅADNIE od: h2: {h2}"

    return f"""â•â•â• BATCH {batch_number}/{total_batches}: {batch_type} â•â•â•
Sekcja H2: "{h2}"
DÅ‚ugoÅ›Ä‡: {min_w}-{max_w} sÅ‚Ã³w{length_hint}{h2_instruction}"""


def _fmt_intro_guidance(pre_batch, batch_type):
    if batch_type not in ("INTRO", "intro"):
        return ""
    guidance = pre_batch.get("intro_guidance", "")

    main_kw = pre_batch.get("main_keyword") or {}
    kw_name = main_kw.get("keyword", "") if isinstance(main_kw, dict) else str(main_kw)

    parts = ["â•â•â• WPROWADZENIE (WSTÄ˜P ARTYKUÅU) â•â•â•",
             "To jest PIERWSZY batch, piszesz WSTÄ˜P artykuÅ‚u.",
             "MUSISZ:",
             f'  1. WpleÄ‡ frazÄ™ gÅ‚Ã³wnÄ… ("{kw_name}") w PIERWSZE zdanie' if kw_name else "  1. FrazÄ™ gÅ‚Ã³wnÄ… umieÅ›Ä‡ w pierwszym zdaniu",
             "  2. ZaczÄ…Ä‡ od angaÅ¼ujÄ…cego haka (hook): pytanie, statystyka, scenariusz",
             "  3. PrzedstawiÄ‡ GÅÃ“WNÄ„ TEZÄ˜ artykuÅ‚u w 1-2 zdaniach",
             "  4. ZapowiedzieÄ‡ co czytelnik znajdzie dalej (bez listy H2!)",
             "  5. NIE zaczynaÄ‡ od definicji ani od 'W dzisiejszych czasach...'",
             "  6. NIE dodawaÄ‡ nagÅ‚Ã³wka h2: (wstÄ™p nie ma nagÅ‚Ã³wka",
             "  7. UtrzymaÄ‡ zwiÄ™zÅ‚oÅ›Ä‡; wstÄ™p to 80-150 sÅ‚Ã³w"]

    if guidance:
        if isinstance(guidance, dict):
            hook = guidance.get("hook", "")
            angle = guidance.get("angle", "")
            if hook:
                parts.append(f"\nHak otwierajÄ…cy: {hook}")
            if angle:
                parts.append(f"KÄ…t artykuÅ‚u: {angle}")
        else:
            parts.append(f"\n{guidance}")

    # AI Overview â€” tylko we wstÄ™pie, Å¼eby intro odpowiadaÅ‚o na to co Google juÅ¼ pokazuje
    serp = pre_batch.get("serp_enrichment") or {}
    ai_ov = serp.get("ai_overview") or {}
    if isinstance(ai_ov, dict):
        ai_ov_text = ai_ov.get("text", "") or ""
    elif isinstance(ai_ov, str):
        ai_ov_text = ai_ov
    else:
        ai_ov_text = ""
    if ai_ov_text and len(ai_ov_text) > 50:
        parts.append("\nâ•â•â• GOOGLE AI OVERVIEW â•â•â•")
        parts.append("Google wyÅ›wietla uÅ¼ytkownikom ten tekst ZANIM kliknÄ… w artykuÅ‚.")
        parts.append("WstÄ™p MUSI nawiÄ…zywaÄ‡ do tego kontekstu i obiecywaÄ‡ gÅ‚Ä™bszÄ… odpowiedÅº:")
        parts.append(f"  {ai_ov_text[:500]}")

    return "\n".join(parts)


def _fmt_smart_instructions(pre_batch):
    """Smart instructions from enhanced_pre_batch : THE most valuable field."""
    enhanced = pre_batch.get("enhanced") or {}
    smart = enhanced.get("smart_instructions_formatted", "")
    if smart:
        return f"â•â•â• INSTRUKCJE DLA TEGO BATCHA â•â•â•\n{smart[:1000]}"
    return ""


def _parse_target_max(target_total_str):
    """
    Parse target_max from backend's target_total field.
    Backend sends target_total as "min-max" string (e.g., "2-6").
    Returns max value as int, or 0 if unparseable.
    """
    if not target_total_str:
        return 0
    if isinstance(target_total_str, (int, float)):
        return int(target_total_str)
    try:
        parts = str(target_total_str).replace("x", "").split("-")
        if len(parts) >= 2:
            return int(parts[-1].strip())
        return int(parts[0].strip())
    except (ValueError, IndexError):
        return 0


def _fmt_keywords(pre_batch):
    """
    Format keywords section with CALCULATED remaining_max.
    
    v1.1: Backend sends actual (current uses) and target_total ("min-max")
    but NOT remaining. We calculate: remaining = target_max - actual.
    Also shows hard_max_this_batch so Claude knows per-batch limits.
    """
    keywords_info = pre_batch.get("keywords") or {}
    keyword_limits = pre_batch.get("keyword_limits") or {}
    soft_caps = pre_batch.get("soft_cap_recommendations") or {}

    # â”€â”€ MUST USE (with calculated remaining) â”€â”€
    must_raw = keywords_info.get("basic_must_use", [])
    must_lines = []
    for kw in must_raw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            
            # Calculate remaining from actual + target_total
            actual = kw.get("actual", kw.get("actual_uses", kw.get("current_count", 0)))
            target_total = kw.get("target_total", "")
            target_max = _parse_target_max(target_total) or kw.get("target_max", 0)
            hard_max = kw.get("hard_max_this_batch", "")
            use_range = kw.get("use_this_batch", "")
            
            # Explicit remaining from backend (if sent), otherwise calculate
            remaining = kw.get("remaining", kw.get("remaining_max", ""))
            if not remaining and target_max and isinstance(actual, (int, float)):
                remaining = max(0, target_max - int(actual))
            
            # Build descriptive line
            parts_line = [f'"{name}"']
            if remaining:
                parts_line.append(f"zostaÅ‚o {remaining}Ã— ogÃ³Å‚em")
            if hard_max:
                parts_line.append(f"max {hard_max}Ã— w tym batchu")
            elif use_range:
                parts_line.append(f"cel: {use_range}Ã— w tym batchu")
            
            must_lines.append(f'  â€¢ {", ".join(parts_line)}')
        else:
            must_lines.append(f'  â€¢ "{kw}"')

    # â”€â”€ EXTENDED (with remaining) â”€â”€
    ext_raw = keywords_info.get("extended_this_batch", [])
    ext_lines = []
    for kw in ext_raw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            actual = kw.get("actual", kw.get("actual_uses", 0))
            target_total = kw.get("target_total", "")
            target_max = _parse_target_max(target_total) or kw.get("target_max", 0)
            remaining = kw.get("remaining", kw.get("remaining_max", ""))
            if not remaining and target_max and isinstance(actual, (int, float)):
                remaining = max(0, target_max - int(actual))
            
            line = f'  â€¢ "{name}"'
            if remaining:
                line += f" , zostaÅ‚o {remaining}Ã—"
            ext_lines.append(line)
        else:
            ext_lines.append(f'  â€¢ "{kw}"')

    # â”€â”€ STOP â”€â”€
    stop_raw = keyword_limits.get("stop_keywords") or []
    stop_lines = []
    for s in stop_raw:
        if isinstance(s, dict):
            name = s.get("keyword", "")
            current = s.get("current_count", s.get("current", s.get("actual", "?")))
            max_c = s.get("max_count", s.get("max", s.get("target_max", "?")))
            stop_lines.append(f'  â€¢ "{name}" (juÅ¼ {current}Ã—, limit {max_c}) , STOP!')
        else:
            stop_lines.append(f'  â€¢ "{s}"')

    # â”€â”€ CAUTION â”€â”€
    caution_raw = keyword_limits.get("caution_keywords") or []
    caution_lines = []
    for c in caution_raw:
        if isinstance(c, dict):
            name = c.get("keyword", "")
            current = c.get("current_count", c.get("current", c.get("actual", "")))
            max_c = c.get("max_count", c.get("max", c.get("target_max", "")))
            line = f'  â€¢ "{name}"'
            if current and max_c:
                line += f" ({current}/{max_c})"
            line += " , max 1Ã— w tym batchu"
            caution_lines.append(line)
        else:
            caution_lines.append(f'  â€¢ "{c}" , max 1Ã—')

    # â”€â”€ SOFT CAPS â”€â”€
    soft_notes = []
    if soft_caps:
        for kw_name, info in soft_caps.items():
            if isinstance(info, dict):
                action = info.get("action", "")
                if action and action != "OK":
                    soft_notes.append(f'  â„¹ï¸ "{kw_name}": {action}')

    # â”€â”€ Build section â”€â”€
    parts = ["â•â•â• FRAZY KLUCZOWE â•â•â•"]

    if must_lines:
        parts.append("ğŸ”´ OBOWIÄ„ZKOWE (wpleÄ‡ naturalnie w tekst):")
        parts.extend(must_lines)

    if ext_lines:
        parts.append("\nğŸŸ¡ ROZSZERZONE (uÅ¼yj jeÅ›li pasujÄ… do kontekstu):")
        parts.extend(ext_lines)

    if stop_lines:
        parts.append("\nğŸ›‘ STOP, NIE UÅ»YWAJ (przekroczone limity!):")
        parts.extend(stop_lines)

    if caution_lines:
        parts.append("\nâš ï¸ OSTROÅ»NIE, uÅ¼yj max 1Ã— lub pomiÅ„:")
        parts.extend(caution_lines)

    if soft_notes:
        parts.append("")
        parts.extend(soft_notes)

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_semantic_plan(pre_batch, h2):
    plan = pre_batch.get("semantic_batch_plan") or {}
    if not plan:
        return ""

    parts = ["â•â•â• CO PISAÄ† W TEJ SEKCJI â•â•â•"]

    h2_coverage = plan.get("h2_coverage") or {}
    for h2_name, info in h2_coverage.items():
        if isinstance(info, dict):
            angle = info.get("semantic_angle", "")
            must = info.get("must_phrases", [])
            if angle:
                parts.append(f'KÄ…t semantyczny: {angle}')
            if must:
                phrases = ", ".join(f'"{p}"' for p in must[:5])
                parts.append(f'ObowiÄ…zkowe frazy w tej sekcji: {phrases}')

    density_targets = plan.get("density_targets") or {}
    overall = density_targets.get("overall")
    if overall:
        parts.append(f'Docelowa gÄ™stoÅ›Ä‡ fraz: {overall}%')

    direction = plan.get("content_direction") or plan.get("writing_direction", "")
    if direction:
        parts.append(f'Kierunek treÅ›ci: {direction}')

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_entity_salience(pre_batch):
    """Entity salience instructions : grammatical positioning, hierarchy.
    
    Based on:
    - Patent US10235423B2 (entity metrics)
    - Patent US9251473B2 (salient items in documents)
    - Dunietz & Gillick (2014) entity salience research
    - Google Cloud NLP API salience scoring
    
    v47.0: Also includes backend placement instructions from competitor analysis
    (entity_salience.py in gpt-ngram-api: salience scoring, co-occurrence, placement)
    
    Data sources:
    - pre_batch["_entity_salience_instructions"] : local positioning rules (from entity_salience.py frontend)
    - pre_batch["_backend_placement_instruction"] : backend placement from competitor analysis
    - pre_batch["_concept_instruction"] : topical concepts agent instruction
    - pre_batch["_must_cover_concepts"] : concept entities that must be covered
    """
    parts = []
    
    # 1. Local salience positioning rules
    local_instructions = pre_batch.get("_entity_salience_instructions", "")
    if local_instructions:
        parts.append(local_instructions)
    
    # 2. v47.0: Backend placement instructions (from gpt-ngram-api competitor analysis)
    backend_placement = pre_batch.get("_backend_placement_instruction", "")
    if backend_placement:
        parts.append("â•â•â• ROZMIESZCZENIE ENCJI (z analizy konkurencji) â•â•â•")
        parts.append(
            "âš ï¸ TO SÄ„ WSKAZÃ“WKI TECHNICZNE â€” NIE kopiuj ich dosÅ‚ownie do tekstu!\n"
            "UÅ¼yj jako inspiracjÄ™/tÅ‚o. Pisz wÅ‚asne zdania. Nie przepisuj fragmentÃ³w poniÅ¼ej."
        )
        parts.append(backend_placement)
    
    # 3. v47.0: Concept instruction + must-cover concepts
    # v52.1: Dodano instrukcjÄ™ fleksji â€” encje podawane sÄ… w mianowniku, Claude musi je odmieniaÄ‡
    FLEXION_NOTE = (
        "\nâš ï¸ FLEKSJA: PojÄ™cia sÄ… w mianowniku â€” odmieniaj je przez przypadki zaleÅ¼nie od kontekstu. "
        'Np. "gaÅ‚ka meblowa" â†’ "gaÅ‚ki meblowej" (dop.), "gaÅ‚kÄ™ meblowÄ…" (bier.). '
        "Gramatyczna poprawnoÅ›Ä‡ > dosÅ‚owne powtÃ³rzenie formy bazowej."
    )
    concept_instr = pre_batch.get("_concept_instruction", "")
    must_concepts = pre_batch.get("_must_cover_concepts", [])
    if concept_instr:
        parts.append(concept_instr + FLEXION_NOTE)
    elif must_concepts:
        # Build instruction from concept list if no agent instruction provided
        concept_names = [c.get("text", c) if isinstance(c, dict) else str(c) for c in must_concepts[:10]]
        parts.append(
            "â•â•â• POJÄ˜CIA TEMATYCZNE (z analizy konkurencji) â•â•â•\n"
            f"NastÄ™pujÄ…ce pojÄ™cia pojawiajÄ… siÄ™ u konkurencji, wpleÄ‡ naturalnie w tekst:\n"
            f"{', '.join(concept_names)}"
            + FLEXION_NOTE
        )
    
    # 4. v50: Co-occurrence pairs: encje ktÃ³re MUSZÄ„ byÄ‡ blisko siebie
    cooc_pairs = pre_batch.get("_cooccurrence_pairs") or []
    if cooc_pairs:
        cooc_lines = []
        for pair in cooc_pairs[:8]:
            if isinstance(pair, dict):
                e1 = pair.get("entity1", pair.get("source", ""))
                e2 = pair.get("entity2", pair.get("target", ""))
                if e1 and e2:
                    cooc_lines.append(f'  â€¢ "{e1}" + "{e2}"  (w tym samym akapicie)')
            elif isinstance(pair, str) and "+" in pair:
                cooc_lines.append(f"  â€¢ {pair}  (w tym samym akapicie)")
        if cooc_lines:
            parts.append(
                "â•â•â• WSPÃ“ÅWYSTÄ˜POWANIE ENCJI (co-occurrence) â•â•â•\n"
                "NastÄ™pujÄ…ce pary encji czÄ™sto pojawiajÄ… siÄ™ RAZEM u konkurencji.\n"
                "UmieÅ›Ä‡ je W TYM SAMYM AKAPICIE , bliskoÅ›Ä‡ buduje kontekst semantyczny:\n"
                + "\n".join(cooc_lines)
            )
    
    # 5. v50: First paragraph entities: encje z pierwszego akapitu top10
    first_para_ents = pre_batch.get("_first_paragraph_entities") or []
    if first_para_ents:
        fp_names = []
        for ent in first_para_ents[:6]:
            name = ent.get("entity", ent.get("text", ent)) if isinstance(ent, dict) else str(ent)
            if name:
                fp_names.append(f'"{name}"')
        if fp_names:
            parts.append(
                "PIERWSZY AKAPIT, encje tematyczne:\n"
                f"WprowadÅº w pierwszym akapicie: {', '.join(fp_names)}.\n"
                "âš ï¸ To POJÄ˜CIA do opisania, NIE ÅºrÃ³dÅ‚a do cytowania. Nie pisz '[encja] podaje/potwierdza...'."
            )
    
    # 6. v50: H2 entities: encje tematyczne do rozmieszczenia w H2
    h2_ents = pre_batch.get("_h2_entities") or []
    if h2_ents:
        h2_names = []
        for ent in h2_ents[:8]:
            name = ent.get("entity", ent.get("text", ent)) if isinstance(ent, dict) else str(ent)
            if name:
                h2_names.append(f'"{name}"')
        if h2_names:
            parts.append(
                "ENCJE TEMATYCZNE W H2:\n"
                f"RozÅ‚Ã³Å¼ w tekÅ›cie: {', '.join(h2_names)}.\n"
                "âš ï¸ To POJÄ˜CIA do opisania, NIE ÅºrÃ³dÅ‚a. Nie pisz '[encja] podaje...'."
            )

    # 7. EAV triples: encja â†’ atrybut â†’ wartoÅ›Ä‡
    # MÃ³wiÄ… modelowi CO NAPISAÄ† o kaÅ¼dej encji â€” konkretny fakt, nie tylko nazwa
    eav_triples = pre_batch.get("_eav_triples") or []
    if eav_triples:
        eav_lines = ["â•â•â• CECHY ENCJI â€” Entity Attribute Value (NAPISZ TE FAKTY) â•â•â•",
                     "Dla kaÅ¼dej poniÅ¼szej encji MUSISZ wyraziÄ‡ podany fakt w tekÅ›cie.",
                     "Nie kopiuj dosÅ‚ownie â€” zbuduj naturalne zdanie zawierajÄ…ce tÄ™ relacjÄ™.",
                     ""]
        primary_eav = [e for e in eav_triples if e.get("is_primary")]
        secondary_eav = [e for e in eav_triples if not e.get("is_primary")]
        if primary_eav:
            e = primary_eav[0]
            eav_lines.append(f'ğŸ¯ GÅÃ“WNA: "{e["entity"]}" â†’ {e["attribute"]} â†’ {e["value"]}')
        for e in secondary_eav[:10]:
            eav_lines.append(f'   â€¢ "{e["entity"]}" ({e.get("type","")}) â†’ {e["attribute"]} â†’ {e["value"]}')
        eav_lines.append("")
        eav_lines.append("âœ… PrzykÅ‚ad zamiany EAV na zdanie:")
        eav_lines.append('   EAV: "kodeks karny â†’ penalizuje â†’ jazdÄ™ po alkoholu art. 178a"')
        eav_lines.append('   ZDANIE: "Art. 178a Kodeksu karnego penalizuje prowadzenie pojazdu w stanie"')
        eav_lines.append('          "nietrzeÅºwoÅ›ci â€” przewiduje karÄ™ do 3 lat pozbawienia wolnoÅ›ci."')
        parts.append("\n".join(eav_lines))

    # 8. SVO triples: podmiot â†’ relacja â†’ obiekt  
    # Gotowe fakty do wbudowania w tekst â€” rdzeÅ„ knowledge graph artykuÅ‚u
    svo_triples = pre_batch.get("_svo_triples") or []
    if svo_triples:
        svo_lines = ["â•â•â• TRÃ“JKI SEMANTYCZNE SVO â€” fakty OBOWIÄ„ZKOWE w artykule â•â•â•",
                     "KaÅ¼da trÃ³jka to fakt ktÃ³ry MUSI znaleÅºÄ‡ siÄ™ gdzieÅ› w artykule.",
                     "MoÅ¼esz rozÅ‚oÅ¼yÄ‡ je na rÃ³Å¼ne sekcje â€” waÅ¼ne Å¼eby byÅ‚y obecne.",
                     ""]
        for i, t in enumerate(svo_triples[:12], 1):
            ctx = f' [{t["context"]}]' if t.get("context") else ""
            svo_lines.append(f'  {i}. {t["subject"]} â†’ {t["verb"]} â†’ {t["object"]}{ctx}')
        svo_lines.append("")
        svo_lines.append("Google Knowledge Graph indeksuje te relacje. Im wiÄ™cej z nich pojawi")
        svo_lines.append("siÄ™ jako wyraÅºne zdania (nie wtrÄ…cenia), tym wyÅ¼szy topic authority.")
        parts.append("\n".join(svo_lines))

    return "\n\n".join(parts) if parts else ""


# _fmt_entities REMOVED v45.4.1 â†’ v50 cleanup: function deleted.
# gpt_instructions_v39 already contains curated "ğŸ§  ENCJE:" section
# (max 3/batch, importanceâ‰¥0.7, with HOW hints). Our version duplicated it
# with dirtier, unfiltered data from S1.

# _fmt_ngrams REMOVED v45.4.1 â†’ v50 cleanup: function deleted.
# Raw statistical n-grams from competitor pages often contain CSS/JS artifacts
# ("button button", "block embed"). Custom GPT produces better text without them.


def _fmt_serp_enrichment(pre_batch):
    serp = pre_batch.get("serp_enrichment") or {}
    enhanced = pre_batch.get("enhanced") or {}

    paa = (serp.get("paa_for_batch") or enhanced.get("paa_from_serp") or [])
    lsi = (serp.get("lsi_keywords") or [])

    if not paa and not lsi:
        return ""

    parts = ["â•â•â• WZBOGACENIE Z SERP â•â•â•"]

    if paa:
        parts.append("Pytania ktÃ³re ludzie zadajÄ… w Google (PAA), odpowiedz na 1-2 w tekÅ›cie:")
        for q in paa[:5]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                parts.append(f'  â“ {q_text}')

    if lsi:
        lsi_names = [l.get("keyword", l) if isinstance(l, dict) else l for l in lsi[:8]]
        parts.append(f'\nFrazy LSI (bliskoznaczne, wpleÄ‡ naturalnie): {", ".join(lsi_names)}')

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_continuation(pre_batch):
    continuation = pre_batch.get("continuation_v39") or {}
    enhanced = pre_batch.get("enhanced") or {}
    cont_ctx = enhanced.get("continuation_context") or {}

    last_h2 = cont_ctx.get("last_h2") or continuation.get("last_h2", "")
    last_ending = cont_ctx.get("last_paragraph_ending") or continuation.get("last_paragraph_ending", "")
    last_topic = cont_ctx.get("last_topic") or continuation.get("last_topic", "")
    transition_hint = continuation.get("transition_hint", "")

    if not last_h2 and not last_ending:
        return ""

    parts = ["â•â•â• KONTYNUACJA â•â•â•",
             "Poprzedni batch zakoÅ„czyÅ‚ siÄ™ na:"]

    if last_h2:
        parts.append(f'  Ostatni H2: "{last_h2}"')
    if last_ending:
        ending_preview = last_ending[:150] + ("..." if len(last_ending) > 150 else "")
        parts.append(f'  Ostatnie zdanie: "{ending_preview}"')
    if last_topic:
        parts.append(f'  Temat: {last_topic}')

    parts.append("\nZacznij PÅYNNIE: nawiÄ…Å¼ do poprzedniego wÄ…tku, ale nie powtarzaj zakoÅ„czenia.")
    if transition_hint:
        parts.append(f'Sugerowane przejÅ›cie: {transition_hint}')

    return "\n".join(parts)


def _fmt_article_memory(article_memory):
    if not article_memory:
        return ""

    parts = ["â•â•â• PAMIÄ˜Ä† ARTYKUÅU (KRYTYCZNE, nie powtarzaj!) â•â•â•"]

    if isinstance(article_memory, dict):
        topics = article_memory.get("topics_covered") or article_memory.get("covered_topics") or []
        if topics:
            parts.append("Sekcje juÅ¼ napisane:")
            for t in topics[:10]:
                if isinstance(t, str):
                    parts.append(f'  âœ“ {t}')
                elif isinstance(t, dict):
                    parts.append(f'  âœ“ {t.get("topic", t.get("h2", ""))}')

        facts = article_memory.get("key_facts_used") or article_memory.get("facts", [])
        # v50.5 FIX 30: Also extract key_points and avoid_repetition from AI memory
        key_points = article_memory.get("key_points") or []
        avoid_rep = article_memory.get("avoid_repetition") or []
        
        all_facts = list(facts) + list(key_points)
        if all_facts:
            parts.append("\nFakty/definicje juÅ¼ podane (NIE POWTARZAJ, odwoÅ‚uj siÄ™: 'wspomniany wczeÅ›niej'):")
            for f in all_facts[:12]:
                parts.append(f'  â€¢ {f}' if isinstance(f, str) else f'  â€¢ {json.dumps(f, ensure_ascii=False)[:100]}')

        if avoid_rep:
            parts.append("\nâ›” TE ZDANIA I FRAZY BYÅY JUÅ» UÅ»YTE â€” NIE POWTARZAJ ICH DOSÅOWNIE:")
            parts.append("   (moÅ¼esz uÅ¼yÄ‡ tego samego SENSU, ale innymi sÅ‚owami)")
            for r in avoid_rep[:8]:
                parts.append(f'  âŒ ZAKAZ: "{r}"')

        phrases_used = article_memory.get("phrases_used") or {}
        if phrases_used:
            high_use = [(k, v) for k, v in phrases_used.items()
                        if isinstance(v, (int, float)) and v >= 3]
            if high_use:
                parts.append("\nFrazy juÅ¼ czÄ™sto uÅ¼yte (ogranicz):")
                for name, count in high_use[:8]:
                    parts.append(f'  â€¢ "{name}" (juÅ¼ {count}Ã—)')
        
        # v50.5 FIX 30: Add strong anti-repetition instruction
        if topics and len(topics) >= 2:
            parts.append(
                "\nâš ï¸ ZASADA ANTY-POWTÃ“RZEÅƒ: JeÅ›li pojÄ™cie (np. prawo Ohma, definicja ampera) "
                "zostaÅ‚o ZDEFINIOWANE w poprzedniej sekcji, NIE definiuj go ponownie. "
                "Zamiast tego: uÅ¼yj go w nowym kontekÅ›cie lub odnieÅ› siÄ™ krÃ³tko: "
                "'zgodnie z omÃ³wionym wczeÅ›niej prawem Ohma'. "
                "PowtÃ³rzenie definicji = utrata punktÃ³w jakoÅ›ci."
            )
    elif isinstance(article_memory, str):
        parts.append(_word_trim(article_memory, 1500))

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_coverage_density(pre_batch):
    coverage = pre_batch.get("coverage") or {}
    density = pre_batch.get("density") or {}
    main_kw = pre_batch.get("main_keyword") or {}
    keyword_tracking = pre_batch.get("keyword_tracking") or {}

    if not coverage and not density and not main_kw:
        return ""

    parts = ["â•â•â• STATUS POKRYCIA FRAZ â•â•â•"]

    if main_kw:
        kw_name = main_kw.get("keyword", "") if isinstance(main_kw, dict) else str(main_kw)
        synonyms = main_kw.get("synonyms", []) if isinstance(main_kw, dict) else []
        if kw_name:
            parts.append(f'HasÅ‚o gÅ‚Ã³wne: "{kw_name}"')
        if synonyms:
            parts.append(f'Synonimy (uÅ¼ywaj zamiennie): {", ".join(synonyms[:5])}')

    current_cov = coverage.get("current", coverage.get("current_coverage"))
    target_cov = coverage.get("target", coverage.get("target_coverage"))
    if current_cov is not None and target_cov is not None:
        parts.append(f'\nPokrycie fraz: {current_cov}% z docelowych {target_cov}%')

    missing = coverage.get("missing_phrases") or coverage.get("uncovered") or []
    if missing:
        parts.append("âš ï¸ BRAKUJÄ„CE FRAZY, wpleÄ‡ w tym batchu:")
        for m in missing[:8]:
            name = m.get("keyword", m) if isinstance(m, dict) else m
            parts.append(f'  â†’ "{name}"')

    if density:
        current_d = density.get("current")
        target_range = density.get("target_range") or []
        if current_d is not None:
            range_str = f'{target_range[0]}-{target_range[1]}%' if len(target_range) >= 2 else "1.5-2.5%"
            status = "âœ… w normie" if target_range and len(target_range) >= 2 and target_range[0] <= current_d <= target_range[1] else "âš ï¸ do korekty"
            parts.append(f'\nGÄ™stoÅ›Ä‡ fraz: {current_d}% (cel: {range_str}) {status}')

        overused_d = density.get("overused") or []
        if overused_d:
            over_names = ", ".join(f'"{o}"' if isinstance(o, str) else f'"{o.get("keyword", "")}"' for o in overused_d[:5])
            parts.append(f'NaduÅ¼ywane: {over_names}, uÅ¼yj synonimÃ³w')

    if keyword_tracking:
        total_kw = keyword_tracking.get("total_keywords", 0)
        covered_kw = keyword_tracking.get("covered", 0)
        if total_kw and covered_kw:
            parts.append(f'\nTracking: {covered_kw}/{total_kw} fraz pokrytych')

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_style(pre_batch):
    style = pre_batch.get("style_instructions") or pre_batch.get("style_instructions_v39") or {}

    if not style:
        return ""

    parts = ["â•â•â• STYL â•â•â•"]

    if isinstance(style, dict):
        tone = style.get("tone", "")
        if tone:
            parts.append(f'Ton: {tone}')

        para_len = style.get("paragraph_length", "")
        if para_len:
            parts.append(f'DÅ‚ugoÅ›Ä‡ akapitÃ³w: {para_len} sÅ‚Ã³w')

        forbidden = style.get("forbidden_phrases") or style.get("avoid_phrases") or []
        if forbidden:
            parts.append(f'ZAKAZANE zwroty: {", ".join(f"{f}" for f in forbidden[:8])}')

        preferred = style.get("preferred_phrases") or style.get("use_phrases") or []
        if preferred:
            parts.append(f'Preferowane zwroty: {", ".join(preferred[:5])}')

        persona = style.get("persona", "")
        if persona:
            parts.append(f'Perspektywa: {persona}')
    elif isinstance(style, str):
        parts.append(_word_trim(style, 500))

    return "\n".join(parts) if len(parts) > 1 else ""


def _fmt_legal_medical(pre_batch):
    legal_ctx = pre_batch.get("legal_context") or {}
    medical_ctx = pre_batch.get("medical_context") or {}
    ymyl_enrich = pre_batch.get("_ymyl_enrichment") or {}
    ymyl_intensity = pre_batch.get("_ymyl_intensity", "full")

    parts = []

    # v50: For "light" YMYL: DON'T inject full legal/medical framework
    if ymyl_intensity == "light":
        light_note = pre_batch.get("_light_ymyl_note", "")
        if light_note:
            parts.append("â•â•â• ASPEKT REGULACYJNY (peryferyjny, NIE gÅ‚Ã³wny temat!) â•â•â•")
            parts.append(f"  {light_note}")
            parts.append("  âš ï¸ OGRANICZENIE: Wspomnij o regulacjach MAX 1-2 razy w CAÅYM artykule.")
            parts.append("  NIE cytuj artykuÅ‚Ã³w ustaw, NIE dodawaj sygnatur orzeczeÅ„,")
            parts.append("  NIE dodawaj disclaimera o konsultacji z prawnikiem/lekarzem.")
            parts.append("  ArtykuÅ‚ jest EDUKACYJNY/TECHNICZNY, nie prawniczy/medyczny.")
        return "\n".join(parts) if parts else ""

    if legal_ctx and legal_ctx.get("active"):
        parts.append("â•â•â• KONTEKST PRAWNY (YMYL) â•â•â•")
        parts.append("Ten artykuÅ‚ dotyczy tematyki prawnej. MUSISZ:")
        parts.append("  1. CytowaÄ‡ realne przepisy i orzeczenia â€” ALE TYLKO te pasujÄ…ce do gaÅ‚Ä™zi prawa artykuÅ‚u")
        parts.append("  2. DodaÄ‡ disclaimer o konsultacji z prawnikiem")
        parts.append("  3. NIE wymyÅ›laÄ‡ sygnatur ani dat orzeczeÅ„")
        parts.append("")
        parts.append("ğŸš« BÅÄ˜DY KRYTYCZNE â€” BEZWZGLÄ˜DNY ZAKAZ:")
        parts.append("  â€¢ JEDNOSTKI: mg/100 ml â†’ BÅÄ„D. UÅ¼ywaj: promile (â€°) lub mg/dmÂ³")
        parts.append("  â€¢ KARA 178a Â§1: do 2 lat â†’ BÅÄ„D. PrawidÅ‚owo: do 3 lat (nowelizacja 2023)")
        parts.append("  â€¢ RECYDYWA: nie definiuj terminem '2 lat' â€” brak takiego wymogu")
        parts.append("  â€¢ SYGNATURA I C / II C w kontekÅ›cie konfiskaty â†’ BÅÄ„D: to sprawa cywilna")
        parts.append("  â€¢ PLACEHOLDER 'odpowiednich przepisÃ³w' â†’ zawsze podaj konkretny art.")
        
        # Inject Wikipedia articles if available
        wiki_arts = pre_batch.get("legal_wiki_articles") or []
        if wiki_arts:
            parts.append("")
            parts.append("WIKIPEDIA â€” TREÅšÄ† PRZEPISÃ“W (moÅ¼esz cytowaÄ‡ jako ÅºrÃ³dÅ‚o uzupeÅ‚niajÄ…ce):")
            for w in wiki_arts[:4]:
                if w.get("found"):
                    parts.append(f"  [{w['article_ref']}] {w['title']}:")
                    parts.append(f"  {w['extract'][:300]}")
                    parts.append(f"  Å¹rÃ³dÅ‚o: {w['url']}")
                    parts.append("")
        parts.append("")
        parts.append("âš ï¸ WERYFIKACJA ORZECZEÅƒ â€” OBOWIÄ„ZKOWA:")
        parts.append("  Sygnatura zdradza typ sprawy:")
        parts.append("  â€¢ II K, III K, AKa, AKo, AKz = KARNA â€” pasuje do art. KK, KW")
        parts.append("  â€¢ I C, II C, ACa, ACo = CYWILNA â€” pasuje do art. KC, KRO")
        parts.append("  â€¢ I P, II P, Pa = PRACY â€” pasuje do KP")
        parts.append("  âŒ NIE cytuj wyroku cywilnego (I C, II C) w artykule o prawie KARNYM")
        parts.append("  âŒ NIE cytuj wyroku karnego (II K) w artykule o prawie CYWILNYM")
        parts.append("  JeÅ›li Å¼aden z podanych wyrokÃ³w nie pasuje do gaÅ‚Ä™zi prawa â€” pomiÅ„ cytowania,")
        parts.append("  napisz artykuÅ‚ bez sygnatur. Lepiej brak cytatu niÅ¼ bÅ‚Ä™dny.")
        
        # v47.2: Claude's enrichment: specific articles and concepts
        legal_enrich = ymyl_enrich.get("legal", {})
        if legal_enrich.get("articles"):
            parts.append("")
            parts.append("PODSTAWA PRAWNA (kluczowe przepisy):")
            for art in legal_enrich["articles"][:5]:
                parts.append(f"  â€¢ {art}")
        if legal_enrich.get("acts"):
            parts.append(f"  Ustawy: {', '.join(legal_enrich['acts'][:4])}")
        if legal_enrich.get("key_concepts"):
            parts.append(f"  Kluczowe pojÄ™cia: {', '.join(legal_enrich['key_concepts'][:6])}")
        
        parts.append("")
        parts.append("FORMATY CYTOWAÅƒ PRAWNYCH:")
        parts.append('  â€¢ Przepisy: "art. 13 Â§ 1 k.c.", "art. 58 Â§ 2 k.r.o."')
        parts.append('  â€¢ Wyroki: "wyrok SN z 12.03.2021, III CZP 45/19"')
        parts.append('  â€¢ Dziennik Ustaw: "Dz.U. 2023 poz. 1234"')
        parts.append('  Causal legal: "niedopeÅ‚nienie obowiÄ…zku skutkuje...", "brak zgÅ‚oszenia prowadzi do..."')

        instruction = legal_ctx.get("legal_instruction", "")
        if instruction:
            parts.append(f'\n{instruction[:600]}')

        judgments = legal_ctx.get("top_judgments") or []
        if judgments:
            parts.append("\nOrzeczenia do zacytowania:")
            for j in judgments[:3]:
                if isinstance(j, dict):
                    sig = j.get("signature", j.get("caseNumber", ""))
                    court = j.get("court", j.get("courtName", ""))
                    date = j.get("date", j.get("judgmentDate", ""))
                    matched = j.get("matched_article", "")
                    line = f'  â€¢ {sig}, {court} ({date})'
                    if matched:
                        line += f' [dot. {matched}]'
                    parts.append(line)

        citation_hint = legal_ctx.get("citation_hint", "")
        if citation_hint:
            parts.append(f'\n{citation_hint}')

    if medical_ctx and medical_ctx.get("active"):
        if parts:
            parts.append("")
        parts.append("â•â•â• KONTEKST MEDYCZNY (YMYL) â•â•â•")
        parts.append("Ten artykuÅ‚ dotyczy tematyki zdrowotnej. MUSISZ:")
        parts.append("  1. CytowaÄ‡ ÅºrÃ³dÅ‚a naukowe (podane niÅ¼ej lub ogÃ³lne: 'badania wskazujÄ…', 'wedÅ‚ug wytycznych')")
        parts.append("  2. NIE wymyÅ›laÄ‡ statystyk ani nazw badaÅ„")
        parts.append("  3. W OSTATNIM batchu: dodaÄ‡ disclaimer 'ArtykuÅ‚ ma charakter informacyjny i nie zastÄ™puje konsultacji lekarskiej.'")
        parts.append("  4. PowoÅ‚aÄ‡ siÄ™ na min. 1 instytucjÄ™ (np. WHO, NFZ, PTOiAu, MZ, Cochrane) per batch")
        parts.append("  5. UÅ¼yÄ‡ min. 1 sformuÅ‚owania opartego na dowodach per batch: 'badania wskazujÄ…...', 'wedÅ‚ug meta-analizy...'")
        parts.append("  WAÅ»NE: ArtykuÅ‚ bez ÅºrÃ³deÅ‚ medycznych = YMYL score 0/100 = odrzucenie.")
        
        # v47.2: Claude's enrichment: specialization, evidence guidelines
        med_enrich = ymyl_enrich.get("medical", {})
        if med_enrich.get("specialization"):
            parts.append(f"\n  Specjalizacja: {med_enrich['specialization']}")
        if med_enrich.get("condition"):
            cond = med_enrich["condition"]
            latin = med_enrich.get("condition_latin", "")
            icd = med_enrich.get("icd10", "")
            parts.append(f"  Choroba/stan: {cond}" + (f" ({latin})" if latin else "") + (f" [ICD-10: {icd}]" if icd else ""))
        if med_enrich.get("key_drugs"):
            parts.append(f"  Kluczowe leki: {', '.join(med_enrich['key_drugs'][:5])}")
        if med_enrich.get("evidence_note"):
            parts.append(f"\n  âš ï¸ WYTYCZNE: {med_enrich['evidence_note']}")
        
        parts.append("")
        parts.append("FORMATY CYTOWAÅƒ MEDYCZNYCH:")
        parts.append('  â€¢ "Smith i wsp. (2023)", "Kowalski et al. (2024)"')
        parts.append('  â€¢ "PMID:12345678", "DOI:10.1000/xyz"')
        parts.append("")
        parts.append("HIERARCHIA DOWODÃ“W (cytuj najwyÅ¼szy dostÄ™pny):")
        parts.append("  1. Meta-analiza / PrzeglÄ…d systematyczny (najsilniejszy)")
        parts.append("  2. RCT (badanie randomizowane)")
        parts.append("  3. Badanie kohortowe")
        parts.append("  4. Opis przypadku")
        parts.append("  5. Opinia eksperta (najsÅ‚abszy)")
        parts.append('  Causal medical: "nieleczone prowadzi do...", "brak terapii skutkuje..."')

        instruction = medical_ctx.get("medical_instruction", "")
        if instruction:
            parts.append(f'\n{instruction[:600]}')

        publications = medical_ctx.get("top_publications") or []
        if publications:
            parts.append("\nPublikacje do zacytowania:")
            for p in publications[:5]:
                if isinstance(p, dict):
                    title = p.get("title", "")[:80]
                    authors = p.get("authors", "")[:40]
                    year = p.get("year", "")
                    pmid = p.get("pmid", "")
                    parts.append(f'  â€¢ {authors} ({year}): "{title}" PMID:{pmid}')

    return "\n".join(parts) if parts else ""


def _fmt_experience_markers(pre_batch):
    enhanced = pre_batch.get("enhanced") or {}
    markers = enhanced.get("experience_markers") or []

    if not markers:
        return ""

    parts = ["â•â•â• SYGNAÅY DOÅšWIADCZENIA (E-E-A-T) â•â•â•",
             "WpleÄ‡ min 1 sygnaÅ‚, Å¼e autor MA doÅ›wiadczenie z tematem:"]

    for m in markers[:5]:
        if isinstance(m, str):
            parts.append(f'  â€¢ {m}')
        elif isinstance(m, dict):
            parts.append(f'  â€¢ {m.get("marker", m.get("text", ""))}')

    return "\n".join(parts)


def _fmt_causal_context(pre_batch):
    enhanced = pre_batch.get("enhanced") or {}
    causal = enhanced.get("causal_context", "")
    info_gain = enhanced.get("information_gain", "")

    parts = []

    if causal:
        parts.append("â•â•â• KONTEKST PRZYCZYNOWO-SKUTKOWY â•â•â•")
        parts.append(_word_trim(causal, 500))

    if info_gain:
        if parts:
            parts.append("")
        parts.append("â•â•â• INFORMATION GAIN (przewaga nad konkurencjÄ…) â•â•â•")
        parts.append(_word_trim(info_gain, 500))

    return "\n".join(parts) if parts else ""


def _fmt_depth_signals(pre_batch):
    """Depth signals: inject when previous batch scored low on depth
    or always for FULL YMYL content.
    
    v50: Only force for full YMYL intensity, not light.
    Based on 10 depth signals from GPT prompt with weights.
    """
    last_depth = pre_batch.get("_last_depth_score")
    is_ymyl = pre_batch.get("_is_ymyl", False)
    ymyl_intensity = pre_batch.get("_ymyl_intensity", "none")
    is_full_ymyl = is_ymyl and ymyl_intensity == "full"
    
    # Only force depth for FULL YMYL, not light
    threshold = 40 if is_full_ymyl else 30
    if last_depth is not None and last_depth >= threshold and not is_full_ymyl:
        return ""
    
    # If no depth data at all and not full YMYL, skip
    if last_depth is None and not is_full_ymyl:
        return ""
    
    parts = ["â•â•â• SYGNAÅY GÅÄ˜BOKOÅšCI (dodaj od najwyÅ¼szej wagi) â•â•â•"]
    
    if last_depth is not None:
        parts.append(f"âš ï¸ Ostatni batch: depth {last_depth}/100 (prÃ³g: {threshold}). Dodaj wiÄ™cej konkretÃ³w!")
    
    parts.append("")
    # v50: Legal references only for FULL YMYL
    if is_full_ymyl:
        parts.append("WAGA 2.5: referencje prawne (art. k.c., wyroki SN, Dz.U.) + naukowe (PMID, DOI, badania)")
    parts.append('WAGA 2.0: konkretne liczby (kwoty PLN, %, okresy, NIE "okoÅ‚o")')
    parts.append('WAGA 1.8: nazwane instytucje (konkretny sÄ…d/urzÄ…d, NIE "wÅ‚aÅ›ciwy sÄ…d") + praktyczne porady (w praktyce, czÄ™sty bÅ‚Ä…d)')
    parts.append("WAGA 1.5: wyjaÅ›nienia przyczynowe (poniewaÅ¼, w wyniku) + wyjÄ…tki (z wyjÄ…tkiem, chyba Å¼e) + konkretne daty")
    parts.append("WAGA 1.2: porÃ³wnania (w odrÃ³Å¼nieniu od) | WAGA 1.0: kroki procedur (najpierw/nastÄ™pnie)")
    
    return "\n".join(parts)


def _fmt_natural_polish(pre_batch):
    """v50: Natural Polish writing instructions: fleksja, spacing, anti-stuffing.

    Based on natural_polish_instructions.py (master-seo-api-main).
    Inlined here because prompt_builder runs in Brajn, not master.
    
    Prevents keyword stuffing by teaching Claude that:
    1. Polish inflected forms count as the same keyword
    2. Minimum spacing between repetitions is required
    3. Max 2 uses of same phrase per paragraph
    """
    # Get keywords from pre_batch
    keywords_info = pre_batch.get("keywords") or {}
    must_kw = keywords_info.get("basic_must_use") or []
    ext_kw = keywords_info.get("extended_this_batch") or []

    all_kw = []
    for kw in must_kw + ext_kw:
        if isinstance(kw, dict):
            name = kw.get("keyword", "")
            kw_type = kw.get("type", "BASIC").upper()
        elif isinstance(kw, str):
            name = kw
            kw_type = "BASIC"
        else:
            continue
        if name:
            all_kw.append((name, kw_type))

    if not all_kw:
        return ""

    # Spacing rules
    SPACING = {"MAIN": 60, "BASIC": 80, "EXTENDED": 120}

    parts = ["â•â•â• NATURALNY POLSKI, ANTY-STUFFING â•â•â•"]

    parts.append(
        "ğŸ”„ FLEKSJA: Odmiany frazy liczÄ… siÄ™ jako jedno uÅ¼ycie!\n"
        '   "zespÃ³Å‚ turnera" = "zespoÅ‚u turnera" = "zespoÅ‚em turnera"\n'
        "   Pisz naturalnie, uÅ¼ywaj rÃ³Å¼nych przypadkÃ³w gramatycznych.\n"
        "   NIE MUSISZ powtarzaÄ‡ frazy w mianowniku. System zaliczy kaÅ¼dÄ… odmianÄ™."
    )

    spacing_lines = []
    for name, kw_type in all_kw[:8]:
        spacing = SPACING.get(kw_type, 80)
        spacing_lines.append(f'  â€¢ "{name}" ({kw_type}): min {spacing} sÅ‚Ã³w miÄ™dzy powtÃ³rzeniami')
    if spacing_lines:
        parts.append("ğŸ“ ODSTÄ˜PY MIÄ˜DZY POWTÃ“RZENIAMI:\n" + "\n".join(spacing_lines))

    parts.append(
        "âš ï¸ ZASADY:\n"
        "  â€¢ Max 2Ã— ta sama fraza w jednym akapicie\n"
        "  â€¢ RozkÅ‚adaj frazy RÃ“WNOMIERNIE w tekÅ›cie (nie grupuj na poczÄ…tku/koÅ„cu)\n"
        "  â€¢ Zamiast powtÃ³rzenia uÅ¼yj: synonimu, zaimka, opisu ('ta choroba', 'omawiany zespÃ³Å‚')\n"
        "  â€¢ Podmiot â†’ dopeÅ‚nienie â†’ synonim â†’ kolejny akapit â†’ ponownie fraza"
    )

    return "\n".join(parts)


def _fmt_phrase_hierarchy(pre_batch):
    """Format phrase hierarchy: roots, extensions, strategy.
    
    Data sources (checked in order):
    1. pre_batch["enhanced"]["phrase_hierarchy"]: from enhanced_pre_batch.py
    2. pre_batch["_phrase_hierarchy"]: injected by app.py from /phrase_hierarchy endpoint
    """
    hier = (pre_batch.get("enhanced") or {}).get("phrase_hierarchy") or pre_batch.get("_phrase_hierarchy") or {}
    if not hier:
        return ""

    parts = ["â•â•â• HIERARCHIA FRAZ â•â•â•"]

    strategies = hier.get("strategies") or {}

    # 1. Extensions sufficient: don't repeat root standalone
    ext_suff = strategies.get("extensions_sufficient") or {}
    ext_roots = ext_suff.get("roots") or []
    if ext_roots:
        parts.append("RDZENIE POKRYTE ROZSZERZENIAMI (NIE powtarzaj samodzielnie!):")
        for root_info in ext_roots[:8]:
            if isinstance(root_info, dict):
                root = root_info.get("root", root_info.get("keyword", ""))
                extensions = root_info.get("extensions", [])
                ext_list = ", ".join(f'"{e}"' if isinstance(e, str) else f'"{e.get("keyword", "")}"' for e in extensions[:5])
                parts.append(f'  â€¢ "{root}" â†’ uÅ¼ywaj rozszerzeÅ„: {ext_list}')
            elif isinstance(root_info, str):
                parts.append(f'  â€¢ "{root_info}" â†’ uÅ¼ywaj rozszerzeÅ„ zamiast rdzenia')

    # 2. Mixed: some standalone + extensions
    mixed = strategies.get("mixed") or {}
    mixed_roots = mixed.get("roots") or []
    if mixed_roots:
        parts.append("RDZENIE MIESZANE (kilka samodzielnych uÅ¼yÄ‡ + rozszerzenia):")
        for root_info in mixed_roots[:8]:
            if isinstance(root_info, dict):
                root = root_info.get("root", root_info.get("keyword", ""))
                standalone = root_info.get("standalone_uses", "1-2")
                extensions = root_info.get("extensions", [])
                ext_list = ", ".join(f'"{e}"' if isinstance(e, str) else f'"{e.get("keyword", "")}"' for e in extensions[:5])
                parts.append(f'  â€¢ "{root}" â†’ {standalone}Ã— samodzielnie + rozszerzenia: {ext_list}')
            elif isinstance(root_info, str):
                parts.append(f'  â€¢ "{root_info}" â†’ kilka samodzielnie + rozszerzenia')

    # 3. Need standalone: extensions insufficient
    standalone = strategies.get("need_standalone") or {}
    standalone_roots = standalone.get("roots") or []
    if standalone_roots:
        parts.append("RDZENIE WYMAGAJÄ„CE SAMODZIELNYCH UÅ»YÄ†:")
        for root_info in standalone_roots[:8]:
            if isinstance(root_info, dict):
                root = root_info.get("root", root_info.get("keyword", ""))
                target = root_info.get("remaining", root_info.get("target", "?"))
                parts.append(f'  â€¢ "{root}" â†’ uÅ¼yj samodzielnie jeszcze ~{target}Ã—')
            elif isinstance(root_info, str):
                parts.append(f'  â€¢ "{root_info}" â†’ uÅ¼yj samodzielnie')

    # 4. Entity phrases (if available)
    entity_phrases = hier.get("entity_phrases") or []
    if entity_phrases:
        ep_list = ", ".join(f'"{e}"' if isinstance(e, str) else f'"{e.get("keyword", "")}"' for e in entity_phrases[:6])
        parts.append(f"FRAZY ENCYJNE (wpleÄ‡ naturalnie): {ep_list}")

    # 5. Triplet phrases (if available)
    triplet_phrases = hier.get("triplet_phrases") or []
    if triplet_phrases:
        tp_list = ", ".join(f'"{t}"' if isinstance(t, str) else f'"{t.get("keyword", "")}"' for t in triplet_phrases[:6])
        parts.append(f"FRAZY TRIPLETOWE (relacje do wplecenia): {tp_list}")

    if len(parts) <= 1:
        return ""

    return "\n".join(parts)


def _fmt_h2_remaining(pre_batch):
    h2_remaining = pre_batch.get("h2_remaining") or []
    if not h2_remaining:
        return ""

    h2_list = ", ".join(f'"{h}"' for h in h2_remaining[:6])
    return f"â•â•â• PLAN â•â•â•\nPozostaÅ‚e sekcje H2 w artykule: {h2_list}\nNie zachodÅº na ich tematy. ZostanÄ… pokryte pÃ³Åºniej."


def _fmt_output_format(h2, batch_type):
    if batch_type in ("INTRO", "intro"):
        return f"""â•â•â• FORMAT ODPOWIEDZI â•â•â•
Pisz TYLKO treÅ›Ä‡ wstÄ™pu. NIE zaczynaj od "h2:". WstÄ™p nie ma nagÅ‚Ã³wka.
80-150 sÅ‚Ã³w. FrazÄ™ gÅ‚Ã³wnÄ… wpleÄ‡ w PIERWSZE zdanie.
NIE dodawaj komentarzy, wyjaÅ›nieÅ„. TYLKO treÅ›Ä‡ wstÄ™pu."""
    
    return f"""â•â•â• FORMAT ODPOWIEDZI â•â•â•
Pisz TYLKO treÅ›Ä‡ tego batcha. Zaczynaj dokÅ‚adnie od:

h2: {h2}

Potem: akapity tekstu (40-150 sÅ‚Ã³w kaÅ¼dy), opcjonalnie h3: [podsekcja].
NIE dodawaj komentarzy, wyjaÅ›nieÅ„, podsumowaÅ„. TYLKO treÅ›Ä‡ artykuÅ‚u."""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FAQ PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_faq_system_prompt(pre_batch=None):
    """System prompt for FAQ generation."""
    base = (
        "JesteÅ› doÅ›wiadczonym polskim copywriterem SEO. "
        "Piszesz sekcjÄ™ FAQ: zwiÄ™zÅ‚e, konkretne odpowiedzi na pytania uÅ¼ytkownikÃ³w. "
        "KaÅ¼da odpowiedÅº ma szansÄ™ trafiÄ‡ do Google Featured Snippet. Pisz bezpoÅ›rednio i merytorycznie."
    )

    gpt_instructions = ""
    if pre_batch:
        gpt_instructions = pre_batch.get("gpt_instructions_v39", "")

    if gpt_instructions:
        return base + "\n\n" + gpt_instructions
    return base


def build_faq_user_prompt(paa_data, pre_batch=None):
    """User prompt for FAQ generation."""
    # Normalize: if paa_data is a list (raw PAA questions), wrap it
    if isinstance(paa_data, list):
        paa_data = {"serp_paa": paa_data}
    elif not isinstance(paa_data, dict):
        paa_data = {}
    paa_questions = paa_data.get("serp_paa") or []
    unused = paa_data.get("unused_keywords") or {}
    avoid = paa_data.get("avoid_in_faq") or []
    if isinstance(avoid, dict):
        avoid = avoid.get("topics") or []
    elif not isinstance(avoid, list):
        avoid = []
    instructions_raw = paa_data.get("instructions", "")
    if isinstance(instructions_raw, dict):
        parts = []
        for k, v in instructions_raw.items():
            if isinstance(v, str):
                parts.append(f"â€¢ {v}")
            elif isinstance(v, dict):
                for sk, sv in v.items():
                    if isinstance(sv, str):
                        parts.append(f"â€¢ {sk}: {sv}")
        instructions = "\n".join(parts)
    elif isinstance(instructions_raw, str):
        instructions = instructions_raw
    else:
        instructions = ""

    enhanced_paa = []
    if pre_batch:
        enhanced = pre_batch.get("enhanced") or {}
        if not isinstance(enhanced, dict):
            enhanced = {}
        enhanced_paa = enhanced.get("paa_from_serp") or []
        if not isinstance(enhanced_paa, list):
            enhanced_paa = []

    keyword_limits = {}
    if pre_batch:
        keyword_limits = pre_batch.get("keyword_limits") or {}
        if not isinstance(keyword_limits, dict):
            keyword_limits = {}
    stop_raw = keyword_limits.get("stop_keywords") or []
    stop_names = [s.get("keyword", s) if isinstance(s, dict) else s for s in stop_raw]

    style = {}
    if pre_batch:
        style = pre_batch.get("style_instructions") or {}

    sections = []

    sections.append("""â•â•â• SEKCJA FAQ â•â•â•
Napisz sekcjÄ™ FAQ. Zaczynaj DOKÅADNIE od:
h2: NajczÄ™Å›ciej zadawane pytania""")

    all_paa = list(dict.fromkeys(paa_questions + enhanced_paa))
    if all_paa:
        sections.append("Pytania z Google (People Also Ask), to NAPRAWDÄ˜ pytajÄ… uÅ¼ytkownicy:")
        for i, q in enumerate(all_paa[:8], 1):
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text and q_text.strip():
                sections.append(f'  {i}. {q_text}')
        sections.append("Wybierz 4-6 najlepszych. MoÅ¼esz przeformuÅ‚owaÄ‡, ale zachowaj sens.")

    if unused:
        if isinstance(unused, dict):
            unused_list = []
            for cat, items in unused.items():
                if isinstance(items, list):
                    unused_list.extend(items[:5])
                elif isinstance(items, str):
                    unused_list.append(items)
            if unused_list:
                names = ", ".join(f'"{u}"' if isinstance(u, str) else f'"{u.get("keyword", "")}"' for u in unused_list[:8])
                sections.append(f'\nFrazy jeszcze nieuÅ¼yte, wpleÄ‡ w odpowiedzi: {names}')
        elif isinstance(unused, list):
            names = ", ".join(f'"{u}"' for u in unused[:8])
            sections.append(f'\nFrazy jeszcze nieuÅ¼yte, wpleÄ‡ w odpowiedzi: {names}')

    if avoid:
        topics = ", ".join(f'"{a}"' if isinstance(a, str) else f'"{a.get("topic", "")}"' for a in avoid[:8])
        sections.append(f'\nNIE powtarzaj tematÃ³w juÅ¼ pokrytych w artykule: {topics}')

    if stop_names:
        sections.append(f'\nğŸ›‘ STOP, NIE UÅ»YWAJ: {", ".join(f"{s}" for s in stop_names[:5])}')

    if style:
        forbidden = style.get("forbidden_phrases") or []
        if forbidden:
            sections.append(f'ZAKAZANE zwroty: {", ".join(forbidden[:5])}')

    if pre_batch and pre_batch.get("article_memory"):
        mem = pre_batch["article_memory"]
        if isinstance(mem, dict):
            topics = mem.get("topics_covered") or []
            if topics:
                topic_names = [t if isinstance(t, str) else t.get("topic", "") for t in topics[:6]]
                sections.append(f'\nTematy z artykuÅ‚u (nie powtarzaj): {", ".join(topic_names)}')

    if instructions:
        sections.append(f'\n{instructions}')

    sections.append("""
â•â•â• FORMAT â•â•â•
h2: NajczÄ™Å›ciej zadawane pytania

h3: [Pytanie, 5-10 sÅ‚Ã³w, zaczynaj od Jak/Czy/Co/Dlaczego/Ile]
[OdpowiedÅº 60-120 sÅ‚Ã³w]
â†’ Zdanie 1: BEZPOÅšREDNIA odpowiedÅº
â†’ Zdanie 2-3: rozwiniÄ™cie z konkretem
â†’ Zdanie 4: praktyczna wskazÃ³wka lub wyjÄ…tek

Napisz 4-6 pytaÅ„. Pisz TYLKO treÅ›Ä‡, bez komentarzy.""")

    return "\n\n".join(sections)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# H2 PLAN PROMPT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_h2_plan_system_prompt():
    """System prompt for H2 plan generation."""
    return (
        "JesteÅ› ekspertem SEO z 10-letnim doÅ›wiadczeniem w planowaniu architektury treÅ›ci. "
        "Tworzysz logiczne, wyczerpujÄ…ce struktury nagÅ‚Ã³wkÃ³w H2, ktÃ³re pokrywajÄ… temat kompleksowo "
        "i dajÄ… przewagÄ™ nad konkurencjÄ… dziÄ™ki pokryciu luk treÅ›ciowych."
    )


def build_h2_plan_user_prompt(main_keyword, mode, s1_data, all_user_phrases, user_h2_hints=None):
    """Build readable H2 plan prompt from S1 analysis data."""
    s1_data = s1_data or {}
    competitor_h2 = s1_data.get("competitor_h2_patterns") or []
    suggested_h2s = (s1_data.get("content_gaps") or {}).get("suggested_new_h2s", [])
    content_gaps = s1_data.get("content_gaps") or {}
    causal_triplets = s1_data.get("causal_triplets") or {}
    paa = s1_data.get("paa") or s1_data.get("paa_questions") or []
    # v52.0: Related searches - Google sugeruje te pytania/frazy uÅ¼ytkownikom
    serp_analysis = s1_data.get("serp_analysis") or {}
    related_searches = (s1_data.get("related_searches")
                        or serp_analysis.get("related_searches") or [])

    sections = []

    mode_desc = "standard = peÅ‚ny artykuÅ‚" if mode == "standard" else "fast = krÃ³tki artykuÅ‚, max 3 sekcje"
    sections.append(f"""HASÅO GÅÃ“WNE: {main_keyword}
TRYB: {mode} ({mode_desc})""")

    if competitor_h2:
        # Sort by count descending if available
        def _h2_count(h):
            if isinstance(h, dict):
                return h.get("count", h.get("sources", 0))
            return 0
        sorted_h2 = sorted(competitor_h2[:30], key=_h2_count, reverse=True)
        total_sources = max((_h2_count(sorted_h2[0]) for _ in [1]), default=1) or 1

        lines = ["â•â•â• WZORCE H2 KONKURENCJI â€” posortowane po popularnoÅ›ci â•â•â•",
                 "Liczba przy H2 = ilu konkurentÃ³w uÅ¼ywa tego tematu.",
                 "H2 z wysokÄ… liczbÄ… = MUST HAVE w Twoim artykule (uÅ¼ytkownicy tego szukajÄ…)."]
        for i, h in enumerate(sorted_h2[:20], 1):
            if isinstance(h, dict):
                pattern = h.get("text", h.get("pattern", h.get("h2", str(h))))
                count = _h2_count(h)
                bar = "â–ˆ" * min(count, 8)
                lines.append(f"  {i:2}. [{bar:<8}] {count}Ã— â€” {pattern}")
            elif isinstance(h, str):
                lines.append(f"  {i:2}. {h}")
        sections.append("\n".join(lines))

    if suggested_h2s:
        lines = ["â•â•â• SUGEROWANE NOWE H2 (luki, tego NIKT z konkurencji nie pokrywa) â•â•â•"]
        for h in suggested_h2s[:10]:
            h_text = h if isinstance(h, str) else h.get("h2", h.get("title", str(h)))
            lines.append(f"  â€¢ {h_text}")
        sections.append("\n".join(lines))

    # Content gaps: ordered by priority (GPT prompt: PAA_UNANSWERED > DEPTH_MISSING > SUBTOPIC_MISSING)
    gap_priority_map = {
        "paa_unanswered": ("ğŸ”´ HIGH", "PAA bez odpowiedzi"),
        "depth_missing": ("ğŸŸ¡ MED-HIGH", "Brak gÅ‚Ä™bi"),
        "subtopic_missing": ("ğŸŸ¢ MED", "BrakujÄ…cy podtemat"),
        "gaps": ("", "Luka"),
    }
    all_gaps = []
    for key in ("paa_unanswered", "depth_missing", "subtopic_missing", "gaps"):
        priority, label = gap_priority_map.get(key, ("", ""))
        items = content_gaps.get(key) or []
        for item in items[:5]:
            gap_text = item if isinstance(item, str) else item.get("gap", item.get("topic", str(item)))
            if gap_text and gap_text not in [g[0] for g in all_gaps]:
                all_gaps.append((gap_text, priority, label))
    if all_gaps:
        lines = ["â•â•â• LUKI TREÅšCIOWE (tematy do pokrycia, priorytet od najwyÅ¼szego) â•â•â•"]
        for gap_text, priority, label in all_gaps[:10]:
            prefix = f"[{priority}] " if priority else ""
            lines.append(f"  â€¢ {prefix}{gap_text}")
        sections.append("\n".join(lines))

    if paa:
        lines = ["â•â•â• PYTANIA PAA (People Also Ask z Google) â•â•â•"]
        for q in paa[:8]:
            q_text = q.get("question", q) if isinstance(q, dict) else q
            if q_text:
                lines.append(f"  â“ {q_text}")
        sections.append("\n".join(lines))

    # v52.0: Related searches - Google podpowiada te frazy po wpisaniu main_keyword.
    # ZawierajÄ… intencje ktÃ³rych czÄ™sto BRAK w H2 konkurencji (np. "warunkowe umorzenie",
    # "doÅ¼ywotni zakaz", "organizmie wynosi") - waÅ¼ny signal dla tematycznego pokrycia H2.
    if related_searches:
        rs_texts = []
        for rs in related_searches[:12]:
            rs_t = rs if isinstance(rs, str) else (rs.get("query", "") or rs.get("text", ""))
            if rs_t:
                rs_texts.append(rs_t)
        if rs_texts:
            lines = ["â•â•â• RELATED SEARCHES (Google podpowiada po main_keyword) â•â•â•",
                     "UÅ¼yj tych fraz jako wskazÃ³wek tematycznych przy tworzeniu H2.",
                     "Wiele z nich to podtematy ktÃ³rych BRAK u konkurencji â€” Twoja szansa:"]
            for rs_t in rs_texts:
                lines.append(f"  ğŸ” {rs_t}")
            sections.append("\n".join(lines))

    triplet_list = (causal_triplets.get("chains") or causal_triplets.get("singles")
                    or causal_triplets.get("triplets") or [])[:8]
    if triplet_list:
        lines = ["â•â•â• PRZYCZYNOWE ZALEÅ»NOÅšCI (causeâ†’effect z konkurencji) â•â•â•",
                 "Confidence: ğŸ”´ â‰¥0.9 UÅ»YJ | ğŸŸ¡ â‰¥0.6 gdy pasuje | ğŸŸ¢ <0.6 opcjonalnie",
                 "is_chain=True (Aâ†’Bâ†’C) = najcenniejsze. Buduj logiczny przepÅ‚yw"]
        for t in triplet_list:
            if isinstance(t, dict):
                cause = t.get("cause", t.get("subject", ""))
                effect = t.get("effect", t.get("object", ""))
                conf = t.get("confidence", 0)
                is_chain = t.get("is_chain", False)
                
                # Priority indicator
                if conf >= 0.9:
                    ind = "ğŸ”´"
                elif conf >= 0.6:
                    ind = "ğŸŸ¡"
                else:
                    ind = "ğŸŸ¢"
                chain_tag = " [CHAIN]" if is_chain else ""
                conf_str = f" ({conf:.1f})" if conf else ""
                lines.append(f"  {ind} {cause} â†’ {effect}{conf_str}{chain_tag}")
            elif isinstance(t, str):
                lines.append(f"  â€¢ {t}")
        sections.append("\n".join(lines))

    # Fix #48: Entity-driven H2 generation â€” top entities should influence H2 names
    entity_seo = s1_data.get("entity_seo") or {}
    concept_ents = entity_seo.get("concept_entities") or entity_seo.get("topical_entities") or []
    must_mention = entity_seo.get("must_mention") or []
    top_named = entity_seo.get("top_entities") or []
    entity_salience = entity_seo.get("entity_salience") or []

    all_ents = []
    seen_ent = set()
    for src in [concept_ents, must_mention, top_named]:
        for e in src[:15]:
            name = e if isinstance(e, str) else (e.get("text") or e.get("entity") or e.get("display_text") or "")
            name_low = name.lower().strip()
            if name_low and name_low not in seen_ent and name_low != main_keyword.lower():
                seen_ent.add(name_low)
                sal = 0
                for se in entity_salience:
                    if isinstance(se, dict) and (se.get("entity", "")).lower() == name_low:
                        sal = se.get("salience", 0)
                        break
                all_ents.append((name, sal))

    if all_ents:
        # Sort by salience descending
        all_ents.sort(key=lambda x: x[1], reverse=True)
        lines = ["â•â•â• TOP ENCJE Z KONKURENCJI â€” UÅ»YJ W NAZEWNICTWIE H2 â•â•â•",
                 "PoniÅ¼sze encje pojawiajÄ… siÄ™ najczÄ™Å›ciej u konkurencji.",
                 "ZASADA: KaÅ¼de H2 powinno zawieraÄ‡ 1-2 encje z tej listy.",
                 "To daje H2 efekt typu Surfer/NeuronWriter â€” H2 bogate w encje.",
                 "NIE kopiuj dosÅ‚ownie, ale wplataj naturalnie w nazwy H2.",
                 "PrzykÅ‚ad: zamiast 'Konsekwencje' â†’ 'Konsekwencje prawne i utrata prawa jazdy'",
                 ""]
        for i, (name, sal) in enumerate(all_ents[:14], 1):
            sal_str = f" (salience: {sal:.2f})" if sal > 0 else ""
            priority = "ğŸ”´ MUST" if i <= 5 else ("ğŸŸ¡ HIGH" if i <= 10 else "ğŸŸ¢ OPT")
            lines.append(f"  {i:2}. [{priority}] {name}{sal_str}")
        sections.append("\n".join(lines))

    if user_h2_hints:
        h2_hints_list = "\n".join(f'  â€¢ "{h}"' for h in user_h2_hints[:10])
        sections.append(f"""â•â•â• FRAZY H2 UÅ»YTKOWNIKA â•â•â•

UÅ¼ytkownik podaÅ‚ te frazy z myÅ›lÄ… o nagÅ‚Ã³wkach H2.
Wykorzystaj je w nagÅ‚Ã³wkach tam, gdzie brzmiÄ… naturalnie po polsku.
Nie musisz uÅ¼yÄ‡ kaÅ¼dej, ale nie ignoruj ich. Dopasuj z wyczuciem.

JeÅ›li fraza brzmi sztucznie jako nagÅ‚Ã³wek, przeformuÅ‚uj lub pomiÅ„ (trafi do treÅ›ci).

FRAZY H2:
{h2_hints_list}""")

    if all_user_phrases:
        phrases_text = ", ".join(f'"{p}"' for p in all_user_phrases[:15])
        sections.append(f"""â•â•â• KONTEKST TEMATYCZNY (frazy BASIC/EXTENDED) â•â•â•

PoniÅ¼sze frazy bÄ™dÄ… uÅ¼yte W TREÅšCI artykuÅ‚u (nie w nagÅ‚Ã³wkach).
PodajÄ™ je Å¼ebyÅ› wiedziaÅ‚ jaki zakres tematyczny artykuÅ‚ musi pokryÄ‡
i zaplanowaÅ‚ H2 tak, by kaÅ¼da fraza miaÅ‚a naturalnÄ… sekcjÄ™:

{phrases_text}""")

    fast_note = "Tryb fast: DOKÅADNIE 3 sekcje + FAQ (4 H2 Å‚Ä…cznie)." if mode == "fast" else ""
    
    # v50.8 FIX 50: H2 scaling: minimum 5-6 sekcji nawet dla krÃ³tkich artykuÅ‚Ã³w.
    # WiÄ™cej sekcji = lepsza struktura, lepsze SEO, Å‚atwiejsze skanowanie.
    length_analysis = s1_data.get("length_analysis") or {}
    rec_length = length_analysis.get("recommended") or s1_data.get("recommended_length") or 0
    median_length = length_analysis.get("median") or s1_data.get("median_length") or 0
    
    if mode != "fast":
        target = rec_length or (median_length * 2) or 1500
        if target <= 1000:
            h2_range = "5-6"
            h2_min, h2_max = 5, 6
        elif target <= 2000:
            h2_range = "6-8"
            h2_min, h2_max = 6, 8
        elif target <= 3500:
            h2_range = "7-9"
            h2_min, h2_max = 7, 9
        else:
            h2_range = "8-12"
            h2_min, h2_max = 8, 12
        
        fast_note = (
            f"Tryb standard: {h2_range} sekcji + FAQ ({h2_min+1}-{h2_max+1} H2 Å‚Ä…cznie).\n"
            f"   UWAGA: Rekomendowana dÅ‚ugoÅ›Ä‡ artykuÅ‚u: ~{target} sÅ‚Ã³w (mediana konkurencji: {median_length}).\n"
            f"   KaÅ¼da sekcja H2 = ~{target // (h2_max + 1)}-{target // h2_min} sÅ‚Ã³w.\n"
            f"   NIE GENERUJ wiÄ™cej niÅ¼ {h2_max + 1} H2 (wliczajÄ…c FAQ)!"
        )
    
    h2_hint_rule = ("UwzglÄ™dnij frazy H2 uÅ¼ytkownika w nagÅ‚Ã³wkach, o ile brzmiÄ… naturalnie."
                    if user_h2_hints else "Dobierz nagÅ‚Ã³wki na podstawie S1 i luk treÅ›ciowych.")

    sections.append(f"""â•â•â• ZASADY â•â•â•

1. LICZBA H2: {fast_note}
2. OSTATNI H2 MUSI byÄ‡: "NajczÄ™Å›ciej zadawane pytania"
3. Pokryj najwaÅ¼niejsze wzorce z konkurencji + luki treÅ›ciowe (przewaga nad konkurencjÄ…)
4. {h2_hint_rule}
5. Logiczna narracja: od ogÃ³Å‚u do szczegÃ³Å‚u, chronologicznie, lub problemowo
6. NIE powtarzaj hasÅ‚a gÅ‚Ã³wnego dosÅ‚ownie w kaÅ¼dym H2
7. H2 muszÄ… brzmieÄ‡ naturalnie po polsku, Å¼adnego keyword stuffingu
8. ENCJE W H2: KaÅ¼de H2 powinno zawieraÄ‡ 1-2 encje z listy TOP ENCJI powyÅ¼ej.
   To poprawia topical authority i pokrycie tematyczne (jak w Surfer/NeuronWriter).
   Nie upychaj na siÅ‚Ä™, ale naturalnie wplataj encje w nazwy H2.
9. Preferuj H2 konkretne i informacyjne (z liczbami, encjami, terminami) nad ogÃ³lnikowe.
   âŒ "Kary" â†’ âœ… "Kary za jazdÄ™ po alkoholu â€” grzywna, zakaz i wiÄ™zienie"
   âŒ "Procedura" â†’ âœ… "Badanie alkomatem i procedura kontroli drogowej"

â•â•â• FORMAT ODPOWIEDZI â•â•â•

Odpowiedz TYLKO JSON array, bez markdown, bez komentarzy:
["H2 pierwszy", "H2 drugi", ..., "NajczÄ™Å›ciej zadawane pytania"]""")

    return "\n\n".join(sections)
