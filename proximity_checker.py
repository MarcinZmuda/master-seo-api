"""
ğŸ†• v36.8: PROXIMITY CHECKER - Wymuszanie bliskoÅ›ci encji

Sprawdza i wymusza:
- Entity proximity (encje w tym samym zdaniu/akapicie)
- Keyword clustering (powiÄ…zane sÅ‚owa kluczowe blisko siebie)
- Context windows dla fraz

Autor: Claude
Wersja: 36.8
"""

import re
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass

# ================================================================
# KONFIGURACJA
# ================================================================

@dataclass
class ProximityConfig:
    """Konfiguracja proximity checker."""
    
    # Progi proximity (w sÅ‚owach)
    SAME_SENTENCE_THRESHOLD: int = 0       # W tym samym zdaniu
    CLOSE_PROXIMITY_THRESHOLD: int = 30    # Blisko (max 30 sÅ‚Ã³w)
    MEDIUM_PROXIMITY_THRESHOLD: int = 75   # Åšrednia odlegÅ‚oÅ›Ä‡
    FAR_THRESHOLD: int = 150               # Daleko (>150 sÅ‚Ã³w = sÅ‚abe powiÄ…zanie)
    
    # Wymagania
    REQUIRE_ENTITY_PAIRS_SAME_SENTENCE: bool = True  # Wymuszaj pary encji w tym samym zdaniu
    REQUIRE_KEYWORD_CONTEXT: bool = True   # Wymuszaj kontekst dla keywords
    
    # Wagi dla proximity score
    SAME_SENTENCE_SCORE: float = 1.0
    CLOSE_SCORE: float = 0.7
    MEDIUM_SCORE: float = 0.4
    FAR_SCORE: float = 0.1

CONFIG = ProximityConfig()

# ================================================================
# TEXT ANALYSIS HELPERS
# ================================================================

def split_into_sentences(text: str) -> List[str]:
    """Dzieli tekst na zdania."""
    # Pattern dla polskich zdaÅ„
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»])', text)
    return [s.strip() for s in sentences if s.strip()]

def split_into_paragraphs(text: str) -> List[str]:
    """Dzieli tekst na akapity."""
    paragraphs = re.split(r'\n\s*\n', text)
    return [p.strip() for p in paragraphs if p.strip()]

def find_positions(text: str, phrase: str) -> List[int]:
    """
    Znajduje pozycje (w sÅ‚owach) wszystkich wystÄ…pieÅ„ frazy.
    
    Returns:
        Lista pozycji (indeks sÅ‚owa gdzie zaczyna siÄ™ fraza)
    """
    if not text or not phrase:
        return []
    
    text_lower = text.lower()
    phrase_lower = phrase.lower()
    words = text_lower.split()
    phrase_words = phrase_lower.split()
    
    positions = []
    phrase_len = len(phrase_words)
    
    for i in range(len(words) - phrase_len + 1):
        if words[i:i + phrase_len] == phrase_words:
            positions.append(i)
    
    # Fallback: szukaj jako substring i konwertuj na pozycjÄ™ sÅ‚owa
    if not positions:
        start = 0
        while True:
            pos = text_lower.find(phrase_lower, start)
            if pos == -1:
                break
            # Konwertuj pozycjÄ™ znaku na pozycjÄ™ sÅ‚owa
            word_pos = len(text_lower[:pos].split())
            positions.append(word_pos)
            start = pos + len(phrase_lower)
    
    return positions

def get_sentence_containing_position(text: str, word_position: int) -> Tuple[int, str]:
    """
    Znajduje zdanie zawierajÄ…ce danÄ… pozycjÄ™ sÅ‚owa.
    
    Returns:
        (sentence_index, sentence_text)
    """
    sentences = split_into_sentences(text)
    words = text.split()
    
    current_word_idx = 0
    for sent_idx, sentence in enumerate(sentences):
        sent_words = len(sentence.split())
        if current_word_idx <= word_position < current_word_idx + sent_words:
            return sent_idx, sentence
        current_word_idx += sent_words
    
    return -1, ""

# ================================================================
# PROXIMITY CALCULATION
# ================================================================

@dataclass
class ProximityResult:
    """Wynik analizy proximity."""
    entity1: str
    entity2: str
    distance_words: int
    same_sentence: bool
    same_paragraph: bool
    proximity_score: float  # 0.0 - 1.0
    status: str  # EXCELLENT, GOOD, FAIR, POOR
    positions: Dict[str, List[int]]

def calculate_proximity(
    text: str,
    phrase1: str,
    phrase2: str
) -> ProximityResult:
    """
    Oblicza proximity miÄ™dzy dwoma frazami.
    
    Args:
        text: Tekst do analizy
        phrase1: Pierwsza fraza
        phrase2: Druga fraza
        
    Returns:
        ProximityResult
    """
    pos1 = find_positions(text, phrase1)
    pos2 = find_positions(text, phrase2)
    
    if not pos1 or not pos2:
        return ProximityResult(
            entity1=phrase1,
            entity2=phrase2,
            distance_words=-1,
            same_sentence=False,
            same_paragraph=False,
            proximity_score=0.0,
            status="NOT_FOUND",
            positions={"phrase1": pos1, "phrase2": pos2}
        )
    
    # ZnajdÅº minimalnÄ… odlegÅ‚oÅ›Ä‡
    min_distance = float('inf')
    best_pos1 = pos1[0]
    best_pos2 = pos2[0]
    
    for p1 in pos1:
        for p2 in pos2:
            dist = abs(p1 - p2)
            if dist < min_distance:
                min_distance = dist
                best_pos1 = p1
                best_pos2 = p2
    
    # SprawdÅº czy w tym samym zdaniu
    sent1_idx, sent1 = get_sentence_containing_position(text, best_pos1)
    sent2_idx, sent2 = get_sentence_containing_position(text, best_pos2)
    same_sentence = (sent1_idx == sent2_idx and sent1_idx >= 0)
    
    # SprawdÅº czy w tym samym akapicie
    paragraphs = split_into_paragraphs(text)
    para1_idx = -1
    para2_idx = -1
    word_count = 0
    
    for para_idx, para in enumerate(paragraphs):
        para_words = len(para.split())
        if word_count <= best_pos1 < word_count + para_words:
            para1_idx = para_idx
        if word_count <= best_pos2 < word_count + para_words:
            para2_idx = para_idx
        word_count += para_words
    
    same_paragraph = (para1_idx == para2_idx and para1_idx >= 0)
    
    # Oblicz proximity score
    if same_sentence:
        score = CONFIG.SAME_SENTENCE_SCORE
        status = "EXCELLENT"
    elif min_distance <= CONFIG.CLOSE_PROXIMITY_THRESHOLD:
        score = CONFIG.CLOSE_SCORE
        status = "GOOD"
    elif min_distance <= CONFIG.MEDIUM_PROXIMITY_THRESHOLD:
        score = CONFIG.MEDIUM_SCORE
        status = "FAIR"
    else:
        score = CONFIG.FAR_SCORE
        status = "POOR"
    
    return ProximityResult(
        entity1=phrase1,
        entity2=phrase2,
        distance_words=min_distance,
        same_sentence=same_sentence,
        same_paragraph=same_paragraph,
        proximity_score=score,
        status=status,
        positions={"phrase1": pos1, "phrase2": pos2}
    )

# ================================================================
# ENTITY PAIRS ANALYSIS
# ================================================================

# Pary encji ktÃ³re powinny wystÄ™powaÄ‡ blisko siebie
REQUIRED_ENTITY_PAIRS = {
    # Prawo
    ("sÄ…d okrÄ™gowy", "wydziaÅ‚ cywilny"): "legal_court",
    ("sÄ…d rejonowy", "wydziaÅ‚ rodzinny"): "legal_court",
    ("kodeks cywilny", "art."): "legal_reference",
    ("kodeks karny", "art."): "legal_reference",
    ("wniosek", "sÄ…d"): "legal_procedure",
    
    # Medycyna
    ("choroba psychiczna", "biegÅ‚y"): "medical_expert",
    ("opinia", "psychiatra"): "medical_expert",
    ("badanie", "lekarz"): "medical_exam",
    
    # Finanse
    ("podatek", "urzÄ…d skarbowy"): "tax_authority",
    ("pit", "zeznanie"): "tax_form",
}

def analyze_entity_pairs(
    text: str,
    entities: List[str],
    custom_pairs: Optional[Dict[Tuple[str, str], str]] = None
) -> Dict[str, Any]:
    """
    Analizuje proximity dla par encji.
    
    Args:
        text: Tekst do analizy
        entities: Lista encji do sprawdzenia
        custom_pairs: Dodatkowe wymagane pary
        
    Returns:
        Analiza par encji
    """
    pairs_to_check = REQUIRED_ENTITY_PAIRS.copy()
    if custom_pairs:
        pairs_to_check.update(custom_pairs)
    
    results = []
    issues = []
    
    text_lower = text.lower()
    entities_lower = [e.lower() for e in entities]
    
    for (e1, e2), pair_type in pairs_to_check.items():
        e1_lower = e1.lower()
        e2_lower = e2.lower()
        
        # SprawdÅº czy obie encje sÄ… w tekÅ›cie
        e1_present = e1_lower in text_lower or any(e1_lower in ent for ent in entities_lower)
        e2_present = e2_lower in text_lower or any(e2_lower in ent for ent in entities_lower)
        
        if e1_present and e2_present:
            proximity = calculate_proximity(text, e1, e2)
            results.append({
                "pair": (e1, e2),
                "type": pair_type,
                "proximity": proximity.proximity_score,
                "distance": proximity.distance_words,
                "same_sentence": proximity.same_sentence,
                "status": proximity.status
            })
            
            # Dodaj issue jeÅ›li proximity jest sÅ‚abe
            if proximity.status in ["FAIR", "POOR"]:
                issues.append({
                    "type": "WEAK_ENTITY_PROXIMITY",
                    "entity1": e1,
                    "entity2": e2,
                    "distance": proximity.distance_words,
                    "recommendation": f"UmieÅ›Ä‡ '{e1}' i '{e2}' bliÅ¼ej siebie (najlepiej w tym samym zdaniu)"
                })
    
    return {
        "pairs_checked": len(results),
        "pairs_found": results,
        "issues": issues,
        "avg_proximity_score": sum(r["proximity"] for r in results) / len(results) if results else 0
    }

# ================================================================
# KEYWORD CONTEXT VALIDATION
# ================================================================

def validate_keyword_context(
    text: str,
    keyword: str,
    required_context_words: List[str],
    context_window: int = 50
) -> Dict[str, Any]:
    """
    Sprawdza czy keyword wystÄ™puje w odpowiednim kontekÅ›cie.
    
    Args:
        text: Tekst do analizy
        keyword: SÅ‚owo kluczowe
        required_context_words: SÅ‚owa ktÃ³re powinny byÄ‡ w pobliÅ¼u
        context_window: Okno kontekstowe (w sÅ‚owach)
        
    Returns:
        Wynik walidacji kontekstu
    """
    keyword_positions = find_positions(text, keyword)
    
    if not keyword_positions:
        return {
            "keyword": keyword,
            "found": False,
            "context_valid": False,
            "missing_context": required_context_words
        }
    
    words = text.lower().split()
    found_context = set()
    missing_context = set(w.lower() for w in required_context_words)
    
    for kw_pos in keyword_positions:
        # SprawdÅº okno kontekstowe
        start = max(0, kw_pos - context_window)
        end = min(len(words), kw_pos + context_window)
        context_words = set(words[start:end])
        
        for ctx_word in required_context_words:
            if ctx_word.lower() in context_words:
                found_context.add(ctx_word.lower())
                missing_context.discard(ctx_word.lower())
    
    return {
        "keyword": keyword,
        "found": True,
        "occurrences": len(keyword_positions),
        "context_valid": len(missing_context) == 0,
        "found_context": list(found_context),
        "missing_context": list(missing_context),
        "context_window": context_window
    }

# ================================================================
# PROXIMITY ENFORCEMENT FOR BATCHES
# ================================================================

def enforce_proximity_requirements(
    batch_text: str,
    entities: List[str],
    keywords: List[str],
    detected_category: str = "general"
) -> Dict[str, Any]:
    """
    Wymusza wymagania proximity dla batcha.
    
    Args:
        batch_text: Tekst batcha
        entities: Encje w batchu
        keywords: Keywords w batchu
        detected_category: Kategoria tematyczna
        
    Returns:
        Wyniki enforcement z issues i recommendations
    """
    results = {
        "entity_pairs": analyze_entity_pairs(batch_text, entities),
        "proximity_issues": [],
        "recommendations": [],
        "overall_score": 0.0
    }
    
    # Zbierz issues z entity pairs
    results["proximity_issues"].extend(results["entity_pairs"]["issues"])
    
    # Dodatkowe sprawdzenia dla kategorii prawnej
    if detected_category == "prawo":
        # SprawdÅº czy artykuÅ‚y kodeksu sÄ… blisko nazwy kodeksu
        legal_context = validate_keyword_context(
            batch_text,
            "art.",
            ["kodeks", "k.c.", "k.k.", "k.p.c.", "ustawa"],
            context_window=20
        )
        
        if legal_context["found"] and not legal_context["context_valid"]:
            results["proximity_issues"].append({
                "type": "LEGAL_CONTEXT_MISSING",
                "keyword": "art.",
                "missing": legal_context["missing_context"],
                "recommendation": "ArtykuÅ‚y powinny mieÄ‡ odniesienie do konkretnego kodeksu/ustawy w tym samym zdaniu"
            })
    
    # Generuj recommendations
    for issue in results["proximity_issues"]:
        if "recommendation" in issue:
            results["recommendations"].append(issue["recommendation"])
    
    # Oblicz overall score
    entity_score = results["entity_pairs"]["avg_proximity_score"]
    issues_penalty = len(results["proximity_issues"]) * 0.1
    results["overall_score"] = max(0, min(1.0, entity_score - issues_penalty))
    
    return results

# ================================================================
# PROXIMITY SUGGESTIONS FOR GPT PROMPT
# ================================================================

def generate_proximity_instructions(
    entities: List[str],
    keywords: List[str],
    detected_category: str = "general"
) -> List[str]:
    """
    Generuje instrukcje proximity dla GPT.
    
    Args:
        entities: Encje do uÅ¼ycia
        keywords: Keywords do uÅ¼ycia
        detected_category: Kategoria tematyczna
        
    Returns:
        Lista instrukcji dla GPT
    """
    instructions = []
    
    # Instrukcje ogÃ³lne
    instructions.append("ğŸ“ PROXIMITY - BliskoÅ›Ä‡ fraz:")
    
    # ZnajdÅº pary encji ktÃ³re powinny byÄ‡ blisko
    entities_lower = [e.lower() for e in entities]
    
    pairs_found = []
    for (e1, e2), pair_type in REQUIRED_ENTITY_PAIRS.items():
        e1_match = any(e1.lower() in ent for ent in entities_lower)
        e2_match = any(e2.lower() in ent for ent in entities_lower)
        
        if e1_match or e2_match:
            # ZnajdÅº peÅ‚ne nazwy encji
            e1_full = next((e for e in entities if e1.lower() in e.lower()), e1)
            e2_full = next((e for e in entities if e2.lower() in e.lower()), e2)
            pairs_found.append((e1_full, e2_full, pair_type))
    
    if pairs_found:
        for e1, e2, ptype in pairs_found[:5]:  # Max 5 par
            instructions.append(f"   â€¢ UmieÅ›Ä‡ '{e1}' i '{e2}' w tym samym zdaniu")
    
    # Instrukcje dla kategorii prawnej
    if detected_category == "prawo":
        instructions.append("   â€¢ CytujÄ…c artykuÅ‚, ZAWSZE podaj ÅºrÃ³dÅ‚o (np. 'art. 13 k.c.')")
        instructions.append("   â€¢ Nazwy sÄ…dÃ³w pisz peÅ‚ne (np. 'SÄ…d OkrÄ™gowy w Warszawie')")
    
    # Instrukcje dla kategorii medycznej
    if detected_category == "medycyna":
        instructions.append("   â€¢ Terminy medyczne wyjaÅ›niaj w nawiasie przy pierwszym uÅ¼yciu")
        instructions.append("   â€¢ Opinie biegÅ‚ych Å‚Ä…cz z ich specjalizacjÄ…")
    
    return instructions

# ================================================================
# TESTING
# ================================================================

def test_proximity_checker():
    """Test proximity checker."""
    print("="*60)
    print("PROXIMITY CHECKER TEST")
    print("="*60)
    
    test_text = """
    Wniosek o ubezwÅ‚asnowolnienie skÅ‚ada siÄ™ do SÄ…du OkrÄ™gowego.
    WydziaÅ‚ Cywilny rozpatruje takie sprawy w trybie nieprocesowym.
    
    SÄ…d powoÅ‚uje biegÅ‚ego psychiatrÄ™ do wydania opinii.
    Choroba psychiczna musi byÄ‡ potwierdzona badaniem.
    
    Zgodnie z art. 13 Kodeksu cywilnego, osoba ubezwÅ‚asnowolniona caÅ‚kowicie
    nie ma zdolnoÅ›ci do czynnoÅ›ci prawnych.
    """
    
    print("\n1. Proximity between phrases:")
    result = calculate_proximity(test_text, "SÄ…d OkrÄ™gowy", "WydziaÅ‚ Cywilny")
    print(f"   'SÄ…d OkrÄ™gowy' <-> 'WydziaÅ‚ Cywilny':")
    print(f"      Distance: {result.distance_words} words")
    print(f"      Same sentence: {result.same_sentence}")
    print(f"      Score: {result.proximity_score}")
    print(f"      Status: {result.status}")
    
    print("\n2. Entity pairs analysis:")
    entities = ["SÄ…d OkrÄ™gowy", "WydziaÅ‚ Cywilny", "biegÅ‚y psychiatra", "choroba psychiczna"]
    analysis = analyze_entity_pairs(test_text, entities)
    print(f"   Pairs found: {analysis['pairs_checked']}")
    print(f"   Issues: {len(analysis['issues'])}")
    for issue in analysis["issues"]:
        print(f"      - {issue['type']}: {issue['entity1']} <-> {issue['entity2']}")
    
    print("\n3. Legal context validation:")
    ctx = validate_keyword_context(test_text, "art.", ["kodeks", "k.c."], context_window=10)
    print(f"   'art.' context valid: {ctx['context_valid']}")
    print(f"   Found context: {ctx['found_context']}")
    
    print("\n4. GPT instructions:")
    instructions = generate_proximity_instructions(entities, [], "prawo")
    for instr in instructions:
        print(f"   {instr}")
    
    print("\n" + "="*60)

if __name__ == "__main__":
    test_proximity_checker()
