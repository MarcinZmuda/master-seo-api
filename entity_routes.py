"""
===============================================================================
üß† ENTITY ROUTES v29.3 - Entity Validation & Synonym API
===============================================================================
Endpointy dla Entity SEO:
- POST /api/project/{id}/validate_entities - walidacja u≈ºycia encji
- GET /api/synonyms/{phrase} - synonimy dla fraz (LSI)
===============================================================================
"""

from flask import Blueprint, request, jsonify
import re
from typing import Dict, List, Any, Optional, Tuple
from collections import Counter
import json
import os

entity_routes = Blueprint('entity_routes', __name__)

# ================================================================
# üìö S≈ÅOWNIK SYNONIM√ìW (LSI)
# ================================================================

SYNONYM_DATABASE = {
    # Pomoce sensoryczne
    "pomoce sensoryczne": [
        "narzƒôdzia terapeutyczne",
        "sprzƒôt SI",
        "akcesoria sensoryczne",
        "materia≈Çy do stymulacji zmys≈Ç√≥w",
        "pomoce do terapii",
        "zestawy sensoryczne",
        "sprzƒôt terapeutyczny"
    ],
    "integracja sensoryczna": [
        "SI",
        "terapia integracji sensorycznej",
        "przetwarzanie sensoryczne",
        "stymulacja zmys≈Ç√≥w",
        "terapia SI",
        "integracja zmys≈Çowa"
    ],
    "≈õcie≈ºka sensoryczna": [
        "tor sensoryczny",
        "≈õcie≈ºka dotykowa",
        "≈õcie≈ºka zmys≈Çowa",
        "tor dotykowy",
        "mata sensoryczna"
    ],
    "dziecko": [
        "maluch",
        "przedszkolak",
        "najm≈Çodsi",
        "pociecha",
        "wychowanek",
        "dziecko przedszkolne",
        "brzdƒÖc"
    ],
    "rozw√≥j": [
        "postƒôp",
        "doskonalenie",
        "kszta≈Çtowanie",
        "wzrost",
        "dojrzewanie",
        "progres"
    ],
    "przedszkole": [
        "plac√≥wka przedszkolna",
        "o≈õrodek przedszkolny",
        "grupa przedszkolna",
        "zer√≥wka",
        "plac√≥wka edukacyjna"
    ],
    "terapia": [
        "zajƒôcia terapeutyczne",
        "sesja terapeutyczna",
        "leczenie",
        "rehabilitacja",
        "wsparcie terapeutyczne"
    ],
    "zmys≈Çy": [
        "percepcja",
        "odczuwanie",
        "postrzeganie",
        "receptory",
        "uk≈Çad sensoryczny"
    ],
    # Og√≥lne SEO
    "wa≈ºny": [
        "istotny",
        "znaczƒÖcy",
        "kluczowy",
        "fundamentalny",
        "decydujƒÖcy"
    ],
    "skuteczny": [
        "efektywny",
        "dzia≈ÇajƒÖcy",
        "sprawdzony",
        "wydajny",
        "rezultatywny"
    ],
    "metoda": [
        "spos√≥b",
        "technika",
        "podej≈õcie",
        "strategia",
        "system"
    ],
    "problem": [
        "trudno≈õƒá",
        "wyzwanie",
        "kwestia",
        "zagadnienie",
        "k≈Çopot"
    ]
}

# LSI Keywords dla popularnych temat√≥w
LSI_DATABASE = {
    "integracja sensoryczna": [
        "przetwarzanie bod≈∫c√≥w",
        "stymulacja zmys≈Ç√≥w",
        "uk≈Çad nerwowy",
        "koordynacja ruchowa",
        "motoryka ma≈Ça",
        "motoryka du≈ºa",
        "r√≥wnowaga",
        "percepcja",
        "zmys≈Ç dotyku",
        "zmys≈Ç r√≥wnowagi",
        "propriocepcja",
        "uk≈Çad przedsionkowy"
    ],
    "pomoce sensoryczne": [
        "≈õcie≈ºka sensoryczna",
        "pi≈Çki sensoryczne",
        "maty dotykowe",
        "panele sensoryczne",
        "hu≈õtawki terapeutyczne",
        "dyski balansowe",
        "faktury",
        "tekstury"
    ],
    "przedszkole": [
        "edukacja przedszkolna",
        "nauczyciel przedszkolny",
        "grupa wiekowa",
        "zajƒôcia przedszkolne",
        "adaptacja",
        "socjalizacja"
    ],
    "montessori": [
        "samodzielno≈õƒá dziecka",
        "przygotowane otoczenie",
        "nauka przez dzia≈Çanie",
        "materia≈Çy montessori",
        "pedagogika montessori"
    ]
}

# ================================================================
# üîç KNOWN ENTITIES DATABASE
# ================================================================

KNOWN_ENTITIES = {
    # Osoby
    "jean ayres": {
        "canonical": "Anna Jean Ayres",
        "type": "PERSON",
        "aliases": ["jean ayres", "a. jean ayres", "dr ayres", "ayres"],
        "definition_hint": "ameryka≈Ñska terapeutka zajƒôciowa i psycholog, tw√≥rczyni teorii integracji sensorycznej"
    },
    "maria montessori": {
        "canonical": "Maria Montessori",
        "type": "PERSON",
        "aliases": ["montessori", "maria montessori"],
        "definition_hint": "w≈Çoska lekarka i pedagog, tw√≥rczyni metody Montessori"
    },
    # Koncepcje
    "integracja sensoryczna": {
        "canonical": "integracja sensoryczna",
        "type": "CONCEPT",
        "aliases": ["si", "integracja zmys≈Çowa", "terapia si"],
        "definition_hint": "proces neurologiczny organizowania bod≈∫c√≥w zmys≈Çowych"
    },
    "propriocepcja": {
        "canonical": "propriocepcja",
        "type": "CONCEPT",
        "aliases": ["zmys≈Ç proprioceptywny", "czucie g≈Çƒôbokie"],
        "definition_hint": "zmys≈Ç informujƒÖcy m√≥zg o po≈Ço≈ºeniu cia≈Ça w przestrzeni"
    },
    "uk≈Çad przedsionkowy": {
        "canonical": "uk≈Çad przedsionkowy",
        "type": "CONCEPT",
        "aliases": ["zmys≈Ç r√≥wnowagi", "uk≈Çad b≈Çƒôdnikowy"],
        "definition_hint": "czƒô≈õƒá ucha wewnƒôtrznego odpowiedzialna za r√≥wnowagƒô"
    }
}


# ================================================================
# üîß HELPER FUNCTIONS
# ================================================================

def normalize_text(text: str) -> str:
    """Normalizuje tekst do por√≥wna≈Ñ."""
    if not text:
        return ""
    text = text.lower()
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def find_entity_in_text(text: str, entity_name: str, aliases: List[str] = None) -> Tuple[int, bool]:
    """
    Znajduje encjƒô w tek≈õcie.
    Zwraca: (count, is_defined)
    """
    text_normalized = normalize_text(text)
    text_lower = text.lower()
    
    names_to_check = [entity_name.lower()]
    if aliases:
        names_to_check.extend([a.lower() for a in aliases])
    
    total_count = 0
    for name in names_to_check:
        # Szukaj jako ca≈Çe s≈Çowa
        pattern = r'\b' + re.escape(normalize_text(name)) + r'\b'
        matches = re.findall(pattern, text_normalized)
        total_count += len(matches)
    
    # Sprawd≈∫ czy jest definicja (heurystyka)
    is_defined = False
    definition_patterns = [
        r'to\s+\w+',  # "X to ..."
        r'czyli\s+',  # "X, czyli ..."
        r'jest\s+to',  # "X jest to ..."
        r'polega\s+na',  # "X polega na ..."
        r'opracowa≈Ç',  # "X opracowa≈Ç/a"
        r'stworzy[≈Çl]',  # "X stworzy≈Ç/a"
        r'[\(\)]',  # "X (skr√≥t od Y)"
    ]
    
    for name in names_to_check:
        for pattern in definition_patterns:
            full_pattern = re.escape(name) + r'[,\s]+' + pattern
            if re.search(full_pattern, text_lower):
                is_defined = True
                break
    
    return total_count, is_defined


def extract_relationships(text: str) -> List[str]:
    """WyciƒÖga relacje miƒôdzy encjami z tekstu."""
    relationships = []
    
    relationship_patterns = [
        (r'(\w+)\s+opracowa≈Ç[a]?\s+(\w+)', "{0} ‚Üí opracowa≈Ç ‚Üí {1}"),
        (r'(\w+)\s+stworzy[≈Çl][a]?\s+(\w+)', "{0} ‚Üí stworzy≈Ç ‚Üí {1}"),
        (r'(\w+)\s+wp≈Çywa\s+na\s+(\w+)', "{0} ‚Üí wp≈Çywa na ‚Üí {1}"),
        (r'(\w+)\s+prowadzi\s+do\s+(\w+)', "{0} ‚Üí prowadzi do ‚Üí {1}"),
        (r'(\w+)\s+wspiera\s+(\w+)', "{0} ‚Üí wspiera ‚Üí {1}"),
        (r'(\w+)\s+jest\s+czƒô≈õciƒÖ\s+(\w+)', "{0} ‚Üí jest czƒô≈õciƒÖ ‚Üí {1}"),
        (r'(\w+)\s+obejmuje\s+(\w+)', "{0} ‚Üí obejmuje ‚Üí {1}"),
    ]
    
    text_lower = text.lower()
    
    for pattern, template in relationship_patterns:
        matches = re.findall(pattern, text_lower)
        for match in matches:
            if len(match) >= 2:
                rel = template.format(match[0], match[1])
                relationships.append(rel)
    
    return relationships[:10]  # Max 10


def calculate_entity_score(
    entities_used: List[Dict],
    must_have: List[str],
    total_words: int
) -> int:
    """Oblicza score encji (0-100)."""
    score = 0
    
    # 1. Coverage of must_have (40 points)
    if must_have:
        covered = sum(1 for e in entities_used if e["name"].lower() in [m.lower() for m in must_have])
        coverage_pct = covered / len(must_have)
        score += int(coverage_pct * 40)
    else:
        score += 40  # Brak must_have = pe≈Çne punkty
    
    # 2. Entity density (30 points)
    unique_entities = len(entities_used)
    optimal_density = total_words / 100 * 3  # ~3 encje na 100 s≈Ç√≥w
    density_ratio = min(unique_entities / max(optimal_density, 1), 1.5)
    score += int(min(density_ratio, 1) * 30)
    
    # 3. Definitions (30 points)
    defined_count = sum(1 for e in entities_used if e.get("defined", False))
    if entities_used:
        definition_ratio = defined_count / len(entities_used)
        score += int(definition_ratio * 30)
    
    return min(score, 100)


# ================================================================
# üì° ENDPOINTS
# ================================================================

@entity_routes.post("/api/project/<project_id>/validate_entities")
def validate_entities(project_id):
    """
    Waliduje u≈ºycie encji w artykule.
    
    Request:
        {
            "text": "Tre≈õƒá artyku≈Çu..."
        }
    
    Response:
        {
            "entity_score": 75,
            "entities_used": [...],
            "entities_missing": [...],
            "entity_density": {...},
            "relationships_found": [...],
            "suggestions": [...]
        }
    """
    from master_api import get_project
    
    try:
        project = get_project(project_id)
        if not project:
            return jsonify({"error": "Project not found"}), 404
        
        data = request.get_json(force=True)
        text = data.get("text", "")
        
        if not text:
            # Spr√≥buj wziƒÖƒá z projektu
            batches = project.get("batches", [])
            text = "\n\n".join([b.get("text", "") for b in batches])
        
        if not text:
            return jsonify({"error": "No text to validate"}), 400
        
        # Pobierz must_have entities z S1
        s1_data = project.get("s1_data", {})
        entity_seo = s1_data.get("entity_seo", {})
        must_mention = entity_seo.get("must_mention_entities", [])
        top_entities_s1 = entity_seo.get("top_entities", [])
        
        # Je≈õli nie ma must_mention, we≈∫ top entities
        if not must_mention and top_entities_s1:
            must_mention = [e.get("name", "") for e in top_entities_s1[:5]]
        
        # Dodaj known entities dla tematu
        main_keyword = project.get("main_keyword", "").lower()
        if "sensorycz" in main_keyword or "integracja" in main_keyword:
            must_mention.extend(["jean ayres", "propriocepcja", "uk≈Çad przedsionkowy"])
        
        must_mention = list(set([m for m in must_mention if m]))
        
        # Analizuj tekst
        text_normalized = normalize_text(text)
        total_words = len(text.split())
        
        entities_used = []
        entities_missing = []
        
        # Sprawd≈∫ known entities
        for entity_key, entity_data in KNOWN_ENTITIES.items():
            count, is_defined = find_entity_in_text(
                text, 
                entity_data["canonical"],
                entity_data.get("aliases", [])
            )
            
            if count > 0:
                entities_used.append({
                    "name": entity_data["canonical"],
                    "type": entity_data["type"],
                    "count": count,
                    "defined": is_defined
                })
        
        # Sprawd≈∫ must_mention
        for entity_name in must_mention:
            found = False
            for used in entities_used:
                if entity_name.lower() in used["name"].lower():
                    found = True
                    break
            
            if not found:
                # Sprawd≈∫ bezpo≈õrednio w tek≈õcie
                count, is_defined = find_entity_in_text(text, entity_name)
                if count > 0:
                    entities_used.append({
                        "name": entity_name,
                        "type": "UNKNOWN",
                        "count": count,
                        "defined": is_defined
                    })
                else:
                    entities_missing.append(entity_name)
        
        # Znajd≈∫ relacje
        relationships = extract_relationships(text)
        
        # Oblicz score
        entity_score = calculate_entity_score(entities_used, must_mention, total_words)
        
        # Oblicz density
        unique_entities = len(entities_used)
        total_mentions = sum(e["count"] for e in entities_used)
        
        entity_density = {
            "total_entities": unique_entities,
            "total_mentions": total_mentions,
            "density_per_100_words": round(total_mentions / max(total_words, 1) * 100, 2),
            "unique_per_100_words": round(unique_entities / max(total_words, 1) * 100, 2)
        }
        
        # Generuj sugestie
        suggestions = []
        
        if entities_missing:
            suggestions.append(f"Dodaj brakujƒÖce encje: {', '.join(entities_missing[:3])}")
        
        undefined = [e["name"] for e in entities_used if not e.get("defined", False)]
        if undefined:
            suggestions.append(f"Zdefiniuj encje przy pierwszym u≈ºyciu: {', '.join(undefined[:3])}")
        
        if entity_density["density_per_100_words"] < 2:
            suggestions.append("Gƒôsto≈õƒá encji za niska - dodaj wiƒôcej odniesie≈Ñ do kluczowych pojƒôƒá")
        
        if not relationships:
            suggestions.append("Brak wykrytych relacji miƒôdzy encjami - po≈ÇƒÖcz je przyczynowo (X wp≈Çywa na Y)")
        
        return jsonify({
            "project_id": project_id,
            "entity_score": entity_score,
            "entities_used": sorted(entities_used, key=lambda x: x["count"], reverse=True),
            "entities_missing": entities_missing,
            "entity_density": entity_density,
            "relationships_found": relationships,
            "suggestions": suggestions,
            "must_mention_entities": must_mention,
            "total_words": total_words
        }), 200
        
    except Exception as e:
        print(f"[ENTITY_VALIDATE] ‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


@entity_routes.get("/api/synonyms/<path:phrase>")
def get_synonyms(phrase):
    """
    Zwraca synonimy dla frazy (LSI).
    
    Response:
        {
            "phrase": "pomoce sensoryczne",
            "synonyms": [...],
            "variants": [...],
            "lsi_related": [...]
        }
    """
    try:
        phrase_normalized = normalize_text(phrase)
        phrase_lower = phrase.lower().strip()
        
        # Szukaj dok≈Çadnego dopasowania
        synonyms = SYNONYM_DATABASE.get(phrase_lower, [])
        
        # Je≈õli nie znaleziono, szukaj czƒô≈õciowego dopasowania
        if not synonyms:
            for key, syns in SYNONYM_DATABASE.items():
                if phrase_lower in key or key in phrase_lower:
                    synonyms = syns
                    break
        
        # Warianty (odmiany)
        variants = []
        if phrase_lower.endswith("y"):
            variants.append(phrase_lower[:-1] + "a")
            variants.append(phrase_lower[:-1] + "ami")
        elif phrase_lower.endswith("a"):
            variants.append(phrase_lower[:-1] + "y")
            variants.append(phrase_lower[:-1] + "ƒÖ")
        
        # LSI related
        lsi_related = []
        for key, lsi_list in LSI_DATABASE.items():
            if phrase_lower in key or key in phrase_lower:
                lsi_related = lsi_list
                break
        
        # Je≈õli nie znaleziono nic, zwr√≥ƒá puste listy
        return jsonify({
            "phrase": phrase,
            "synonyms": synonyms[:10],
            "variants": variants[:5],
            "lsi_related": lsi_related[:10],
            "found": bool(synonyms or lsi_related)
        }), 200
        
    except Exception as e:
        print(f"[SYNONYMS] ‚ùå Error: {e}")
        return jsonify({
            "phrase": phrase,
            "synonyms": [],
            "variants": [],
            "lsi_related": [],
            "error": str(e)
        }), 200


@entity_routes.get("/api/entities/known")
def get_known_entities():
    """Zwraca listƒô znanych encji."""
    return jsonify({
        "entities": [
            {
                "name": data["canonical"],
                "type": data["type"],
                "aliases": data.get("aliases", [])
            }
            for key, data in KNOWN_ENTITIES.items()
        ],
        "total": len(KNOWN_ENTITIES)
    }), 200


@entity_routes.get("/api/synonyms/database")
def get_synonym_database():
    """Zwraca ca≈ÇƒÖ bazƒô synonim√≥w."""
    return jsonify({
        "synonyms": SYNONYM_DATABASE,
        "lsi": LSI_DATABASE,
        "total_phrases": len(SYNONYM_DATABASE),
        "total_lsi_topics": len(LSI_DATABASE)
    }), 200


# ================================================================
# üß™ TEST
# ================================================================

if __name__ == "__main__":
    # Test synonyms
    test_phrase = "pomoce sensoryczne"
    print(f"\nüîç Synonimy dla: {test_phrase}")
    syns = SYNONYM_DATABASE.get(test_phrase, [])
    print(f"   Synonimy: {syns}")
    
    # Test entity detection
    test_text = """
    Integracjƒô sensorycznƒÖ opracowa≈Ça dr Jean Ayres, ameryka≈Ñska terapeutka zajƒôciowa.
    Metoda Montessori, stworzona przez Mariƒô Montessori, wspiera samodzielno≈õƒá dziecka.
    Propriocepcja wp≈Çywa na r√≥wnowagƒô i koordynacjƒô ruchowƒÖ.
    """
    
    print(f"\nüîç Test entity detection:")
    for entity_key, entity_data in KNOWN_ENTITIES.items():
        count, defined = find_entity_in_text(test_text, entity_data["canonical"], entity_data.get("aliases"))
        if count > 0:
            print(f"   ‚úÖ {entity_data['canonical']}: {count}x, defined={defined}")
    
    print(f"\nüîç Test relationships:")
    rels = extract_relationships(test_text)
    for rel in rels:
        print(f"   ‚Üí {rel}")
