"""
===============================================================================
ðŸ¤– AI DETECTION METRICS v32.0
===============================================================================
ModuÅ‚ do wykrywania tekstu wygenerowanego przez AI.

Metryki:
- Burstiness (zmiennoÅ›Ä‡ dÅ‚ugoÅ›ci zdaÅ„)
- Vocabulary Richness (TTR - Type-Token Ratio)
- Lexical Sophistication (Å›rednia czÄ™stoÅ›Ä‡ sÅ‚Ã³w - Zipf)
- Starter Entropy (rÃ³Å¼norodnoÅ›Ä‡ poczÄ…tkÃ³w zdaÅ„)
- Word Repetition (powtÃ³rzenia sÅ‚Ã³w)

Humanness Score = Å›rednia waÅ¼ona wszystkich metryk (0-100)

CRITICAL validations:
- Forbidden phrases check
- JITTER validation
- Triplets validation
===============================================================================
"""

import re
import math
import statistics
from collections import Counter
from typing import Dict, List, Any, Tuple
from enum import Enum

# ================================================================
# ðŸ“¦ Opcjonalny import wordfreq
# ================================================================
try:
    from wordfreq import zipf_frequency
    WORDFREQ_AVAILABLE = True
    print("[AI_DETECTION] âœ… wordfreq available")
except ImportError:
    WORDFREQ_AVAILABLE = False
    print("[AI_DETECTION] âš ï¸ wordfreq not available - lexical sophistication disabled")


# ================================================================
# ðŸ“Š KONFIGURACJA
# ================================================================
class AIDetectionConfig:
    """Progi dla metryk AI detection."""
    
    # Burstiness (zmiennoÅ›Ä‡ dÅ‚ugoÅ›ci zdaÅ„)
    BURSTINESS_CRITICAL_LOW = 2.0
    BURSTINESS_WARNING_LOW = 2.8
    BURSTINESS_OK_MIN = 2.8
    BURSTINESS_OK_MAX = 4.2
    BURSTINESS_WARNING_HIGH = 4.8
    BURSTINESS_CRITICAL_HIGH = 4.8
    
    # Vocabulary Richness (TTR)
    TTR_CRITICAL = 0.40
    TTR_WARNING = 0.48
    TTR_OK = 0.55
    
    # Lexical Sophistication (Zipf)
    ZIPF_CRITICAL = 5.5
    ZIPF_WARNING = 5.0
    ZIPF_OK = 4.5
    
    # Starter Entropy
    ENTROPY_CRITICAL = 0.50
    ENTROPY_WARNING = 0.65
    ENTROPY_OK = 0.75
    
    # Word Repetition
    REPETITION_OK = 5
    REPETITION_WARNING = 7
    REPETITION_CRITICAL = 8
    
    # Humanness Score
    HUMANNESS_CRITICAL = 50
    HUMANNESS_WARNING = 70
    
    # Wagi
    WEIGHTS = {
        "burstiness": 0.25,
        "vocabulary": 0.20,
        "sophistication": 0.15,
        "entropy": 0.20,
        "repetition": 0.20
    }


class Severity(Enum):
    OK = "OK"
    WARNING = "WARNING"
    CRITICAL = "CRITICAL"


# ================================================================
# ðŸ‡µðŸ‡± POLSKIE STOP WORDS
# ================================================================
POLISH_STOP_WORDS = {
    "i", "w", "na", "z", "do", "Å¼e", "siÄ™", "nie", "to", "o", "jak", 
    "ale", "co", "jest", "za", "po", "tak", "czy", "juÅ¼", "od", "przez",
    "dla", "by", "byÄ‡", "a", "wiÄ™c", "teÅ¼", "tylko", "lub", "oraz",
    "jego", "jej", "ich", "tym", "tego", "tej", "te", "ta", "ten",
    "ktÃ³ry", "ktÃ³ra", "ktÃ³re", "ktÃ³rych", "ktÃ³rzy", "ktÃ³rej",
    "moÅ¼e", "bardzo", "kiedy", "gdy", "tu", "tam", "teraz", "wtedy",
    "mnie", "mi", "ci", "ciÄ™", "go", "mu", "jÄ…", "je", "nas", "was", "im",
    "jednak", "jeszcze", "bÄ™dzie", "byÅ‚y", "byÅ‚", "byÅ‚a", "byÅ‚o",
    "sÄ…", "bÄ™dÄ…", "majÄ…", "ma", "moÅ¼na", "trzeba", "naleÅ¼y"
}


# ================================================================
# ðŸ”§ FUNKCJE POMOCNICZE
# ================================================================
def split_into_sentences(text: str) -> List[str]:
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»])', text)
    sentences = [s.strip() for s in sentences if len(s.strip().split()) >= 3]
    return sentences


def tokenize(text: str) -> List[str]:
    text = re.sub(r'<[^>]+>', ' ', text)
    text = text.lower()
    words = re.findall(r'\b[a-zÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼A-ZÄ„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»]+\b', text)
    return words


def tokenize_no_stopwords(text: str) -> List[str]:
    words = tokenize(text)
    return [w for w in words if w not in POLISH_STOP_WORDS]


# ================================================================
# ðŸ“Š METRYKI
# ================================================================
def calculate_burstiness(text: str) -> Dict[str, Any]:
    sentences = split_into_sentences(text)
    
    if len(sentences) < 5:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Za maÅ‚o zdaÅ„ do analizy (min 5)",
            "sentence_count": len(sentences)
        }
    
    lengths = [len(s.split()) for s in sentences]
    mean_len = statistics.mean(lengths)
    std_len = statistics.stdev(lengths) if len(lengths) > 1 else 0
    
    burstiness = (std_len / mean_len * 5) if mean_len > 0 else 0
    burstiness = round(burstiness, 2)
    
    config = AIDetectionConfig()
    if burstiness < config.BURSTINESS_CRITICAL_LOW:
        status = Severity.CRITICAL
        message = f"Tekst monotonny (burstiness {burstiness} < {config.BURSTINESS_CRITICAL_LOW}). Dodaj krÃ³tkie zdania 5-8 sÅ‚Ã³w."
    elif burstiness < config.BURSTINESS_WARNING_LOW:
        status = Severity.WARNING
        message = f"Niska zmiennoÅ›Ä‡ zdaÅ„. Dodaj wiÄ™cej krÃ³tkich zdaÅ„."
    elif burstiness > config.BURSTINESS_CRITICAL_HIGH:
        status = Severity.CRITICAL
        message = f"Tekst chaotyczny (burstiness {burstiness} > {config.BURSTINESS_CRITICAL_HIGH}). WyrÃ³wnaj rytm."
    elif burstiness > config.BURSTINESS_OK_MAX:
        status = Severity.WARNING
        message = f"Za duÅ¼a zmiennoÅ›Ä‡. WyrÃ³wnaj dÅ‚ugoÅ›ci zdaÅ„."
    else:
        status = Severity.OK
        message = "Burstiness w normie"
    
    return {
        "value": burstiness,
        "status": status.value,
        "message": message,
        "sentence_count": len(sentences),
        "mean_length": round(mean_len, 1),
        "std_length": round(std_len, 1),
        "min_length": min(lengths),
        "max_length": max(lengths)
    }


def calculate_vocabulary_richness(text: str) -> Dict[str, Any]:
    words = tokenize_no_stopwords(text)
    
    if len(words) < 50:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Za maÅ‚o sÅ‚Ã³w do analizy (min 50)",
            "word_count": len(words)
        }
    
    unique_words = set(words)
    ttr = len(unique_words) / len(words)
    ttr = round(ttr, 3)
    
    config = AIDetectionConfig()
    if ttr < config.TTR_CRITICAL:
        status = Severity.CRITICAL
        message = f"Bardzo ubogi zasÃ³b sÅ‚Ã³w (TTR {ttr} < {config.TTR_CRITICAL})"
    elif ttr < config.TTR_WARNING:
        status = Severity.WARNING
        message = f"MaÅ‚o urozmaicone sÅ‚ownictwo. UÅ¼yj synonimÃ³w."
    elif ttr >= config.TTR_OK:
        status = Severity.OK
        message = "Bogate sÅ‚ownictwo"
    else:
        status = Severity.WARNING
        message = "SÅ‚ownictwo poniÅ¼ej optimum"
    
    return {
        "value": ttr,
        "status": status.value,
        "message": message,
        "unique_words": len(unique_words),
        "total_words": len(words)
    }


def calculate_lexical_sophistication(text: str) -> Dict[str, Any]:
    if not WORDFREQ_AVAILABLE:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "wordfreq niedostÄ™pny",
            "available": False
        }
    
    words = tokenize_no_stopwords(text)
    
    if len(words) < 50:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Za maÅ‚o sÅ‚Ã³w do analizy",
            "available": True
        }
    
    zipf_scores = []
    for word in words:
        freq = zipf_frequency(word, 'pl')
        if freq > 0:
            zipf_scores.append(freq)
    
    if not zipf_scores:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Nie udaÅ‚o siÄ™ obliczyÄ‡ czÄ™stoÅ›ci sÅ‚Ã³w",
            "available": True
        }
    
    avg_zipf = statistics.mean(zipf_scores)
    avg_zipf = round(avg_zipf, 2)
    
    config = AIDetectionConfig()
    if avg_zipf > config.ZIPF_CRITICAL:
        status = Severity.CRITICAL
        message = f"Zbyt proste sÅ‚ownictwo (avg Zipf {avg_zipf} > {config.ZIPF_CRITICAL})"
    elif avg_zipf > config.ZIPF_WARNING:
        status = Severity.WARNING
        message = f"SÅ‚ownictwo doÅ›Ä‡ podstawowe"
    elif avg_zipf <= config.ZIPF_OK:
        status = Severity.OK
        message = "Dobry mix sÅ‚ownictwa"
    else:
        status = Severity.WARNING
        message = "SÅ‚ownictwo w normie"
    
    return {
        "value": avg_zipf,
        "status": status.value,
        "message": message,
        "words_analyzed": len(zipf_scores),
        "available": True
    }


def calculate_starter_entropy(text: str) -> Dict[str, Any]:
    sentences = split_into_sentences(text)
    
    if len(sentences) < 5:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Za maÅ‚o zdaÅ„ do analizy",
            "sentence_count": len(sentences)
        }
    
    starters = []
    for s in sentences:
        words = s.split()[:3]
        if words:
            starter = ' '.join(words).lower()
            starters.append(starter)
    
    if not starters:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Nie znaleziono starterÃ³w"
        }
    
    counter = Counter(starters)
    total = len(starters)
    
    entropy = 0
    for count in counter.values():
        p = count / total
        if p > 0:
            entropy -= p * math.log2(p)
    
    max_entropy = math.log2(total) if total > 1 else 1
    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
    normalized_entropy = round(normalized_entropy, 2)
    
    repetitive = {k: v for k, v in counter.items() if v >= 2}
    
    config = AIDetectionConfig()
    if normalized_entropy < config.ENTROPY_CRITICAL:
        status = Severity.CRITICAL
        message = f"Bardzo powtarzalne poczÄ…tki zdaÅ„ (entropy {normalized_entropy})"
    elif normalized_entropy < config.ENTROPY_WARNING:
        status = Severity.WARNING
        message = f"MaÅ‚o rÃ³Å¼norodne poczÄ…tki zdaÅ„"
    elif normalized_entropy >= config.ENTROPY_OK:
        status = Severity.OK
        message = "Dobra rÃ³Å¼norodnoÅ›Ä‡ poczÄ…tkÃ³w zdaÅ„"
    else:
        status = Severity.WARNING
        message = "RÃ³Å¼norodnoÅ›Ä‡ starterÃ³w poniÅ¼ej optimum"
    
    suggestions = []
    for starter, count in sorted(repetitive.items(), key=lambda x: -x[1])[:3]:
        suggestions.append(f"ZmieÅ„ starter '{starter}' (uÅ¼yty {count}Ã—)")
    
    return {
        "value": normalized_entropy,
        "status": status.value,
        "message": message,
        "unique_starters": len(counter),
        "total_sentences": len(sentences),
        "repetitive_starters": repetitive,
        "suggestions": suggestions
    }


def calculate_word_repetition(text: str) -> Dict[str, Any]:
    words = tokenize_no_stopwords(text)
    
    if len(words) < 50:
        return {
            "value": 1.0,
            "status": Severity.WARNING.value,
            "message": "Za maÅ‚o sÅ‚Ã³w do analizy",
            "word_count": len(words)
        }
    
    counter = Counter(words)
    config = AIDetectionConfig()
    
    overused = {}
    warnings_list = []
    critical_list = []
    
    for word, count in counter.most_common(20):
        if count > config.REPETITION_CRITICAL:
            critical_list.append({"word": word, "count": count})
            overused[word] = count
        elif count > config.REPETITION_WARNING:
            warnings_list.append({"word": word, "count": count})
            overused[word] = count
        elif count > config.REPETITION_OK:
            warnings_list.append({"word": word, "count": count})
    
    overused_count = sum(overused.values())
    score = 1 - (overused_count / len(words)) if words else 1
    score = round(max(0, score), 2)
    
    if critical_list:
        status = Severity.CRITICAL
        message = f"SÅ‚owa powtÃ³rzone > {config.REPETITION_CRITICAL}Ã—: {', '.join([c['word'] for c in critical_list[:3]])}"
    elif warnings_list:
        status = Severity.WARNING
        message = f"SÅ‚owa powtÃ³rzone > {config.REPETITION_OK}Ã—. UÅ¼yj synonimÃ³w."
    else:
        status = Severity.OK
        message = "Brak nadmiernych powtÃ³rzeÅ„"
    
    SYNONYM_MAP = {
        "firma": ["przedsiÄ™biorstwo", "spÃ³Å‚ka", "wykonawca", "usÅ‚ugodawca"],
        "usÅ‚uga": ["Å›wiadczenie", "realizacja", "obsÅ‚uga"],
        "oferowaÄ‡": ["zapewniaÄ‡", "proponowaÄ‡", "Å›wiadczyÄ‡"],
        "klient": ["zleceniodawca", "usÅ‚ugobiorca", "zamawiajÄ…cy"],
        "profesjonalny": ["doÅ›wiadczony", "wykwalifikowany", "certyfikowany"],
        "cena": ["koszt", "stawka", "wycena", "taryfa"],
    }
    
    suggestions = []
    for word in overused:
        if word in SYNONYM_MAP:
            suggestions.append(f"'{word}' â†’ {', '.join(SYNONYM_MAP[word][:3])}")
    
    return {
        "value": score,
        "status": status.value,
        "message": message,
        "overused_words": overused,
        "critical_words": critical_list,
        "warning_words": warnings_list,
        "suggestions": suggestions[:5]
    }


# ================================================================
# ðŸŽ¯ GÅÃ“WNA FUNKCJA - HUMANNESS SCORE
# ================================================================
def calculate_humanness_score(text: str) -> Dict[str, Any]:
    config = AIDetectionConfig()
    
    burstiness = calculate_burstiness(text)
    vocabulary = calculate_vocabulary_richness(text)
    sophistication = calculate_lexical_sophistication(text)
    entropy = calculate_starter_entropy(text)
    repetition = calculate_word_repetition(text)
    
    def normalize_burstiness(val):
        if val < config.BURSTINESS_CRITICAL_LOW:
            return 0.0
        elif val < config.BURSTINESS_OK_MIN:
            return (val - config.BURSTINESS_CRITICAL_LOW) / (config.BURSTINESS_OK_MIN - config.BURSTINESS_CRITICAL_LOW) * 0.5
        elif val <= config.BURSTINESS_OK_MAX:
            return 1.0
        elif val < config.BURSTINESS_CRITICAL_HIGH:
            return 1.0 - (val - config.BURSTINESS_OK_MAX) / (config.BURSTINESS_CRITICAL_HIGH - config.BURSTINESS_OK_MAX) * 0.5
        else:
            return 0.0
    
    def normalize_ttr(val):
        if val >= config.TTR_OK:
            return 1.0
        elif val >= config.TTR_WARNING:
            return 0.5 + (val - config.TTR_WARNING) / (config.TTR_OK - config.TTR_WARNING) * 0.5
        elif val >= config.TTR_CRITICAL:
            return (val - config.TTR_CRITICAL) / (config.TTR_WARNING - config.TTR_CRITICAL) * 0.5
        else:
            return 0.0
    
    def normalize_zipf(val):
        if not WORDFREQ_AVAILABLE or val == 0:
            return 0.5
        if val <= config.ZIPF_OK:
            return 1.0
        elif val <= config.ZIPF_WARNING:
            return 0.5 + (config.ZIPF_WARNING - val) / (config.ZIPF_WARNING - config.ZIPF_OK) * 0.5
        elif val <= config.ZIPF_CRITICAL:
            return (config.ZIPF_CRITICAL - val) / (config.ZIPF_CRITICAL - config.ZIPF_WARNING) * 0.5
        else:
            return 0.0
    
    def normalize_entropy(val):
        if val >= config.ENTROPY_OK:
            return 1.0
        elif val >= config.ENTROPY_WARNING:
            return 0.5 + (val - config.ENTROPY_WARNING) / (config.ENTROPY_OK - config.ENTROPY_WARNING) * 0.5
        elif val >= config.ENTROPY_CRITICAL:
            return (val - config.ENTROPY_CRITICAL) / (config.ENTROPY_WARNING - config.ENTROPY_CRITICAL) * 0.5
        else:
            return 0.0
    
    scores = {
        "burstiness": normalize_burstiness(burstiness.get("value", 0)),
        "vocabulary": normalize_ttr(vocabulary.get("value", 0)),
        "sophistication": normalize_zipf(sophistication.get("value", 0)),
        "entropy": normalize_entropy(entropy.get("value", 0)),
        "repetition": repetition.get("value", 1.0)
    }
    
    weights = config.WEIGHTS
    humanness = sum(scores[k] * weights[k] for k in scores)
    humanness_score = round(humanness * 100, 0)
    
    if humanness_score < config.HUMANNESS_CRITICAL:
        status = Severity.CRITICAL
        overall_message = f"CRITICAL: Tekst wyglÄ…da na AI (score {humanness_score}). Przepisz!"
    elif humanness_score < config.HUMANNESS_WARNING:
        status = Severity.WARNING
        overall_message = f"WARNING: Tekst wymaga poprawy (score {humanness_score})"
    else:
        status = Severity.OK
        overall_message = f"OK: Tekst wyglÄ…da naturalnie (score {humanness_score})"
    
    all_warnings = []
    if burstiness.get("status") != "OK":
        all_warnings.append(burstiness.get("message"))
    if vocabulary.get("status") != "OK":
        all_warnings.append(vocabulary.get("message"))
    if sophistication.get("status") not in ["OK", "WARNING"] or sophistication.get("value", 0) > config.ZIPF_WARNING:
        all_warnings.append(sophistication.get("message"))
    if entropy.get("status") != "OK":
        all_warnings.append(entropy.get("message"))
    if repetition.get("status") != "OK":
        all_warnings.append(repetition.get("message"))
    
    all_suggestions = []
    all_suggestions.extend(entropy.get("suggestions", []))
    all_suggestions.extend(repetition.get("suggestions", []))
    
    return {
        "humanness_score": int(humanness_score),
        "status": status.value,
        "message": overall_message,
        "components": {
            "burstiness": burstiness,
            "vocabulary_richness": vocabulary,
            "lexical_sophistication": sophistication,
            "starter_entropy": entropy,
            "word_repetition": repetition
        },
        "normalized_scores": scores,
        "warnings": all_warnings[:5],
        "suggestions": all_suggestions[:5]
    }


# ================================================================
# ðŸ” QUICK CHECK
# ================================================================
def quick_ai_check(text: str) -> Dict[str, Any]:
    burstiness = calculate_burstiness(text)
    humanness = calculate_humanness_score(text)
    
    return {
        "humanness_score": humanness["humanness_score"],
        "status": humanness["status"],
        "burstiness": burstiness["value"],
        "top_warning": humanness["warnings"][0] if humanness["warnings"] else None
    }


# ================================================================
# ðŸ†• CRITICAL: FORBIDDEN PHRASES CHECK
# ================================================================
FORBIDDEN_PATTERNS = [
    (r'\bwarto wiedzieÄ‡\b', "warto wiedzieÄ‡"),
    (r'\bnaleÅ¼y pamiÄ™taÄ‡\b', "naleÅ¼y pamiÄ™taÄ‡"),
    (r'\bkluczowy aspekt\b', "kluczowy aspekt"),
    (r'\bkompleksowe rozwiÄ…zanie\b', "kompleksowe rozwiÄ…zanie"),
    (r'\bholistyczne podejÅ›cie\b', "holistyczne podejÅ›cie"),
    (r'\bw dzisiejszych czasach\b', "w dzisiejszych czasach"),
    (r'\bnie ulega wÄ…tpliwoÅ›ci\b', "nie ulega wÄ…tpliwoÅ›ci"),
    (r'\bcoraz wiÄ™cej osÃ³b\b', "coraz wiÄ™cej osÃ³b"),
    (r'\bw tym artykule\b', "w tym artykule"),
    (r'\bpodsumowujÄ…c\b', "podsumowujÄ…c"),
    (r'\bjak juÅ¼ wspomniano\b', "jak juÅ¼ wspomniano"),
    (r'\bkaÅ¼dy z nas\b', "kaÅ¼dy z nas"),
    (r'\bnie jest tajemnicÄ…\b', "nie jest tajemnicÄ…"),
    (r'\bpowszechnie wiadomo\b', "powszechnie wiadomo"),
]

def check_forbidden_phrases(text: str) -> Dict[str, Any]:
    text_lower = text.lower()
    found = []
    
    for pattern, name in FORBIDDEN_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            found.append(name)
    
    if found:
        status = Severity.CRITICAL if len(found) >= 3 else Severity.WARNING
        message = f"Znaleziono {len(found)} zakazanych fraz AI: {', '.join(found[:3])}"
    else:
        status = Severity.OK
        message = "Brak zakazanych fraz"
    
    return {
        "status": status.value,
        "forbidden_found": found,
        "count": len(found),
        "message": message
    }


# ================================================================
# ðŸ†• CRITICAL: JITTER VALIDATION
# ================================================================
def validate_jitter(current_paragraphs: int, previous_paragraphs: int = None) -> Dict[str, Any]:
    if previous_paragraphs is None:
        return {
            "status": Severity.OK.value,
            "message": "Pierwszy batch - JITTER OK",
            "current": current_paragraphs,
            "previous": None
        }
    
    if current_paragraphs == previous_paragraphs:
        return {
            "status": Severity.WARNING.value,
            "message": f"JITTER fail: {current_paragraphs}ak = poprzedni ({previous_paragraphs}ak). ZmieÅ„ liczbÄ™ akapitÃ³w!",
            "current": current_paragraphs,
            "previous": previous_paragraphs
        }
    
    return {
        "status": Severity.OK.value,
        "message": f"JITTER OK: {current_paragraphs}ak â‰  {previous_paragraphs}ak",
        "current": current_paragraphs,
        "previous": previous_paragraphs
    }


# ================================================================
# ðŸ†• CRITICAL: TRIPLETS VALIDATION
# ================================================================
def validate_triplets(text: str, s1_relationships: List[Dict]) -> Dict[str, Any]:
    if not s1_relationships:
        return {
            "status": Severity.OK.value,
            "message": "Brak tripletÃ³w z S1 do sprawdzenia",
            "found": 0,
            "expected": 0
        }
    
    text_lower = text.lower()
    found = []
    
    for rel in s1_relationships:
        subject = rel.get("subject", "").lower()
        predicate = rel.get("predicate", "").lower()
        obj = rel.get("object", "").lower()
        
        if subject and predicate and obj:
            if subject in text_lower and predicate in text_lower and obj in text_lower:
                found.append(rel)
    
    expected = min(3, len(s1_relationships))
    
    if len(found) >= 2:
        status = Severity.OK
        message = f"Znaleziono {len(found)} tripletÃ³w (min 2)"
    elif len(found) == 1:
        status = Severity.WARNING
        message = f"Tylko 1 triplet znaleziony (min 2)"
    else:
        status = Severity.WARNING
        message = f"Brak tripletÃ³w z S1 (min 2)"
    
    return {
        "status": status.value,
        "message": message,
        "found": len(found),
        "expected": expected,
        "triplets_found": found[:5]
    }


# ================================================================
# ðŸŽ¯ FULL AI DETECTION (z CRITICAL validations)
# ================================================================
def full_ai_detection(
    text: str, 
    previous_paragraphs: int = None,
    s1_relationships: List[Dict] = None
) -> Dict[str, Any]:
    """
    PeÅ‚na analiza AI detection + walidacje CRITICAL.
    """
    humanness = calculate_humanness_score(text)
    forbidden = check_forbidden_phrases(text)
    
    current_paragraphs = len(re.split(r'\n\s*\n', text.strip()))
    jitter = validate_jitter(current_paragraphs, previous_paragraphs)
    
    triplets = validate_triplets(text, s1_relationships or [])
    
    statuses = [
        humanness["status"],
        forbidden["status"],
        jitter["status"],
        triplets["status"]
    ]
    
    if "CRITICAL" in statuses:
        overall_status = Severity.CRITICAL.value
    elif "WARNING" in statuses:
        overall_status = Severity.WARNING.value
    else:
        overall_status = Severity.OK.value
    
    all_warnings = humanness.get("warnings", [])
    if forbidden["status"] != "OK":
        all_warnings.append(forbidden["message"])
    if jitter["status"] != "OK":
        all_warnings.append(jitter["message"])
    if triplets["status"] != "OK":
        all_warnings.append(triplets["message"])
    
    return {
        "humanness_score": humanness["humanness_score"],
        "status": overall_status,
        "components": humanness["components"],
        "validations": {
            "forbidden_phrases": forbidden,
            "jitter": jitter,
            "triplets": triplets
        },
        "warnings": all_warnings[:7],
        "suggestions": humanness.get("suggestions", [])[:5]
    }


# ================================================================
# ðŸ†• FAZA 2: ENTITY SPLIT 60/40
# ================================================================
def calculate_entity_split(text: str, s1_entities: List[Dict]) -> Dict[str, Any]:
    """
    Oblicza proporcjÄ™ Core vs Supporting entities.
    
    Cel: 60% Core, 40% Supporting
    """
    if not s1_entities:
        return {
            "status": "NO_DATA",
            "message": "Brak danych o encjach z S1",
            "core_ratio": 0,
            "supporting_ratio": 0
        }
    
    text_lower = text.lower()
    
    # Rozdziel encje na Core i Supporting
    core_entities = []
    supporting_entities = []
    
    for e in s1_entities:
        category = e.get("category", "").upper()
        importance = e.get("importance", 0.5)
        
        # JeÅ›li brak category, uÅ¼yj importance do klasyfikacji
        if category == "CORE" or (not category and importance >= 0.6):
            core_entities.append(e)
        elif category == "SUPPORTING" or (not category and importance < 0.6):
            supporting_entities.append(e)
        else:
            # DomyÅ›lnie jako supporting
            supporting_entities.append(e)
    
    # Zlicz znalezione
    core_found = 0
    supporting_found = 0
    core_used = []
    supporting_used = []
    
    for e in core_entities:
        name = e.get("name", e.get("text", "")).lower()
        if name and name in text_lower:
            core_found += 1
            core_used.append(name)
    
    for e in supporting_entities:
        name = e.get("name", e.get("text", "")).lower()
        if name and name in text_lower:
            supporting_found += 1
            supporting_used.append(name)
    
    total_found = core_found + supporting_found
    
    if total_found == 0:
        return {
            "status": "WARNING",
            "message": "Nie znaleziono Å¼adnych encji w tekÅ›cie",
            "core_ratio": 0,
            "supporting_ratio": 0,
            "core_found": 0,
            "supporting_found": 0
        }
    
    core_ratio = core_found / total_found
    supporting_ratio = supporting_found / total_found
    
    # Status: OK jeÅ›li core_ratio miÄ™dzy 0.55 a 0.65
    if 0.55 <= core_ratio <= 0.65:
        status = "OK"
        message = f"Entity split OK: {core_ratio:.0%} core / {supporting_ratio:.0%} supporting"
    elif core_ratio > 0.65:
        status = "WARNING"
        message = f"Za duÅ¼o Core entities ({core_ratio:.0%}). Dodaj wiÄ™cej Supporting (ubezpieczenie, certyfikaty, normy)"
    else:
        status = "WARNING"
        message = f"Za maÅ‚o Core entities ({core_ratio:.0%}). Dodaj wiÄ™cej Core (gÅ‚Ã³wne tematy)"
    
    return {
        "status": status,
        "message": message,
        "core_ratio": round(core_ratio, 2),
        "supporting_ratio": round(supporting_ratio, 2),
        "core_found": core_found,
        "supporting_found": supporting_found,
        "core_total": len(core_entities),
        "supporting_total": len(supporting_entities),
        "core_used": core_used[:10],
        "supporting_used": supporting_used[:10]
    }


# ================================================================
# ðŸ†• FAZA 2: TOPIC COMPLETENESS
# ================================================================
def calculate_topic_completeness(text: str, s1_topics: List[Dict]) -> Dict[str, Any]:
    """
    Oblicza pokrycie tematÃ³w z S1.
    """
    if not s1_topics:
        return {
            "status": "NO_DATA",
            "score": 0,
            "message": "Brak danych o tematach z S1"
        }
    
    text_lower = text.lower()
    
    # Rozdziel tematy wedÅ‚ug priorytetu
    must_topics = []
    high_topics = []
    medium_topics = []
    
    for t in s1_topics:
        priority = t.get("priority", "MEDIUM").upper()
        if priority == "MUST":
            must_topics.append(t)
        elif priority == "HIGH":
            high_topics.append(t)
        else:
            medium_topics.append(t)
    
    # SprawdÅº pokrycie
    def check_topic_covered(topic):
        name = topic.get("name", topic.get("subtopic", "")).lower()
        keywords = topic.get("keywords", [])
        
        # SprawdÅº nazwÄ™
        if name and name in text_lower:
            return True
        
        # SprawdÅº sÅ‚owa kluczowe
        for kw in keywords:
            if kw.lower() in text_lower:
                return True
        
        # SprawdÅº sample_h2 jeÅ›li istnieje
        sample_h2 = topic.get("sample_h2", "").lower()
        if sample_h2:
            words = sample_h2.split()
            matches = sum(1 for w in words if len(w) > 3 and w in text_lower)
            if matches >= len(words) * 0.5:
                return True
        
        return False
    
    # Zlicz pokryte
    must_covered = [t for t in must_topics if check_topic_covered(t)]
    high_covered = [t for t in high_topics if check_topic_covered(t)]
    medium_covered = [t for t in medium_topics if check_topic_covered(t)]
    
    # Oblicz score (MUST ma najwyÅ¼szÄ… wagÄ™)
    total_weight = len(must_topics) * 3 + len(high_topics) * 2 + len(medium_topics) * 1
    covered_weight = len(must_covered) * 3 + len(high_covered) * 2 + len(medium_covered) * 1
    
    score = covered_weight / total_weight if total_weight > 0 else 0
    score = round(score, 2)
    
    # ZnajdÅº brakujÄ…ce MUST i HIGH
    must_missing = [t.get("name", t.get("subtopic", "unknown")) for t in must_topics if t not in must_covered]
    high_missing = [t.get("name", t.get("subtopic", "unknown")) for t in high_topics if t not in high_covered]
    
    # Status
    if score >= 0.8:
        status = "OK"
        message = f"Dobre pokrycie tematÃ³w ({score:.0%})"
    elif score >= 0.6:
        status = "WARNING"
        message = f"Pokrycie tematÃ³w {score:.0%} - dodaj brakujÄ…ce"
    else:
        status = "WARNING"
        message = f"Niskie pokrycie tematÃ³w ({score:.0%}) - pilnie uzupeÅ‚nij!"
    
    return {
        "status": status,
        "score": score,
        "score_percent": round(score * 100, 1),
        "message": message,
        "must_covered": len(must_covered),
        "must_total": len(must_topics),
        "high_covered": len(high_covered),
        "high_total": len(high_topics),
        "must_missing": must_missing[:5],
        "high_missing": high_missing[:5]
    }


# ================================================================
# ðŸ†• FAZA 2: BATCH HISTORY TRACKING
# ================================================================
def analyze_batch_trend(batch_history: List[Dict]) -> Dict[str, Any]:
    """
    Analizuje trend metryk miÄ™dzy batchami.
    """
    if not batch_history or len(batch_history) < 2:
        return {
            "trend": "insufficient_data",
            "message": "Za maÅ‚o danych do analizy trendu"
        }
    
    # Pobierz ostatnie 3 batche (lub mniej jeÅ›li brak)
    recent = batch_history[-3:]
    
    # Analizuj humanness score
    humanness_scores = [b.get("humanness_score", 0) for b in recent]
    
    # Oblicz trend
    if len(humanness_scores) >= 2:
        first_half = sum(humanness_scores[:len(humanness_scores)//2 + 1]) / (len(humanness_scores)//2 + 1)
        second_half = sum(humanness_scores[len(humanness_scores)//2:]) / (len(humanness_scores) - len(humanness_scores)//2)
        
        diff = second_half - first_half
        
        if diff > 5:
            trend = "improving"
            trend_message = f"ðŸ“ˆ Trend rosnÄ…cy (+{diff:.1f} punktÃ³w)"
        elif diff < -5:
            trend = "declining"
            trend_message = f"ðŸ“‰ Trend spadkowy ({diff:.1f} punktÃ³w)"
        else:
            trend = "stable"
            trend_message = "âž¡ï¸ Trend stabilny"
    else:
        trend = "stable"
        trend_message = "âž¡ï¸ Trend stabilny"
    
    # Åšrednie metryki
    avg_humanness = sum(humanness_scores) / len(humanness_scores) if humanness_scores else 0
    
    burstiness_scores = [b.get("burstiness", 0) for b in recent if b.get("burstiness")]
    avg_burstiness = sum(burstiness_scores) / len(burstiness_scores) if burstiness_scores else 0
    
    return {
        "trend": trend,
        "message": trend_message,
        "avg_humanness": round(avg_humanness, 1),
        "avg_burstiness": round(avg_burstiness, 2),
        "batches_analyzed": len(recent),
        "last_scores": humanness_scores
    }


def create_batch_record(
    batch_number: int,
    humanness_score: int,
    burstiness: float,
    paragraphs: int,
    entity_density: float = 0,
    topic_completeness: float = 0
) -> Dict[str, Any]:
    """
    Tworzy rekord batcha do historii.
    """
    return {
        "batch": batch_number,
        "humanness_score": humanness_score,
        "burstiness": round(burstiness, 2),
        "paragraphs": paragraphs,
        "entity_density": round(entity_density, 2),
        "topic_completeness": round(topic_completeness, 2)
    }


# ================================================================
# ðŸ†• FAZA 3: PER-SENTENCE SCORING
# ================================================================
AI_PATTERN_FLAGS = [
    (r'\bwarto\b', "warto"),
    (r'\bnaleÅ¼y\b', "naleÅ¼y"),
    (r'\bkluczowy\b', "kluczowy"),
    (r'\bkompleksowy\b', "kompleksowy"),
    (r'\binnowacyjny\b', "innowacyjny"),
    (r'\bprofesjonalny\b', "profesjonalny"),
    (r'\bwysokiej jakoÅ›ci\b', "wysokiej jakoÅ›ci"),
    (r'\bszeroki zakres\b', "szeroki zakres"),
    (r'\bw peÅ‚ni\b', "w peÅ‚ni"),
    (r'\bw szczegÃ³lnoÅ›ci\b', "w szczegÃ³lnoÅ›ci"),
]

GENERIC_STARTERS = [
    "firma oferuje",
    "firma zapewnia",
    "firma gwarantuje",
    "usÅ‚ugi obejmujÄ…",
    "klienci otrzymujÄ…",
    "warto wiedzieÄ‡",
    "naleÅ¼y pamiÄ™taÄ‡",
    "waÅ¼ne jest",
]


def score_single_sentence(sentence: str) -> Dict[str, Any]:
    """
    Ocenia pojedyncze zdanie pod kÄ…tem AI-like patterns.
    """
    sentence_lower = sentence.lower().strip()
    words = sentence.split()
    word_count = len(words)
    
    # Flagi AI
    ai_flags = []
    for pattern, name in AI_PATTERN_FLAGS:
        if re.search(pattern, sentence_lower):
            ai_flags.append(name)
    
    # SprawdÅº starter
    starter = ' '.join(words[:3]).lower() if len(words) >= 3 else sentence_lower
    generic_starter = any(gs in starter for gs in GENERIC_STARTERS)
    
    # Oblicz score zdania (0-100)
    score = 100
    
    # Kary
    if ai_flags:
        score -= len(ai_flags) * 15  # -15 za kaÅ¼dÄ… flagÄ™ AI
    
    if generic_starter:
        score -= 20  # -20 za generyczny starter
    
    # Kara za zbyt rÃ³wnÄ… dÅ‚ugoÅ›Ä‡ (typowe dla AI: 12-18 sÅ‚Ã³w)
    if 12 <= word_count <= 18:
        score -= 5  # lekka kara za "Å›redniÄ…" dÅ‚ugoÅ›Ä‡
    
    # Bonus za krÃ³tkie (<8) lub dÅ‚ugie (>25) zdania
    if word_count < 8 or word_count > 25:
        score += 10
    
    score = max(0, min(100, score))
    
    # Status
    if score >= 70:
        status = "OK"
    elif score >= 50:
        status = "WARNING"
    else:
        status = "AI_LIKE"
    
    return {
        "text": sentence[:80] + ("..." if len(sentence) > 80 else ""),
        "word_count": word_count,
        "starter": starter,
        "score": score,
        "status": status,
        "ai_flags": ai_flags,
        "generic_starter": generic_starter
    }


def score_sentences(text: str, limit: int = 20) -> Dict[str, Any]:
    """
    Ocenia wszystkie zdania w tekÅ›cie.
    Zwraca posortowane od najgorszych.
    """
    sentences = split_into_sentences(text)
    
    if not sentences:
        return {
            "status": "NO_DATA",
            "message": "Brak zdaÅ„ do analizy",
            "sentences": []
        }
    
    scored = []
    for s in sentences:
        result = score_single_sentence(s)
        scored.append(result)
    
    # Sortuj od najgorszych (najniÅ¼szy score)
    scored.sort(key=lambda x: x["score"])
    
    # Statystyki
    scores = [s["score"] for s in scored]
    avg_score = sum(scores) / len(scores) if scores else 0
    
    ai_like_count = sum(1 for s in scored if s["status"] == "AI_LIKE")
    warning_count = sum(1 for s in scored if s["status"] == "WARNING")
    ok_count = sum(1 for s in scored if s["status"] == "OK")
    
    # Status ogÃ³lny
    if ai_like_count >= 3:
        overall_status = "CRITICAL"
        message = f"Znaleziono {ai_like_count} zdaÅ„ wyglÄ…dajÄ…cych na AI. Przepisz je!"
    elif ai_like_count >= 1 or warning_count >= 5:
        overall_status = "WARNING"
        message = f"Znaleziono {ai_like_count} AI-like i {warning_count} warning zdaÅ„"
    else:
        overall_status = "OK"
        message = "Zdania wyglÄ…dajÄ… naturalnie"
    
    # Sugestie poprawy dla najgorszych zdaÅ„
    suggestions = []
    for s in scored[:5]:  # Top 5 najgorszych
        if s["status"] in ["AI_LIKE", "WARNING"]:
            if s["ai_flags"]:
                suggestions.append(f"Zdanie '{s['text'][:40]}...' - usuÅ„: {', '.join(s['ai_flags'][:2])}")
            elif s["generic_starter"]:
                suggestions.append(f"Zdanie '{s['text'][:40]}...' - zmieÅ„ starter")
    
    return {
        "status": overall_status,
        "message": message,
        "total_sentences": len(sentences),
        "avg_score": round(avg_score, 1),
        "ai_like_count": ai_like_count,
        "warning_count": warning_count,
        "ok_count": ok_count,
        "worst_sentences": scored[:limit],
        "suggestions": suggestions[:5]
    }


# ================================================================
# ðŸ†• FAZA 3: N-GRAM NATURALNESS CHECK (z wordfreq)
# ================================================================

# Znane nienaturalne frazy AI (blacklist)
AI_BLACKLIST_NGRAMS = [
    "kluczowy aspekt",
    "holistyczne podejÅ›cie", 
    "innowacyjne rozwiÄ…zanie",
    "strategiczne znaczenie",
    "fundamentalne znaczenie",
    "nie ulega wÄ…tpliwoÅ›ci",
    "warto zauwaÅ¼yÄ‡ Å¼e",
    "naleÅ¼y podkreÅ›liÄ‡ Å¼e",
    "kompleksowe rozwiÄ…zanie",
    "szeroki zakres usÅ‚ug",
    "indywidualne podejÅ›cie",
    "wysoki standard",
    "peÅ‚en profesjonalizm",
    "bogaty doÅ›wiadczenie",
    "dynamicznie rozwijajÄ…cy",
]

# NaduÅ¼ywane frazy SEO (nie bÅ‚Ä…d, ale za czÄ™sto = AI)
OVERUSED_SEO_PHRASES = [
    "firma oferuje",
    "profesjonalne usÅ‚ugi", 
    "wysoka jakoÅ›Ä‡",
    "kompleksowa obsÅ‚uga",
    "konkurencyjne ceny",
    "doÅ›wiadczony zespÃ³Å‚",
    "wieloletnie doÅ›wiadczenie",
    "szeroka oferta",
    "najwyÅ¼sza jakoÅ›Ä‡",
]


def get_word_frequency(word: str) -> float:
    """
    Zwraca czÄ™stoÅ›Ä‡ sÅ‚owa (skala Zipf 0-7).
    JeÅ›li wordfreq niedostÄ™pny, zwraca domyÅ›lnÄ… wartoÅ›Ä‡.
    """
    if not WORDFREQ_AVAILABLE:
        return 4.0  # Å›rednia domyÅ›lna
    
    try:
        freq = zipf_frequency(word, 'pl')
        return freq if freq > 0 else 1.0  # nieznane sÅ‚owa = rzadkie
    except:
        return 4.0


def calculate_ngram_frequency(ngram: str) -> Dict[str, Any]:
    """
    Oblicza Å›redniÄ… czÄ™stoÅ›Ä‡ n-gramu na podstawie czÄ™stoÅ›ci sÅ‚Ã³w.
    """
    words = ngram.lower().split()
    if not words:
        return {"ngram": ngram, "avg_freq": 0, "min_freq": 0}
    
    freqs = [get_word_frequency(w) for w in words if w not in POLISH_STOP_WORDS]
    
    if not freqs:
        # Wszystkie sÅ‚owa to stop words - wysoka czÄ™stoÅ›Ä‡
        return {"ngram": ngram, "avg_freq": 6.0, "min_freq": 6.0}
    
    return {
        "ngram": ngram,
        "avg_freq": round(sum(freqs) / len(freqs), 2),
        "min_freq": round(min(freqs), 2),
        "word_count": len(words)
    }


def extract_ngrams(text: str, n: int = 2) -> List[str]:
    """
    WyciÄ…ga n-gramy z tekstu.
    """
    # UsuÅ„ HTML i normalizuj
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    text = re.sub(r'\s+', ' ', text).strip()
    
    words = text.split()
    
    if len(words) < n:
        return []
    
    ngrams = []
    for i in range(len(words) - n + 1):
        ngram = ' '.join(words[i:i+n])
        ngrams.append(ngram)
    
    return ngrams


def check_ngram_naturalness(text: str) -> Dict[str, Any]:
    """
    Sprawdza naturalnoÅ›Ä‡ fraz w tekÅ›cie uÅ¼ywajÄ…c wordfreq.
    
    Metoda:
    1. WyciÄ…ga bigramy i trigramy
    2. Oblicza czÄ™stoÅ›Ä‡ kaÅ¼dego n-gramu (Å›rednia Zipf sÅ‚Ã³w)
    3. Identyfikuje rzadkie/nienaturalne frazy
    4. Sprawdza blacklistÄ™ AI
    5. Sprawdza naduÅ¼ywane frazy SEO
    """
    text_lower = text.lower()
    words = text_lower.split()
    
    if len(words) < 50:
        return {
            "status": "NO_DATA",
            "message": "Za maÅ‚o tekstu do analizy n-gramÃ³w",
            "wordfreq_available": WORDFREQ_AVAILABLE
        }
    
    # 1. SprawdÅº blacklistÄ™ AI
    ai_phrases_found = []
    for phrase in AI_BLACKLIST_NGRAMS:
        count = text_lower.count(phrase)
        if count > 0:
            ai_phrases_found.append({"phrase": phrase, "count": count})
    
    # 2. SprawdÅº naduÅ¼ywane frazy SEO
    overused_found = []
    for phrase in OVERUSED_SEO_PHRASES:
        count = text_lower.count(phrase)
        if count >= 2:  # 2+ = naduÅ¼ywane
            overused_found.append({"phrase": phrase, "count": count})
    
    # 3. WyciÄ…gnij i przeanalizuj bigramy (jeÅ›li wordfreq dostÄ™pny)
    unusual_ngrams = []
    low_freq_ngrams = []
    
    if WORDFREQ_AVAILABLE:
        bigrams = extract_ngrams(text, n=2)
        
        # Zlicz bigramy
        bigram_counts = Counter(bigrams)
        
        # Analizuj najczÄ™stsze bigramy (potencjalnie naduÅ¼ywane)
        for bigram, count in bigram_counts.most_common(30):
            if count >= 3:  # PowtÃ³rzone 3+ razy
                freq_data = calculate_ngram_frequency(bigram)
                
                # JeÅ›li niska Å›rednia czÄ™stoÅ›Ä‡ = dziwna fraza
                if freq_data["avg_freq"] < 3.5:
                    unusual_ngrams.append({
                        "ngram": bigram,
                        "count": count,
                        "avg_freq": freq_data["avg_freq"],
                        "reason": "low_frequency"
                    })
                # JeÅ›li wysoka czÄ™stoÅ›Ä‡ ale duÅ¼o powtÃ³rzeÅ„ = naduÅ¼ywane
                elif count >= 5:
                    unusual_ngrams.append({
                        "ngram": bigram,
                        "count": count,
                        "avg_freq": freq_data["avg_freq"],
                        "reason": "overused"
                    })
        
        # ZnajdÅº ogÃ³lnie rzadkie bigramy (min_freq < 2.5)
        unique_bigrams = list(set(bigrams))[:100]  # SprawdÅº max 100
        for bigram in unique_bigrams:
            freq_data = calculate_ngram_frequency(bigram)
            if freq_data["min_freq"] < 2.0 and freq_data["min_freq"] > 0:
                low_freq_ngrams.append({
                    "ngram": bigram,
                    "min_freq": freq_data["min_freq"],
                    "avg_freq": freq_data["avg_freq"]
                })
        
        # Sortuj po czÄ™stoÅ›ci (najrzadsze najpierw)
        low_freq_ngrams.sort(key=lambda x: x["min_freq"])
        low_freq_ngrams = low_freq_ngrams[:10]
    
    # 4. Oblicz naturalness score
    penalty = 0
    
    # Kary za AI phrases (najwiÄ™ksza kara)
    penalty += len(ai_phrases_found) * 0.15
    
    # Kary za naduÅ¼ywane SEO
    penalty += len(overused_found) * 0.08
    
    # Kary za unusual ngrams
    penalty += len(unusual_ngrams) * 0.05
    
    # Kary za low freq ngrams
    penalty += min(len(low_freq_ngrams) * 0.03, 0.15)
    
    naturalness_score = max(0, 1.0 - penalty)
    naturalness_score = round(naturalness_score, 2)
    
    # 5. Status
    if naturalness_score >= 0.75:
        status = "OK"
        message = f"Frazy brzmiÄ… naturalnie (score {naturalness_score})"
    elif naturalness_score >= 0.5:
        status = "WARNING"
        message = f"NiektÃ³re frazy wymagajÄ… poprawy (score {naturalness_score})"
    else:
        status = "CRITICAL"
        message = f"Wiele fraz brzmi nienaturalnie/AI (score {naturalness_score})"
    
    # 6. Sugestie
    suggestions = []
    
    # Sugestie dla AI phrases (priorytet)
    for item in ai_phrases_found[:3]:
        suggestions.append(f"âŒ UsuÅ„ AI-frazÄ™: '{item['phrase']}'")
    
    # Sugestie dla naduÅ¼ywanych
    for item in overused_found[:2]:
        suggestions.append(f"âš ï¸ Ogranicz '{item['phrase']}' (uÅ¼yte {item['count']}Ã—)")
    
    # Sugestie dla unusual
    for item in unusual_ngrams[:2]:
        if item["reason"] == "overused":
            suggestions.append(f"ðŸ“ Zmniejsz powtÃ³rzenia: '{item['ngram']}' ({item['count']}Ã—)")
        else:
            suggestions.append(f"ðŸ“ SprawdÅº frazÄ™: '{item['ngram']}' (rzadka)")
    
    return {
        "status": status,
        "message": message,
        "naturalness_score": naturalness_score,
        "wordfreq_available": WORDFREQ_AVAILABLE,
        
        # SzczegÃ³Å‚y
        "ai_phrases_found": ai_phrases_found[:5],
        "overused_seo_phrases": overused_found[:5],
        "unusual_ngrams": unusual_ngrams[:5],
        "low_frequency_ngrams": low_freq_ngrams[:5],
        
        # Statystyki
        "stats": {
            "ai_phrases_count": len(ai_phrases_found),
            "overused_count": len(overused_found),
            "unusual_count": len(unusual_ngrams)
        },
        
        "suggestions": suggestions[:7]
    }


# ================================================================
# ðŸŽ¯ FAZA 3: FULL ADVANCED ANALYSIS
# ================================================================
def full_advanced_analysis(
    text: str,
    previous_paragraphs: int = None,
    s1_relationships: List[Dict] = None,
    s1_entities: List[Dict] = None,
    s1_topics: List[Dict] = None
) -> Dict[str, Any]:
    """
    PeÅ‚na zaawansowana analiza tekstu - wszystkie metryki.
    """
    # Podstawowa analiza AI
    humanness = calculate_humanness_score(text)
    forbidden = check_forbidden_phrases(text)
    
    # Walidacje
    current_paragraphs = len(re.split(r'\n\s*\n', text.strip()))
    jitter = validate_jitter(current_paragraphs, previous_paragraphs)
    triplets = validate_triplets(text, s1_relationships or [])
    
    # Faza 2
    entity_split = calculate_entity_split(text, s1_entities or [])
    topic_completeness = calculate_topic_completeness(text, s1_topics or [])
    
    # Faza 3
    sentence_analysis = score_sentences(text, limit=10)
    ngram_analysis = check_ngram_naturalness(text)
    
    # ÅÄ…czny status
    statuses = [
        humanness["status"],
        forbidden["status"],
        sentence_analysis["status"],
        ngram_analysis["status"]
    ]
    
    if "CRITICAL" in statuses:
        overall_status = "CRITICAL"
    elif statuses.count("WARNING") >= 2:
        overall_status = "WARNING"
    elif "WARNING" in statuses:
        overall_status = "OK"  # pojedynczy warning OK
    else:
        overall_status = "OK"
    
    # Zbierz wszystkie sugestie
    all_suggestions = []
    all_suggestions.extend(humanness.get("suggestions", []))
    all_suggestions.extend(sentence_analysis.get("suggestions", []))
    all_suggestions.extend(ngram_analysis.get("suggestions", []))
    
    # Zbierz wszystkie warnings
    all_warnings = humanness.get("warnings", [])
    if forbidden["status"] != "OK":
        all_warnings.append(forbidden["message"])
    if sentence_analysis["status"] != "OK":
        all_warnings.append(sentence_analysis["message"])
    if ngram_analysis["status"] != "OK":
        all_warnings.append(ngram_analysis["message"])
    
    return {
        "overall_status": overall_status,
        "humanness_score": humanness["humanness_score"],
        
        # Podstawowe metryki
        "components": humanness["components"],
        
        # Walidacje (Faza 1)
        "validations": {
            "forbidden_phrases": forbidden,
            "jitter": jitter,
            "triplets": triplets
        },
        
        # Faza 2
        "entity_split": entity_split,
        "topic_completeness": topic_completeness,
        
        # Faza 3
        "sentence_analysis": {
            "status": sentence_analysis["status"],
            "avg_score": sentence_analysis["avg_score"],
            "ai_like_count": sentence_analysis["ai_like_count"],
            "worst_sentences": sentence_analysis["worst_sentences"][:5]
        },
        "ngram_analysis": ngram_analysis,
        
        # Podsumowanie
        "warnings": all_warnings[:10],
        "suggestions": all_suggestions[:10]
    }
