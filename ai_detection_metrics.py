"""
===============================================================================
ü§ñ AI DETECTION METRICS v35.0
===============================================================================
Modu≈Ç do wykrywania tekstu wygenerowanego przez AI.

Metryki:
- Burstiness (zmienno≈õƒá d≈Çugo≈õci zda≈Ñ)
- Vocabulary Richness (TTR - Type-Token Ratio)
- Lexical Sophistication (≈õrednia czƒôsto≈õƒá s≈Ç√≥w - Zipf)
- Starter Entropy (r√≥≈ºnorodno≈õƒá poczƒÖtk√≥w zda≈Ñ)
- Word Repetition (powt√≥rzenia s≈Ç√≥w)
- Sentence Distribution (rozk≈Çad d≈Çugo≈õci zda≈Ñ) üÜï

Humanness Score = ≈õrednia wa≈ºona wszystkich metryk (0-100)

CRITICAL validations:
- Forbidden phrases check (BLOKUJE batch!)
- Burstiness < 1.5 (CV < 0.3 - BLOKUJE batch!)
- JITTER validation
- Triplets validation
- Word repetition > 8√ó (BLOKUJE batch!)

v35.0 CHANGES (zgodnie z dokumentem f.pdf - badania NKJP):
- Progi burstiness zgodne z badaniami: CV 0.3 = AI, CV 0.5 = ludzki
- Nowe progi rozk≈Çadu zda≈Ñ: kr√≥tkie 20-25%, ≈õrednie 50-60%, d≈Çugie 15-25%
- Kr√≥tkie zdania: 2-10 s≈Ç√≥w (by≈Ço 5-8)
- Dodano wykrywanie wzorca AI (koncentracja w przedziale 15-22 s≈Ç√≥w)
- Dodano warto≈õƒá CV w komunikatach diagnostycznych
- Zwiƒôkszona waga burstiness (0.30 vs 0.25) jako kluczowy marker AI
- Nowa metryka: sentence_distribution score

v33.0 CHANGES:
- Rozszerzono SHORT_INSERTS_LIBRARY (29 wtrƒÖce≈Ñ)
- Rozszerzono SYNONYM_MAP (27 s≈Ç√≥w) + dynamiczne z synonym_service
- Nowe progi: burstiness < 1.5 = CRITICAL, < 2.0 = WARNING
- Dodano fix_instructions z konkretnymi przyk≈Çadami
- Dodano analyze_sentence_distribution, generate_burstiness_fix
- Integracja z synonym_service.py
===============================================================================
"""

import re
import math
import statistics
from collections import Counter
from typing import Dict, List, Any, Tuple
from enum import Enum

# ================================================================
# üì¶ Opcjonalny import wordfreq
# ================================================================
try:
    from wordfreq import zipf_frequency
    WORDFREQ_AVAILABLE = True
    print("[AI_DETECTION] ‚úÖ wordfreq available")
except ImportError:
    WORDFREQ_AVAILABLE = False
    print("[AI_DETECTION] ‚ö†Ô∏è wordfreq not available - lexical sophistication disabled")

# ================================================================
# üì¶ v33.3: Opcjonalny import spacy dla POS diversity
# ================================================================
try:
    import spacy
    try:
        _nlp_pos = spacy.load("pl_core_news_sm")
        SPACY_POS_AVAILABLE = True
        print("[AI_DETECTION] ‚úÖ spacy pl_core_news_sm loaded for POS analysis")
    except OSError:
        SPACY_POS_AVAILABLE = False
        print("[AI_DETECTION] ‚ö†Ô∏è spacy pl_core_news_sm not found - POS diversity disabled")
except ImportError:
    SPACY_POS_AVAILABLE = False
    print("[AI_DETECTION] ‚ö†Ô∏è spacy not available - POS diversity disabled")


# ================================================================
# üìä KONFIGURACJA
# ================================================================
class AIDetectionConfig:
    """
    Progi dla metryk AI detection.
    
    üÜï v35.0 - PROGI ZGODNE Z BADANIAMI NKJP (f.pdf):
    - Burstiness: CV > 0.5 = ludzki, CV < 0.3 = AI (formu≈Ça: burstiness = CV * 5)
    - TTR: 0.45-0.55 dla blog√≥w/SEO (surowy na 1000 s≈Ç√≥w)
    - Rozk≈Çad zda≈Ñ: 20-25% kr√≥tkich, 50-60% ≈õrednich, 15-25% d≈Çugich
    """
    
    # ================================================================
    # BURSTINESS - zgodnie z dokumentem f.pdf
    # Formu≈Ça: burstiness = (std / mean) * 5, czyli CV * 5
    # CV 0.3 = 1.5, CV 0.5 = 2.5, CV 0.7 = 3.5
    # ================================================================
    BURSTINESS_CRITICAL_LOW = 1.5   # CV 0.3 - pr√≥g AI (by≈Ço 2.0)
    BURSTINESS_WARNING_LOW = 2.0    # CV 0.4 - strefa neutralna (by≈Ço 2.8)
    BURSTINESS_OK_MIN = 2.5         # CV 0.5 - pr√≥g ludzkiego tekstu (by≈Ço 2.8)
    BURSTINESS_OK_MAX = 4.0         # CV 0.8 - g√≥rna granica OK (by≈Ço 4.2)
    BURSTINESS_WARNING_HIGH = 4.5   # CV 0.9 (by≈Ço 4.8)
    BURSTINESS_CRITICAL_HIGH = 5.0  # CV 1.0 - tekst chaotyczny (by≈Ço 4.8)
    
    # ================================================================
    # TTR (Type-Token Ratio) - zgodnie z dokumentem f.pdf
    # Polskie blogi/SEO: TTR surowy 0.45-0.55 (na 1000 s≈Ç√≥w)
    # ================================================================
    TTR_CRITICAL = 0.42   # by≈Ço 0.40
    TTR_WARNING = 0.48    # bez zmian
    TTR_OK = 0.55         # bez zmian
    
    # ================================================================
    # ROZK≈ÅAD ZDA≈É - zgodnie z dokumentem f.pdf (NOWE!)
    # Kr√≥tkie (2-10 s≈Ç√≥w): 20-25%
    # ≈örednie (12-18 s≈Ç√≥w): 50-60%
    # D≈Çugie (20-30 s≈Ç√≥w): 15-25%
    # ================================================================
    SHORT_SENTENCE_MIN_WORDS = 2
    SHORT_SENTENCE_MAX_WORDS = 10
    SHORT_SENTENCE_TARGET_PCT_MIN = 20
    SHORT_SENTENCE_TARGET_PCT_MAX = 25
    
    MEDIUM_SENTENCE_MIN_WORDS = 12
    MEDIUM_SENTENCE_MAX_WORDS = 18
    MEDIUM_SENTENCE_TARGET_PCT_MIN = 50
    MEDIUM_SENTENCE_TARGET_PCT_MAX = 60
    
    LONG_SENTENCE_MIN_WORDS = 20
    LONG_SENTENCE_MAX_WORDS = 30
    LONG_SENTENCE_TARGET_PCT_MIN = 15
    LONG_SENTENCE_TARGET_PCT_MAX = 25
    
    # ≈örednia d≈Çugo≈õƒá zdania: 10-18 s≈Ç√≥w dla blog√≥w/SEO
    AVG_SENTENCE_LENGTH_MIN = 10
    AVG_SENTENCE_LENGTH_MAX = 18
    AVG_SENTENCE_LENGTH_AI_MIN = 15  # AI typowo 15-22 monotonnie
    AVG_SENTENCE_LENGTH_AI_MAX = 22
    
    # ================================================================
    # Pozosta≈Çe metryki (bez zmian)
    # ================================================================
    # Lexical Sophistication (Zipf)
    ZIPF_CRITICAL = 5.5
    ZIPF_WARNING = 5.0
    ZIPF_OK = 4.5
    
    # Starter Entropy
    ENTROPY_CRITICAL = 0.50
    ENTROPY_WARNING = 0.65
    ENTROPY_OK = 0.75
    
    # Word Repetition
    REPETITION_OK = 5
    REPETITION_WARNING = 7
    REPETITION_CRITICAL = 8
    
    # Humanness Score
    HUMANNESS_CRITICAL = 50
    HUMANNESS_WARNING = 70
    
    # Wagi - üîß FIX v35.0: Zwiƒôkszona waga burstiness zgodnie z badaniami
    WEIGHTS = {
        "burstiness": 0.30,       # by≈Ço 0.25 - zwiƒôkszone, kluczowy marker AI
        "vocabulary": 0.15,
        "sophistication": 0.10,
        "entropy": 0.15,          # by≈Ço 0.20
        "repetition": 0.15,       # by≈Ço 0.20
        "pos_diversity": 0.10,
        "sentence_distribution": 0.05  # üÜï nowa metryka
    }


class Severity(Enum):
    OK = "OK"
    WARNING = "WARNING"
    CRITICAL = "CRITICAL"


# ================================================================
# üáµüá± POLSKIE STOP WORDS
# ================================================================
POLISH_STOP_WORDS = {
    "i", "w", "na", "z", "do", "≈ºe", "siƒô", "nie", "to", "o", "jak", 
    "ale", "co", "jest", "za", "po", "tak", "czy", "ju≈º", "od", "przez",
    "dla", "by", "byƒá", "a", "wiƒôc", "te≈º", "tylko", "lub", "oraz",
    "jego", "jej", "ich", "tym", "tego", "tej", "te", "ta", "ten",
    "kt√≥ry", "kt√≥ra", "kt√≥re", "kt√≥rych", "kt√≥rzy", "kt√≥rej",
    "mo≈ºe", "bardzo", "kiedy", "gdy", "tu", "tam", "teraz", "wtedy",
    "mnie", "mi", "ci", "ciƒô", "go", "mu", "jƒÖ", "je", "nas", "was", "im",
    "jednak", "jeszcze", "bƒôdzie", "by≈Çy", "by≈Ç", "by≈Ça", "by≈Ço",
    "sƒÖ", "bƒôdƒÖ", "majƒÖ", "ma", "mo≈ºna", "trzeba", "nale≈ºy"
}


# ================================================================
# üîß FUNKCJE POMOCNICZE
# ================================================================
def split_into_sentences(text: str) -> List[str]:
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª])', text)
    sentences = [s.strip() for s in sentences if len(s.strip().split()) >= 3]
    return sentences


def tokenize(text: str) -> List[str]:
    text = re.sub(r'<[^>]+>', ' ', text)
    text = text.lower()
    words = re.findall(r'\b[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈ºA-ZƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]+\b', text)
    return words


def tokenize_no_stopwords(text: str) -> List[str]:
    words = tokenize(text)
    return [w for w in words if w not in POLISH_STOP_WORDS]


# ================================================================
# üìä METRYKI
# ================================================================
def calculate_burstiness(text: str) -> Dict[str, Any]:
    """
    Oblicza burstiness (zmienno≈õƒá d≈Çugo≈õci zda≈Ñ).
    
    üÜï v35.0: Formu≈Ça: burstiness = (std / mean) * 5 = CV * 5
    Progi zgodne z dokumentem f.pdf (NKJP):
    - CV < 0.3 (burstiness < 1.5) = CRITICAL (AI)
    - CV 0.3-0.4 (burstiness 1.5-2.0) = WARNING
    - CV 0.5-0.8 (burstiness 2.5-4.0) = OK (ludzki)
    """
    sentences = split_into_sentences(text)
    
    if len(sentences) < 5:
        return {
            "value": 0,
            "cv": 0,
            "status": Severity.WARNING.value,
            "message": "Za ma≈Ço zda≈Ñ do analizy (min 5)",
            "sentence_count": len(sentences)
        }
    
    lengths = [len(s.split()) for s in sentences]
    mean_len = statistics.mean(lengths)
    std_len = statistics.stdev(lengths) if len(lengths) > 1 else 0
    
    # Wsp√≥≈Çczynnik zmienno≈õci (CV) i burstiness
    cv_value = std_len / mean_len if mean_len > 0 else 0
    burstiness = round(cv_value * 5, 2)  # burstiness = CV * 5
    
    config = AIDetectionConfig()
    if burstiness < config.BURSTINESS_CRITICAL_LOW:
        status = Severity.CRITICAL
        message = f"‚ö†Ô∏è SYGNA≈Å AI: burstiness {burstiness} (CV {cv_value:.2f} < 0.3). Dodaj kr√≥tkie zdania 2-10 s≈Ç√≥w."
    elif burstiness < config.BURSTINESS_WARNING_LOW:
        status = Severity.WARNING
        message = f"Strefa neutralna: burstiness {burstiness} (CV {cv_value:.2f} < 0.4). Dodaj wiƒôcej kr√≥tkich zda≈Ñ."
    elif burstiness < config.BURSTINESS_OK_MIN:
        status = Severity.WARNING
        message = f"Poni≈ºej optymalnego: burstiness {burstiness} (CV {cv_value:.2f} < 0.5). Zwiƒôksz zmienno≈õƒá."
    elif burstiness > config.BURSTINESS_CRITICAL_HIGH:
        status = Severity.CRITICAL
        message = f"Tekst chaotyczny: burstiness {burstiness} (CV {cv_value:.2f} > 1.0). Wyr√≥wnaj rytm."
    elif burstiness > config.BURSTINESS_WARNING_HIGH:
        status = Severity.WARNING
        message = f"Za du≈ºa zmienno≈õƒá: burstiness {burstiness} (CV {cv_value:.2f} > 0.9). Wyr√≥wnaj d≈Çugo≈õci."
    else:
        status = Severity.OK
        message = f"Burstiness OK: {burstiness} (CV {cv_value:.2f} w normie 0.5-0.8)"
    
    return {
        "value": burstiness,
        "cv": round(cv_value, 2),
        "status": status.value,
        "message": message,
        "sentence_count": len(sentences),
        "mean_length": round(mean_len, 1),
        "std_length": round(std_len, 1),
        "min_length": min(lengths),
        "max_length": max(lengths),
        "thresholds": {
            "critical_low": config.BURSTINESS_CRITICAL_LOW,
            "warning_low": config.BURSTINESS_WARNING_LOW,
            "ok_min": config.BURSTINESS_OK_MIN,
            "ok_max": config.BURSTINESS_OK_MAX,
            "warning_high": config.BURSTINESS_WARNING_HIGH,
            "critical_high": config.BURSTINESS_CRITICAL_HIGH
        }
    }


# ================================================================
# üÜï v33.3: POS DIVERSITY (r√≥≈ºnorodno≈õƒá czƒô≈õci mowy)
# ================================================================
def calculate_pos_diversity(text: str) -> Dict[str, Any]:
    """
    v33.3: Mierzy zr√≥≈ºnicowanie czƒô≈õci mowy na poczƒÖtku zda≈Ñ.
    
    AI czƒôsto zaczyna zdania od tych samych konstrukcji gramatycznych
    (np. "Warto..." - VERB, "Wa≈ºne jest..." - ADJ).
    
    Wysoka entropia POS = bardziej ludzki tekst.
    
    Returns:
        Dict z: value (0-1), status, first_pos_distribution
    """
    if not SPACY_POS_AVAILABLE:
        return {
            "value": 0.5,  # Neutral default
            "status": "DISABLED",
            "message": "spacy niedostƒôpny - POS analysis wy≈ÇƒÖczona",
            "enabled": False
        }
    
    try:
        doc = _nlp_pos(text)
        first_pos = []
        
        for sent in doc.sents:
            tokens = [t for t in sent if not t.is_punct and not t.is_space]
            if tokens:
                first_pos.append(tokens[0].pos_)
        
        if len(first_pos) < 5:
            return {
                "value": 0.5,
                "status": "WARNING",
                "message": "Za ma≈Ço zda≈Ñ do analizy POS (min 5)",
                "enabled": True,
                "sentence_count": len(first_pos)
            }
        
        # Oblicz entropiƒô Shannon'a dla POS
        counter = Counter(first_pos)
        total = len(first_pos)
        
        entropy = 0
        for count in counter.values():
            if count > 0:
                p = count / total
                entropy -= p * math.log2(p)
        
        # Normalizuj do 0-1
        unique_pos = len(counter)
        max_entropy = math.log2(unique_pos) if unique_pos > 1 else 1
        normalized = entropy / max_entropy if max_entropy > 0 else 0
        normalized = round(normalized, 2)
        
        # Status
        if normalized < 0.4:
            status = "CRITICAL"
            message = f"Monotonne konstrukcje gramatyczne (POS entropy {normalized})"
        elif normalized < 0.6:
            status = "WARNING"
            message = f"Niska r√≥≈ºnorodno≈õƒá gramatyczna"
        else:
            status = "OK"
            message = f"Dobra r√≥≈ºnorodno≈õƒá POS"
        
        # Top 3 najczƒôstsze POS
        top_pos = counter.most_common(3)
        
        return {
            "value": normalized,
            "status": status,
            "message": message,
            "enabled": True,
            "unique_pos_count": unique_pos,
            "total_sentences": total,
            "top_pos": [{"pos": p, "count": c, "percent": round(c/total*100)} for p, c in top_pos],
            "pos_distribution": dict(counter)
        }
        
    except Exception as e:
        print(f"[AI_DETECTION] POS analysis error: {e}")
        return {
            "value": 0.5,
            "status": "ERROR",
            "message": f"B≈ÇƒÖd analizy POS: {e}",
            "enabled": True
        }


# ================================================================
# üÜï v33.0: SHORT INSERTS LIBRARY (dla fix_instructions)
# ================================================================
SHORT_INSERTS_LIBRARY = [
    # Potwierdzenia (2-4 s≈Çowa)
    "To dzia≈Ça.",
    "Efekt? Natychmiastowy.",
    "Proste rozwiƒÖzanie.",
    "I to nie wszystko.",
    "Ale jest wiƒôcej.",
    "Sprawdzone.",
    "Nic trudnego.",
    "R√≥≈ºnica jest widoczna.",
    "Brzmi skomplikowanie? Nie jest.",
    "Warto spr√≥bowaƒá.",
    "Klucz do sukcesu.",
    "To podstawa.",
    "Efekt? Szybki.",
    "Proste, prawda?",
    "A co dalej?",
    "Dzia≈Ça od razu.",
    "Bez niespodzianek.",
    "Czas na konkrety.",
    "I tu zaczyna siƒô magia.",
    "Rezultat m√≥wi sam za siebie.",
    # Pytania retoryczne
    "Ale czy to wystarczy?",
    "Co dalej?",
    "Dlaczego to wa≈ºne?",
    "Jak to osiƒÖgnƒÖƒá?",
    "A mo≈ºe inaczej?",
    # Akcenty dramatyczne
    "Efekt.",
    "Rezultat?",
    "Prosto.",
    "Skutecznie.",
]


# ================================================================
# üÜï v33.0: ANALYZE SENTENCE DISTRIBUTION
# üîß v35.0: Progi zgodne z dokumentem f.pdf (NKJP)
# ================================================================
def analyze_sentence_distribution(text: str) -> Dict[str, Any]:
    """
    Analizuje rozk≈Çad d≈Çugo≈õci zda≈Ñ dla burstiness fix.
    
    üÜï v35.0 - Progi zgodne z badaniami NKJP (f.pdf):
    - Kr√≥tkie (2-10 s≈Ç√≥w): 20-25%
    - ≈örednie (12-18 s≈Ç√≥w): 50-60%  
    - D≈Çugie (20-30 s≈Ç√≥w): 15-25%
    """
    config = AIDetectionConfig()  # üîß FIX: Dodano brakujƒÖcƒÖ definicjƒô
    sentences = split_into_sentences(text)
    
    if len(sentences) < 3:
        return {
            "short_count": 0, "medium_count": 0, "long_count": 0,
            "total": len(sentences), "distribution": [0, 0, 0],
            "issues": ["Za ma≈Ço zda≈Ñ do analizy"]
        }
    
    lengths = [len(s.split()) for s in sentences]
    
    # üîß v35.0: Nowe progi zgodne z dokumentem f.pdf
    short = sum(1 for l in lengths if config.SHORT_SENTENCE_MIN_WORDS <= l <= config.SHORT_SENTENCE_MAX_WORDS)
    medium = sum(1 for l in lengths if config.MEDIUM_SENTENCE_MIN_WORDS <= l <= config.MEDIUM_SENTENCE_MAX_WORDS)
    long = sum(1 for l in lengths if config.LONG_SENTENCE_MIN_WORDS <= l <= config.LONG_SENTENCE_MAX_WORDS)
    very_long = sum(1 for l in lengths if l > config.LONG_SENTENCE_MAX_WORDS)  # >30 s≈Ç√≥w
    
    total = len(lengths)
    avg_length = sum(lengths) / total if total > 0 else 0
    
    distribution = [
        round(short / total * 100, 1),
        round(medium / total * 100, 1),
        round(long / total * 100, 1)
    ]
    
    issues = []
    
    # üîß v35.0: Walidacja zgodna z dokumentem f.pdf
    # Kr√≥tkie zdania: cel 20-25%
    if distribution[0] < config.SHORT_SENTENCE_TARGET_PCT_MIN:
        issues.append(f"Za ma≈Ço kr√≥tkich zda≈Ñ (2-10 s≈Ç√≥w): {distribution[0]}% vs cel {config.SHORT_SENTENCE_TARGET_PCT_MIN}-{config.SHORT_SENTENCE_TARGET_PCT_MAX}%")
    elif distribution[0] > config.SHORT_SENTENCE_TARGET_PCT_MAX:
        issues.append(f"Za du≈ºo kr√≥tkich zda≈Ñ: {distribution[0]}% vs cel {config.SHORT_SENTENCE_TARGET_PCT_MIN}-{config.SHORT_SENTENCE_TARGET_PCT_MAX}%")
    
    # ≈örednie zdania: cel 50-60% (NOWA WALIDACJA!)
    if distribution[1] < config.MEDIUM_SENTENCE_TARGET_PCT_MIN:
        issues.append(f"Za ma≈Ço ≈õrednich zda≈Ñ (12-18 s≈Ç√≥w): {distribution[1]}% vs cel {config.MEDIUM_SENTENCE_TARGET_PCT_MIN}-{config.MEDIUM_SENTENCE_TARGET_PCT_MAX}%")
    elif distribution[1] > config.MEDIUM_SENTENCE_TARGET_PCT_MAX:
        issues.append(f"Za du≈ºo ≈õrednich zda≈Ñ: {distribution[1]}% vs cel {config.MEDIUM_SENTENCE_TARGET_PCT_MIN}-{config.MEDIUM_SENTENCE_TARGET_PCT_MAX}%")
    
    # D≈Çugie zdania: cel 15-25%
    if distribution[2] < config.LONG_SENTENCE_TARGET_PCT_MIN:
        issues.append(f"Za ma≈Ço d≈Çugich zda≈Ñ (20-30 s≈Ç√≥w): {distribution[2]}% vs cel {config.LONG_SENTENCE_TARGET_PCT_MIN}-{config.LONG_SENTENCE_TARGET_PCT_MAX}%")
    elif distribution[2] > config.LONG_SENTENCE_TARGET_PCT_MAX:
        issues.append(f"Za du≈ºo d≈Çugich zda≈Ñ: {distribution[2]}% vs cel {config.LONG_SENTENCE_TARGET_PCT_MIN}-{config.LONG_SENTENCE_TARGET_PCT_MAX}%")
    
    # Ostrze≈ºenie o bardzo d≈Çugich zdaniach (>30 s≈Ç√≥w)
    if very_long > 0:
        issues.append(f"{very_long} zda≈Ñ >30 s≈Ç√≥w - mogƒÖ byƒá trudne w odbiorze")
    
    # Walidacja ≈õredniej d≈Çugo≈õci zdania
    if avg_length < config.AVG_SENTENCE_LENGTH_MIN:
        issues.append(f"≈örednia d≈Çugo≈õƒá zda≈Ñ za niska: {avg_length:.1f} vs cel {config.AVG_SENTENCE_LENGTH_MIN}-{config.AVG_SENTENCE_LENGTH_MAX}")
    elif avg_length > config.AVG_SENTENCE_LENGTH_MAX:
        issues.append(f"≈örednia d≈Çugo≈õƒá zda≈Ñ za wysoka: {avg_length:.1f} vs cel {config.AVG_SENTENCE_LENGTH_MIN}-{config.AVG_SENTENCE_LENGTH_MAX}")
    
    # Wykrywanie wzorca AI (monotonna d≈Çugo≈õƒá 15-22)
    ai_range_count = sum(1 for l in lengths if config.AVG_SENTENCE_LENGTH_AI_MIN <= l <= config.AVG_SENTENCE_LENGTH_AI_MAX)
    ai_concentration = ai_range_count / total * 100 if total > 0 else 0
    if ai_concentration > 60:
        issues.append(f"‚ö†Ô∏è WZORZEC AI: {ai_concentration:.0f}% zda≈Ñ w przedziale 15-22 s≈Ç√≥w (monotonia)")
    
    # Oblicz score rozk≈Çadu (0-100)
    distribution_score = 100
    if distribution[0] < config.SHORT_SENTENCE_TARGET_PCT_MIN:
        distribution_score -= (config.SHORT_SENTENCE_TARGET_PCT_MIN - distribution[0]) * 2
    if distribution[1] < config.MEDIUM_SENTENCE_TARGET_PCT_MIN:
        distribution_score -= (config.MEDIUM_SENTENCE_TARGET_PCT_MIN - distribution[1])
    if distribution[2] < config.LONG_SENTENCE_TARGET_PCT_MIN:
        distribution_score -= (config.LONG_SENTENCE_TARGET_PCT_MIN - distribution[2]) * 2
    distribution_score = max(0, min(100, distribution_score))
    
    return {
        "short_count": short,
        "medium_count": medium,
        "long_count": long,
        "very_long_count": very_long,
        "total": total,
        "avg_length": round(avg_length, 1),
        "distribution": distribution,
        "distribution_label": f"[{distribution[0]}% kr√≥tkich (2-10), {distribution[1]}% ≈õrednich (12-18), {distribution[2]}% d≈Çugich (20-30)]",
        "distribution_score": distribution_score,
        "ai_concentration": round(ai_concentration, 1),
        "issues": issues,
        "targets": {
            "short": f"{config.SHORT_SENTENCE_TARGET_PCT_MIN}-{config.SHORT_SENTENCE_TARGET_PCT_MAX}%",
            "medium": f"{config.MEDIUM_SENTENCE_TARGET_PCT_MIN}-{config.MEDIUM_SENTENCE_TARGET_PCT_MAX}%",
            "long": f"{config.LONG_SENTENCE_TARGET_PCT_MIN}-{config.LONG_SENTENCE_TARGET_PCT_MAX}%"
        }
    }


# ================================================================
# üÜï v33.0: GENERATE BURSTINESS FIX INSTRUCTION
# üîß v35.0: Progi zgodne z dokumentem f.pdf
# ================================================================
def generate_burstiness_fix(burstiness: float, sentence_distribution: Dict) -> Dict[str, Any]:
    """
    Generuje konkretne instrukcje naprawy burstiness.
    
    üîß v35.0: Nowy pr√≥g >= 2.5 (CV 0.5) zgodnie z badaniami NKJP
    """
    import random
    config = AIDetectionConfig()  # üîß FIX: Dodano brakujƒÖcƒÖ definicjƒô
    
    # üîß v35.0: Nowy pr√≥g zgodny z dokumentem (CV 0.5 = 2.5)
    if burstiness >= config.BURSTINESS_OK_MIN:
        return {"needed": False, "message": "Burstiness OK (‚â•2.5, CV ‚â•0.5)"}
    
    inserts = random.sample(SHORT_INSERTS_LIBRARY, min(3, len(SHORT_INSERTS_LIBRARY)))
    
    rewrite_examples = [
        {
            "before": "Witamina C wspomaga syntezƒô kolagenu, co poprawia elastyczno≈õƒá sk√≥ry.",
            "after": "Witamina C? Klucz do kolagenu. Wspomaga jego syntezƒô i poprawia elastyczno≈õƒá sk√≥ry ‚Äì efekt widaƒá ju≈º po kilku tygodniach."
        },
        {
            "before": "Suplementy diety zawierajƒÖ wiele cennych sk≈Çadnik√≥w od≈ºywczych.",
            "after": "Suplementy dzia≈ÇajƒÖ. ZawierajƒÖ sk≈Çadniki, kt√≥re wspierajƒÖ sk√≥rƒô od wewnƒÖtrz ‚Äì witaminy, minera≈Çy, antyoksydanty. Proste i skuteczne."
        }
    ]
    
    # Buduj fix_instruction bez backslashy w f-string
    quoted_inserts = ['"' + s + '"' for s in inserts]
    fix_instruction = "Dodaj kr√≥tkie zdania (2-10 s≈Ç√≥w): " + ", ".join(quoted_inserts)
    
    # Okre≈õl poziom problemu
    if burstiness < config.BURSTINESS_CRITICAL_LOW:
        severity = "CRITICAL"
        cv_value = burstiness / 5
        explanation = f"CV {cv_value:.2f} < 0.3 = silny sygna≈Ç AI"
    elif burstiness < config.BURSTINESS_WARNING_LOW:
        severity = "WARNING"
        cv_value = burstiness / 5
        explanation = f"CV {cv_value:.2f} < 0.4 = strefa neutralna/podejrzana"
    else:
        severity = "INFO"
        cv_value = burstiness / 5
        explanation = f"CV {cv_value:.2f} < 0.5 = blisko progu ludzkiego"
    
    return {
        "needed": True,
        "severity": severity,
        "burstiness": burstiness,
        "cv_value": round(cv_value, 2),
        "target": f"‚â• {config.BURSTINESS_OK_MIN} (CV ‚â•0.5)",
        "explanation": explanation,
        "fix_instruction": fix_instruction,
        "insert_suggestions": inserts,
        "rewrite_example": random.choice(rewrite_examples),
        "distribution": sentence_distribution.get("distribution_label", ""),
        "tip": "Wz√≥r wg NKJP: KR√ìTKIE (2-10 s≈Ç√≥w, 20-25%) + ≈öREDNIE (12-18 s≈Ç√≥w, 50-60%) + D≈ÅUGIE (20-30 s≈Ç√≥w, 15-25%)"
    }


# ================================================================
# üÜï v33.0: EXTENDED SYNONYM MAP
# ================================================================
SYNONYM_MAP = {
    # Sk√≥ra / uroda
    "sk√≥ra": ["cera", "nask√≥rek", "powierzchnia sk√≥ry", "tkanka", "pow≈Çoka"],
    "witamina": ["mikrosk≈Çadnik", "substancja od≈ºywcza", "sk≈Çadnik", "nutrient"],
    "suplement": ["preparat", "produkt", "≈õrodek", "wsparcie"],
    "kolagen": ["bia≈Çko strukturalne", "w≈Ç√≥kna kolagenowe", "substancja budulcowa"],
    "nawil≈ºenie": ["hydratacja", "uwodnienie", "poziom wilgoci"],
    # Przymiotniki
    "wa≈ºny": ["istotny", "znaczƒÖcy", "zasadniczy", "niezbƒôdny", "donios≈Çy"],
    "dobry": ["skuteczny", "warto≈õciowy", "korzystny", "efektywny", "pomocny"],
    "zdrowy": ["prawid≈Çowy", "w≈Ça≈õciwy", "optymalny"],
    "du≈ºy": ["znaczny", "spory", "poka≈∫ny", "niema≈Çy"],
    "ma≈Çy": ["niewielki", "drobny", "ograniczony"],
    "nowy": ["nowoczesny", "≈õwie≈ºy", "najnowszy", "aktualny"],
    # Czasowniki
    "poprawia": ["wspiera", "wzmacnia", "podnosi", "ulepsza"],
    "pomaga": ["wspiera", "u≈Çatwia", "wspomaga", "przyczynia siƒô"],
    "zawiera": ["posiada", "obejmuje", "ma w sk≈Çadzie"],
    "powoduje": ["wywo≈Çuje", "skutkuje", "prowadzi do"],
    "dzia≈Ça": ["funkcjonuje", "pracuje", "oddzia≈Çuje", "wp≈Çywa"],
    "chroni": ["zabezpiecza", "ochrania", "os≈Çania"],
    # Us≈Çugi / biznes
    "firma": ["przedsiƒôbiorstwo", "sp√≥≈Çka", "wykonawca", "us≈Çugodawca"],
    "us≈Çuga": ["≈õwiadczenie", "realizacja", "obs≈Çuga", "serwis"],
    "klient": ["zleceniodawca", "us≈Çugobiorca", "zamawiajƒÖcy"],
    "cena": ["koszt", "stawka", "wycena", "taryfa"],
    "profesjonalny": ["do≈õwiadczony", "wykwalifikowany", "fachowy"],
}


# ================================================================
# üÜï v33.0: CHECK WORD REPETITION DETAILED (z dynamicznymi synonimami)
# ================================================================
def check_word_repetition_detailed(text: str, max_per_500: int = 5) -> Dict[str, Any]:
    """
    Sprawdza powt√≥rzenia s≈Ç√≥w z dynamicznymi sugestiami synonim√≥w.
    """
    # Pr√≥ba dynamicznego importu synonym_service
    try:
        from synonym_service import get_synonyms
        use_dynamic = True
    except ImportError:
        use_dynamic = False
    
    words = re.findall(r'\b[a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]{4,}\b', text.lower())
    word_count = len(words)
    word_freq = Counter(words)
    
    stop_words = {'jest', 'oraz', 'jako', 'przez', 'kt√≥re', 'kt√≥ra', 'kt√≥ry', 
                  'mo≈ºe', 'bƒôdzie', 'by≈Ço', 'by≈Çy', 'tego', 'tej', 'tych',
                  'bardzo', 'tak≈ºe', 'r√≥wnie≈º', 'jednak', 'wiƒôc', 'czyli'}
    
    scale = max(1, word_count / 500)
    limit = int(max_per_500 * scale)
    
    violations = []
    warnings = []
    
    def _get_synonyms(word: str) -> List[str]:
        if use_dynamic:
            result = get_synonyms(word)
            return result.get("synonyms", [])
        return SYNONYM_MAP.get(word, [])
    
    for word, count in word_freq.most_common(20):
        if word in stop_words:
            continue
        
        if count > limit * 1.6:  # > 8√ó = CRITICAL
            synonyms = _get_synonyms(word)
            violations.append({
                "word": word, "count": count, "limit": limit,
                "synonyms": synonyms,
                "suggestion": f"U≈ºyj: {', '.join(synonyms[:3])}" if synonyms else "Znajd≈∫ synonimy"
            })
        elif count > limit:  # > 5√ó = WARNING
            synonyms = _get_synonyms(word)
            warnings.append({
                "word": word, "count": count, "limit": limit, "synonyms": synonyms
            })
    
    if violations:
        status = Severity.CRITICAL
        viol_str = ', '.join([f'{v["word"]}({v["count"]}√ó)' for v in violations[:3]])
        message = f"üî¥ POWT√ìRZENIA: {viol_str}"
        should_block = True
    elif warnings:
        status = Severity.WARNING
        warn_str = ', '.join([f'{w["word"]}({w["count"]}√ó)' for w in warnings[:3]])
        message = f"‚ö†Ô∏è Powt√≥rzenia: {warn_str}"
        should_block = False
    else:
        status = Severity.OK
        message = "Powt√≥rzenia OK ‚úì"
        should_block = False
    
    top_words = [(w, c) for w, c in word_freq.most_common(10) if w not in stop_words][:5]
    
    return {
        "status": status.value,
        "violations": violations,
        "warnings": warnings,
        "message": message,
        "top_words": top_words,
        "should_block": should_block
    }


def calculate_vocabulary_richness(text: str) -> Dict[str, Any]:
    words = tokenize_no_stopwords(text)
    
    if len(words) < 50:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Za ma≈Ço s≈Ç√≥w do analizy (min 50)",
            "word_count": len(words)
        }
    
    unique_words = set(words)
    ttr = len(unique_words) / len(words)
    ttr = round(ttr, 3)
    
    config = AIDetectionConfig()
    if ttr < config.TTR_CRITICAL:
        status = Severity.CRITICAL
        message = f"Bardzo ubogi zas√≥b s≈Ç√≥w (TTR {ttr} < {config.TTR_CRITICAL})"
    elif ttr < config.TTR_WARNING:
        status = Severity.WARNING
        message = f"Ma≈Ço urozmaicone s≈Çownictwo. U≈ºyj synonim√≥w."
    elif ttr >= config.TTR_OK:
        status = Severity.OK
        message = "Bogate s≈Çownictwo"
    else:
        status = Severity.WARNING
        message = "S≈Çownictwo poni≈ºej optimum"
    
    return {
        "value": ttr,
        "status": status.value,
        "message": message,
        "unique_words": len(unique_words),
        "total_words": len(words)
    }


def calculate_lexical_sophistication(text: str) -> Dict[str, Any]:
    if not WORDFREQ_AVAILABLE:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "wordfreq niedostƒôpny",
            "available": False
        }
    
    words = tokenize_no_stopwords(text)
    
    if len(words) < 50:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Za ma≈Ço s≈Ç√≥w do analizy",
            "available": True
        }
    
    zipf_scores = []
    for word in words:
        freq = zipf_frequency(word, 'pl')
        if freq > 0:
            zipf_scores.append(freq)
    
    if not zipf_scores:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Nie uda≈Ço siƒô obliczyƒá czƒôsto≈õci s≈Ç√≥w",
            "available": True
        }
    
    avg_zipf = statistics.mean(zipf_scores)
    avg_zipf = round(avg_zipf, 2)
    
    config = AIDetectionConfig()
    if avg_zipf > config.ZIPF_CRITICAL:
        status = Severity.CRITICAL
        message = f"Zbyt proste s≈Çownictwo (avg Zipf {avg_zipf} > {config.ZIPF_CRITICAL})"
    elif avg_zipf > config.ZIPF_WARNING:
        status = Severity.WARNING
        message = f"S≈Çownictwo do≈õƒá podstawowe"
    elif avg_zipf <= config.ZIPF_OK:
        status = Severity.OK
        message = "Dobry mix s≈Çownictwa"
    else:
        status = Severity.WARNING
        message = "S≈Çownictwo w normie"
    
    return {
        "value": avg_zipf,
        "status": status.value,
        "message": message,
        "words_analyzed": len(zipf_scores),
        "available": True
    }


def calculate_starter_entropy(text: str) -> Dict[str, Any]:
    sentences = split_into_sentences(text)
    
    if len(sentences) < 5:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Za ma≈Ço zda≈Ñ do analizy",
            "sentence_count": len(sentences)
        }
    
    starters = []
    for s in sentences:
        words = s.split()[:3]
        if words:
            starter = ' '.join(words).lower()
            starters.append(starter)
    
    if not starters:
        return {
            "value": 0,
            "status": Severity.WARNING.value,
            "message": "Nie znaleziono starter√≥w"
        }
    
    counter = Counter(starters)
    total = len(starters)
    
    entropy = 0
    for count in counter.values():
        p = count / total
        if p > 0:
            entropy -= p * math.log2(p)
    
    max_entropy = math.log2(total) if total > 1 else 1
    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
    normalized_entropy = round(normalized_entropy, 2)
    
    repetitive = {k: v for k, v in counter.items() if v >= 2}
    
    config = AIDetectionConfig()
    if normalized_entropy < config.ENTROPY_CRITICAL:
        status = Severity.CRITICAL
        message = f"Bardzo powtarzalne poczƒÖtki zda≈Ñ (entropy {normalized_entropy})"
    elif normalized_entropy < config.ENTROPY_WARNING:
        status = Severity.WARNING
        message = f"Ma≈Ço r√≥≈ºnorodne poczƒÖtki zda≈Ñ"
    elif normalized_entropy >= config.ENTROPY_OK:
        status = Severity.OK
        message = "Dobra r√≥≈ºnorodno≈õƒá poczƒÖtk√≥w zda≈Ñ"
    else:
        status = Severity.WARNING
        message = "R√≥≈ºnorodno≈õƒá starter√≥w poni≈ºej optimum"
    
    suggestions = []
    for starter, count in sorted(repetitive.items(), key=lambda x: -x[1])[:3]:
        suggestions.append(f"Zmie≈Ñ starter '{starter}' (u≈ºyty {count}√ó)")
    
    return {
        "value": normalized_entropy,
        "status": status.value,
        "message": message,
        "unique_starters": len(counter),
        "total_sentences": len(sentences),
        "repetitive_starters": repetitive,
        "suggestions": suggestions
    }


def calculate_word_repetition(text: str) -> Dict[str, Any]:
    words = tokenize_no_stopwords(text)
    
    if len(words) < 50:
        return {
            "value": 1.0,
            "status": Severity.WARNING.value,
            "message": "Za ma≈Ço s≈Ç√≥w do analizy",
            "word_count": len(words)
        }
    
    counter = Counter(words)
    config = AIDetectionConfig()
    
    overused = {}
    warnings_list = []
    critical_list = []
    
    for word, count in counter.most_common(20):
        if count > config.REPETITION_CRITICAL:
            critical_list.append({"word": word, "count": count})
            overused[word] = count
        elif count > config.REPETITION_WARNING:
            warnings_list.append({"word": word, "count": count})
            overused[word] = count
        elif count > config.REPETITION_OK:
            warnings_list.append({"word": word, "count": count})
    
    overused_count = sum(overused.values())
    score = 1 - (overused_count / len(words)) if words else 1
    score = round(max(0, score), 2)
    
    if critical_list:
        status = Severity.CRITICAL
        message = f"S≈Çowa powt√≥rzone > {config.REPETITION_CRITICAL}√ó: {', '.join([c['word'] for c in critical_list[:3]])}"
    elif warnings_list:
        status = Severity.WARNING
        message = f"S≈Çowa powt√≥rzone > {config.REPETITION_OK}√ó. U≈ºyj synonim√≥w."
    else:
        status = Severity.OK
        message = "Brak nadmiernych powt√≥rze≈Ñ"
    
    # üîß FIX v34.3: Usuniƒôto lokalnƒÖ SYNONYM_MAP - u≈ºywamy globalnej (27 s≈Ç√≥w)
    suggestions = []
    for word in overused:
        if word in SYNONYM_MAP:  # U≈ºywa globalnej SYNONYM_MAP z linii 431
            suggestions.append(f"'{word}' ‚Üí {', '.join(SYNONYM_MAP[word][:3])}")
    
    return {
        "value": score,
        "status": status.value,
        "message": message,
        "overused_words": overused,
        "critical_words": critical_list,
        "warning_words": warnings_list,
        "suggestions": suggestions[:5]
    }


# ================================================================
# üéØ G≈Å√ìWNA FUNKCJA - HUMANNESS SCORE
# ================================================================
def calculate_humanness_score(text: str) -> Dict[str, Any]:
    config = AIDetectionConfig()
    
    burstiness = calculate_burstiness(text)
    vocabulary = calculate_vocabulary_richness(text)
    sophistication = calculate_lexical_sophistication(text)
    entropy = calculate_starter_entropy(text)
    repetition = calculate_word_repetition(text)
    
    # v33.3: POS diversity
    pos_diversity = calculate_pos_diversity(text)
    
    def normalize_burstiness(val):
        if val < config.BURSTINESS_CRITICAL_LOW:
            return 0.0
        elif val < config.BURSTINESS_OK_MIN:
            return (val - config.BURSTINESS_CRITICAL_LOW) / (config.BURSTINESS_OK_MIN - config.BURSTINESS_CRITICAL_LOW) * 0.5
        elif val <= config.BURSTINESS_OK_MAX:
            return 1.0
        elif val < config.BURSTINESS_CRITICAL_HIGH:
            return 1.0 - (val - config.BURSTINESS_OK_MAX) / (config.BURSTINESS_CRITICAL_HIGH - config.BURSTINESS_OK_MAX) * 0.5
        else:
            return 0.0
    
    def normalize_ttr(val):
        if val >= config.TTR_OK:
            return 1.0
        elif val >= config.TTR_WARNING:
            return 0.5 + (val - config.TTR_WARNING) / (config.TTR_OK - config.TTR_WARNING) * 0.5
        elif val >= config.TTR_CRITICAL:
            return (val - config.TTR_CRITICAL) / (config.TTR_WARNING - config.TTR_CRITICAL) * 0.5
        else:
            return 0.0
    
    def normalize_zipf(val):
        if not WORDFREQ_AVAILABLE or val == 0:
            return 0.5
        if val <= config.ZIPF_OK:
            return 1.0
        elif val <= config.ZIPF_WARNING:
            return 0.5 + (config.ZIPF_WARNING - val) / (config.ZIPF_WARNING - config.ZIPF_OK) * 0.5
        elif val <= config.ZIPF_CRITICAL:
            return (config.ZIPF_CRITICAL - val) / (config.ZIPF_CRITICAL - config.ZIPF_WARNING) * 0.5
        else:
            return 0.0
    
    def normalize_entropy(val):
        if val >= config.ENTROPY_OK:
            return 1.0
        elif val >= config.ENTROPY_WARNING:
            return 0.5 + (val - config.ENTROPY_WARNING) / (config.ENTROPY_OK - config.ENTROPY_WARNING) * 0.5
        elif val >= config.ENTROPY_CRITICAL:
            return (val - config.ENTROPY_CRITICAL) / (config.ENTROPY_WARNING - config.ENTROPY_CRITICAL) * 0.5
        else:
            return 0.0
    
    # v33.3: Normalize POS diversity (same scale as entropy)
    def normalize_pos(val):
        if not SPACY_POS_AVAILABLE or val == 0:
            return 0.5  # Neutral if disabled
        if val >= 0.6:
            return 1.0
        elif val >= 0.4:
            return 0.5 + (val - 0.4) / 0.2 * 0.5
        else:
            return val / 0.4 * 0.5
    
    scores = {
        "burstiness": normalize_burstiness(burstiness.get("value", 0)),
        "vocabulary": normalize_ttr(vocabulary.get("value", 0)),
        "sophistication": normalize_zipf(sophistication.get("value", 0)),
        "entropy": normalize_entropy(entropy.get("value", 0)),
        "repetition": repetition.get("value", 1.0),
        "pos_diversity": normalize_pos(pos_diversity.get("value", 0.5))  # v33.3
    }
    
    # üîß FIX v34.3: U≈ºywamy wag z konfiguracji (jedno ≈∫r√≥d≈Ço prawdy)
    weights = config.WEIGHTS
    
    humanness = sum(scores[k] * weights.get(k, 0) for k in scores)
    humanness_score = round(humanness * 100, 0)
    
    if humanness_score < config.HUMANNESS_CRITICAL:
        status = Severity.CRITICAL
        overall_message = f"CRITICAL: Tekst wyglƒÖda na AI (score {humanness_score}). Przepisz!"
    elif humanness_score < config.HUMANNESS_WARNING:
        status = Severity.WARNING
        overall_message = f"WARNING: Tekst wymaga poprawy (score {humanness_score})"
    else:
        status = Severity.OK
        overall_message = f"OK: Tekst wyglƒÖda naturalnie (score {humanness_score})"
    
    all_warnings = []
    if burstiness.get("status") != "OK":
        all_warnings.append(burstiness.get("message"))
    if vocabulary.get("status") != "OK":
        all_warnings.append(vocabulary.get("message"))
    if sophistication.get("status") not in ["OK", "WARNING"] or sophistication.get("value", 0) > config.ZIPF_WARNING:
        all_warnings.append(sophistication.get("message"))
    if entropy.get("status") != "OK":
        all_warnings.append(entropy.get("message"))
    if repetition.get("status") != "OK":
        all_warnings.append(repetition.get("message"))
    # v33.3: POS diversity warnings
    if pos_diversity.get("status") not in ["OK", "DISABLED"] and pos_diversity.get("enabled", True):
        all_warnings.append(pos_diversity.get("message"))
    
    all_suggestions = []
    all_suggestions.extend(entropy.get("suggestions", []))
    all_suggestions.extend(repetition.get("suggestions", []))
    
    return {
        "humanness_score": int(humanness_score),
        "status": status.value,
        "message": overall_message,
        "components": {
            "burstiness": burstiness,
            "vocabulary_richness": vocabulary,
            "lexical_sophistication": sophistication,
            "starter_entropy": entropy,
            "word_repetition": repetition,
            "pos_diversity": pos_diversity  # v33.3
        },
        "normalized_scores": scores,
        "warnings": all_warnings[:5],
        "suggestions": all_suggestions[:5]
    }


# ================================================================
# üîç QUICK CHECK
# ================================================================
def quick_ai_check(text: str) -> Dict[str, Any]:
    burstiness = calculate_burstiness(text)
    humanness = calculate_humanness_score(text)
    
    return {
        "humanness_score": humanness["humanness_score"],
        "status": humanness["status"],
        "burstiness": burstiness["value"],
        "top_warning": humanness["warnings"][0] if humanness["warnings"] else None
    }


# ================================================================
# üÜï v33.0: CRITICAL: FORBIDDEN PHRASES CHECK (rozszerzono!)
# ================================================================
FORBIDDEN_PATTERNS = [
    # Frazy typowe dla AI
    (r'\bwarto wiedzieƒá\b', "warto wiedzieƒá"),
    (r'\bnale≈ºy pamiƒôtaƒá\b', "nale≈ºy pamiƒôtaƒá"),
    (r'\bnale≈ºy podkre≈õliƒá\b', "nale≈ºy podkre≈õliƒá"),
    (r'\bkluczowy aspekt\b', "kluczowy aspekt"),
    (r'\bkompleksowe rozwiƒÖzanie\b', "kompleksowe rozwiƒÖzanie"),
    (r'\bholistyczne podej≈õcie\b', "holistyczne podej≈õcie"),
    (r'\bw dzisiejszych czasach\b', "w dzisiejszych czasach"),
    (r'\bnie ulega wƒÖtpliwo≈õci\b', "nie ulega wƒÖtpliwo≈õci"),
    (r'\bcoraz wiƒôcej os√≥b\b', "coraz wiƒôcej os√≥b"),
    (r'\bw tym artykule\b', "w tym artykule"),
    (r'\bpodsumowujƒÖc\b', "podsumowujƒÖc"),
    (r'\bjak ju≈º wspomniano\b', "jak ju≈º wspomniano"),
    (r'\bka≈ºdy z nas\b', "ka≈ºdy z nas"),
    (r'\bnie jest tajemnicƒÖ\b', "nie jest tajemnicƒÖ"),
    (r'\bpowszechnie wiadomo\b', "powszechnie wiadomo"),
    (r'\btrudno przeceniƒá\b', "trudno przeceniƒá"),
    (r'\bw erze\s+\w+\b', "w erze..."),
    (r'\bw dobie\s+\w+\b', "w dobie..."),
    (r'\bw obliczu\b', "w obliczu"),
    (r'\bna przestrzeni lat\b', "na przestrzeni lat"),
]

# üÜï v33.0: S≈Çowa zakazane (pojedyncze)
FORBIDDEN_WORDS = [
    "kluczowy", "kompleksowy", "innowacyjny", "holistyczny", 
    "transformacyjny", "fundamentalny", "niewƒÖtpliwie", "wieloaspektowy",
    "prze≈Çomowy", "bezsprzecznie", "rewolucyjny", "optymalizowaƒá"
]

# üÜï v33.0: Replacements dla zakazanych fraz
FORBIDDEN_REPLACEMENTS = {
    "coraz wiƒôcej os√≥b": "wiele os√≥b",
    "w dzisiejszych czasach": "[USU≈É]",
    "warto wiedzieƒá": "[USU≈É]",
    "nale≈ºy podkre≈õliƒá": "[USU≈É]",
    "podsumowujƒÖc": "[zamie≈Ñ na konkretne zako≈Ñczenie]",
    "w tym artykule": "[NIGDY nie u≈ºywaj]",
    "kluczowy": "istotny/wa≈ºny",
    "kompleksowy": "pe≈Çny/ca≈Ço≈õciowy",
    "innowacyjny": "nowoczesny/nowatorski",
    "holistyczny": "ca≈Ço≈õciowy",
}

def check_forbidden_phrases(text: str) -> Dict[str, Any]:
    """
    üÜï v33.0: Sprawdza zakazane frazy i s≈Çowa.
    Zwraca should_block=True je≈õli znaleziono ‚â•1 frazƒô!
    """
    text_lower = text.lower()
    found_phrases = []
    found_words = []
    replacements = []
    
    # Sprawd≈∫ frazy
    for pattern, name in FORBIDDEN_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            found_phrases.append(name)
            if name in FORBIDDEN_REPLACEMENTS:
                replacements.append(f"'{name}' ‚Üí {FORBIDDEN_REPLACEMENTS[name]}")
    
    # Sprawd≈∫ pojedyncze s≈Çowa
    for word in FORBIDDEN_WORDS:
        if re.search(rf'\b{word}\b', text_lower, re.IGNORECASE):
            found_words.append(word)
            if word in FORBIDDEN_REPLACEMENTS:
                replacements.append(f"'{word}' ‚Üí {FORBIDDEN_REPLACEMENTS[word]}")
    
    all_found = found_phrases + found_words
    
    if all_found:
        # üî¥ v33.0: BLOKUJ je≈õli znaleziono zakazane frazy!
        status = Severity.CRITICAL
        message = f"üö´ ZAKAZANE FRAZY ({len(all_found)}√ó): {', '.join(all_found[:5])}"
        should_block = True
    else:
        status = Severity.OK
        message = "Brak zakazanych fraz ‚úì"
        should_block = False
    
    return {
        "status": status.value,
        "forbidden_found": all_found,
        "phrases": found_phrases,
        "words": found_words,
        "count": len(all_found),
        "message": message,
        "replacements": replacements,
        "should_block": should_block
    }


# ================================================================
# üÜï CRITICAL: JITTER VALIDATION
# ================================================================
def validate_jitter(current_paragraphs: int, previous_paragraphs: int = None) -> Dict[str, Any]:
    if previous_paragraphs is None:
        return {
            "status": Severity.OK.value,
            "message": "Pierwszy batch - JITTER OK",
            "current": current_paragraphs,
            "previous": None
        }
    
    if current_paragraphs == previous_paragraphs:
        return {
            "status": Severity.WARNING.value,
            "message": f"JITTER fail: {current_paragraphs}ak = poprzedni ({previous_paragraphs}ak). Zmie≈Ñ liczbƒô akapit√≥w!",
            "current": current_paragraphs,
            "previous": previous_paragraphs
        }
    
    return {
        "status": Severity.OK.value,
        "message": f"JITTER OK: {current_paragraphs}ak ‚â† {previous_paragraphs}ak",
        "current": current_paragraphs,
        "previous": previous_paragraphs
    }


# ================================================================
# üÜï CRITICAL: TRIPLETS VALIDATION
# ================================================================
def validate_triplets(text: str, s1_relationships: List[Dict]) -> Dict[str, Any]:
    if not s1_relationships:
        return {
            "status": Severity.OK.value,
            "message": "Brak triplet√≥w z S1 do sprawdzenia",
            "found": 0,
            "expected": 0
        }
    
    text_lower = text.lower()
    found = []
    
    for rel in s1_relationships:
        subject = rel.get("subject", "").lower()
        predicate = rel.get("predicate", "").lower()
        obj = rel.get("object", "").lower()
        
        if subject and predicate and obj:
            if subject in text_lower and predicate in text_lower and obj in text_lower:
                found.append(rel)
    
    expected = min(3, len(s1_relationships))
    
    if len(found) >= 2:
        status = Severity.OK
        message = f"Znaleziono {len(found)} triplet√≥w (min 2)"
    elif len(found) == 1:
        status = Severity.WARNING
        message = f"Tylko 1 triplet znaleziony (min 2)"
    else:
        status = Severity.WARNING
        message = f"Brak triplet√≥w z S1 (min 2)"
    
    return {
        "status": status.value,
        "message": message,
        "found": len(found),
        "expected": expected,
        "triplets_found": found[:5]
    }


# ================================================================
# üéØ FULL AI DETECTION (z CRITICAL validations)
# ================================================================
def full_ai_detection(
    text: str, 
    previous_paragraphs: int = None,
    s1_relationships: List[Dict] = None
) -> Dict[str, Any]:
    """
    Pe≈Çna analiza AI detection + walidacje CRITICAL.
    """
    humanness = calculate_humanness_score(text)
    forbidden = check_forbidden_phrases(text)
    
    current_paragraphs = len(re.split(r'\n\s*\n', text.strip()))
    jitter = validate_jitter(current_paragraphs, previous_paragraphs)
    
    triplets = validate_triplets(text, s1_relationships or [])
    
    statuses = [
        humanness["status"],
        forbidden["status"],
        jitter["status"],
        triplets["status"]
    ]
    
    if "CRITICAL" in statuses:
        overall_status = Severity.CRITICAL.value
    elif "WARNING" in statuses:
        overall_status = Severity.WARNING.value
    else:
        overall_status = Severity.OK.value
    
    all_warnings = humanness.get("warnings", [])
    if forbidden["status"] != "OK":
        all_warnings.append(forbidden["message"])
    if jitter["status"] != "OK":
        all_warnings.append(jitter["message"])
    if triplets["status"] != "OK":
        all_warnings.append(triplets["message"])
    
    return {
        "humanness_score": humanness["humanness_score"],
        "status": overall_status,
        "components": humanness["components"],
        "validations": {
            "forbidden_phrases": forbidden,
            "jitter": jitter,
            "triplets": triplets
        },
        "warnings": all_warnings[:7],
        "suggestions": humanness.get("suggestions", [])[:5]
    }


# ================================================================
# üÜï FAZA 2: ENTITY SPLIT 60/40
# ================================================================
def calculate_entity_split(text: str, s1_entities: List[Dict]) -> Dict[str, Any]:
    """
    Oblicza proporcjƒô Core vs Supporting entities.
    
    Cel: 60% Core, 40% Supporting
    """
    if not s1_entities:
        return {
            "status": "NO_DATA",
            "message": "Brak danych o encjach z S1",
            "core_ratio": 0,
            "supporting_ratio": 0
        }
    
    text_lower = text.lower()
    
    # Rozdziel encje na Core i Supporting
    core_entities = []
    supporting_entities = []
    
    for e in s1_entities:
        category = e.get("category", "").upper()
        importance = e.get("importance", 0.5)
        
        # Je≈õli brak category, u≈ºyj importance do klasyfikacji
        if category == "CORE" or (not category and importance >= 0.6):
            core_entities.append(e)
        elif category == "SUPPORTING" or (not category and importance < 0.6):
            supporting_entities.append(e)
        else:
            # Domy≈õlnie jako supporting
            supporting_entities.append(e)
    
    # Zlicz znalezione
    core_found = 0
    supporting_found = 0
    core_used = []
    supporting_used = []
    
    for e in core_entities:
        name = e.get("name", e.get("text", "")).lower()
        if name and name in text_lower:
            core_found += 1
            core_used.append(name)
    
    for e in supporting_entities:
        name = e.get("name", e.get("text", "")).lower()
        if name and name in text_lower:
            supporting_found += 1
            supporting_used.append(name)
    
    total_found = core_found + supporting_found
    
    if total_found == 0:
        return {
            "status": "WARNING",
            "message": "Nie znaleziono ≈ºadnych encji w tek≈õcie",
            "core_ratio": 0,
            "supporting_ratio": 0,
            "core_found": 0,
            "supporting_found": 0
        }
    
    core_ratio = core_found / total_found
    supporting_ratio = supporting_found / total_found
    
    # Status: OK je≈õli core_ratio miƒôdzy 0.55 a 0.65
    if 0.55 <= core_ratio <= 0.65:
        status = "OK"
        message = f"Entity split OK: {core_ratio:.0%} core / {supporting_ratio:.0%} supporting"
    elif core_ratio > 0.65:
        status = "WARNING"
        message = f"Za du≈ºo Core entities ({core_ratio:.0%}). Dodaj wiƒôcej Supporting (ubezpieczenie, certyfikaty, normy)"
    else:
        status = "WARNING"
        message = f"Za ma≈Ço Core entities ({core_ratio:.0%}). Dodaj wiƒôcej Core (g≈Ç√≥wne tematy)"
    
    return {
        "status": status,
        "message": message,
        "core_ratio": round(core_ratio, 2),
        "supporting_ratio": round(supporting_ratio, 2),
        "core_found": core_found,
        "supporting_found": supporting_found,
        "core_total": len(core_entities),
        "supporting_total": len(supporting_entities),
        "core_used": core_used[:10],
        "supporting_used": supporting_used[:10]
    }


# ================================================================
# üÜï FAZA 2: TOPIC COMPLETENESS
# ================================================================
def calculate_topic_completeness(text: str, s1_topics: List[Dict]) -> Dict[str, Any]:
    """
    Oblicza pokrycie temat√≥w z S1.
    """
    if not s1_topics:
        return {
            "status": "NO_DATA",
            "score": 0,
            "message": "Brak danych o tematach z S1"
        }
    
    text_lower = text.lower()
    
    # Rozdziel tematy wed≈Çug priorytetu
    must_topics = []
    high_topics = []
    medium_topics = []
    
    for t in s1_topics:
        priority = t.get("priority", "MEDIUM").upper()
        if priority == "MUST":
            must_topics.append(t)
        elif priority == "HIGH":
            high_topics.append(t)
        else:
            medium_topics.append(t)
    
    # Sprawd≈∫ pokrycie
    def check_topic_covered(topic):
        name = topic.get("name", topic.get("subtopic", "")).lower()
        keywords = topic.get("keywords", [])
        
        # Sprawd≈∫ nazwƒô
        if name and name in text_lower:
            return True
        
        # Sprawd≈∫ s≈Çowa kluczowe
        for kw in keywords:
            if kw.lower() in text_lower:
                return True
        
        # Sprawd≈∫ sample_h2 je≈õli istnieje
        sample_h2 = topic.get("sample_h2", "").lower()
        if sample_h2:
            words = sample_h2.split()
            matches = sum(1 for w in words if len(w) > 3 and w in text_lower)
            if matches >= len(words) * 0.5:
                return True
        
        return False
    
    # Zlicz pokryte
    must_covered = [t for t in must_topics if check_topic_covered(t)]
    high_covered = [t for t in high_topics if check_topic_covered(t)]
    medium_covered = [t for t in medium_topics if check_topic_covered(t)]
    
    # Oblicz score (MUST ma najwy≈ºszƒÖ wagƒô)
    total_weight = len(must_topics) * 3 + len(high_topics) * 2 + len(medium_topics) * 1
    covered_weight = len(must_covered) * 3 + len(high_covered) * 2 + len(medium_covered) * 1
    
    score = covered_weight / total_weight if total_weight > 0 else 0
    score = round(score, 2)
    
    # Znajd≈∫ brakujƒÖce MUST i HIGH
    must_missing = [t.get("name", t.get("subtopic", "unknown")) for t in must_topics if t not in must_covered]
    high_missing = [t.get("name", t.get("subtopic", "unknown")) for t in high_topics if t not in high_covered]
    
    # Status
    if score >= 0.8:
        status = "OK"
        message = f"Dobre pokrycie temat√≥w ({score:.0%})"
    elif score >= 0.6:
        status = "WARNING"
        message = f"Pokrycie temat√≥w {score:.0%} - dodaj brakujƒÖce"
    else:
        status = "WARNING"
        message = f"Niskie pokrycie temat√≥w ({score:.0%}) - pilnie uzupe≈Çnij!"
    
    return {
        "status": status,
        "score": score,
        "score_percent": round(score * 100, 1),
        "message": message,
        "must_covered": len(must_covered),
        "must_total": len(must_topics),
        "high_covered": len(high_covered),
        "high_total": len(high_topics),
        "must_missing": must_missing[:5],
        "high_missing": high_missing[:5]
    }


# ================================================================
# üÜï FAZA 2: BATCH HISTORY TRACKING
# ================================================================
def analyze_batch_trend(batch_history: List[Dict]) -> Dict[str, Any]:
    """
    Analizuje trend metryk miƒôdzy batchami.
    """
    if not batch_history or len(batch_history) < 2:
        return {
            "trend": "insufficient_data",
            "message": "Za ma≈Ço danych do analizy trendu"
        }
    
    # Pobierz ostatnie 3 batche (lub mniej je≈õli brak)
    recent = batch_history[-3:]
    
    # Analizuj humanness score
    humanness_scores = [b.get("humanness_score", 0) for b in recent]
    
    # Oblicz trend
    if len(humanness_scores) >= 2:
        first_half = sum(humanness_scores[:len(humanness_scores)//2 + 1]) / (len(humanness_scores)//2 + 1)
        second_half = sum(humanness_scores[len(humanness_scores)//2:]) / (len(humanness_scores) - len(humanness_scores)//2)
        
        diff = second_half - first_half
        
        if diff > 5:
            trend = "improving"
            trend_message = f"üìà Trend rosnƒÖcy (+{diff:.1f} punkt√≥w)"
        elif diff < -5:
            trend = "declining"
            trend_message = f"üìâ Trend spadkowy ({diff:.1f} punkt√≥w)"
        else:
            trend = "stable"
            trend_message = "‚û°Ô∏è Trend stabilny"
    else:
        trend = "stable"
        trend_message = "‚û°Ô∏è Trend stabilny"
    
    # ≈örednie metryki
    avg_humanness = sum(humanness_scores) / len(humanness_scores) if humanness_scores else 0
    
    burstiness_scores = [b.get("burstiness", 0) for b in recent if b.get("burstiness")]
    avg_burstiness = sum(burstiness_scores) / len(burstiness_scores) if burstiness_scores else 0
    
    return {
        "trend": trend,
        "message": trend_message,
        "avg_humanness": round(avg_humanness, 1),
        "avg_burstiness": round(avg_burstiness, 2),
        "batches_analyzed": len(recent),
        "last_scores": humanness_scores
    }


def create_batch_record(
    batch_number: int,
    humanness_score: int,
    burstiness: float,
    paragraphs: int,
    entity_density: float = 0,
    topic_completeness: float = 0
) -> Dict[str, Any]:
    """
    Tworzy rekord batcha do historii.
    """
    return {
        "batch": batch_number,
        "humanness_score": humanness_score,
        "burstiness": round(burstiness, 2),
        "paragraphs": paragraphs,
        "entity_density": round(entity_density, 2),
        "topic_completeness": round(topic_completeness, 2)
    }


# ================================================================
# üÜï FAZA 3: PER-SENTENCE SCORING
# ================================================================
AI_PATTERN_FLAGS = [
    (r'\bwarto\b', "warto"),
    (r'\bnale≈ºy\b', "nale≈ºy"),
    (r'\bkluczowy\b', "kluczowy"),
    (r'\bkompleksowy\b', "kompleksowy"),
    (r'\binnowacyjny\b', "innowacyjny"),
    (r'\bprofesjonalny\b', "profesjonalny"),
    (r'\bwysokiej jako≈õci\b', "wysokiej jako≈õci"),
    (r'\bszeroki zakres\b', "szeroki zakres"),
    (r'\bw pe≈Çni\b', "w pe≈Çni"),
    (r'\bw szczeg√≥lno≈õci\b', "w szczeg√≥lno≈õci"),
]

GENERIC_STARTERS = [
    "firma oferuje",
    "firma zapewnia",
    "firma gwarantuje",
    "us≈Çugi obejmujƒÖ",
    "klienci otrzymujƒÖ",
    "warto wiedzieƒá",
    "nale≈ºy pamiƒôtaƒá",
    "wa≈ºne jest",
]


def score_single_sentence(sentence: str) -> Dict[str, Any]:
    """
    Ocenia pojedyncze zdanie pod kƒÖtem AI-like patterns.
    """
    sentence_lower = sentence.lower().strip()
    words = sentence.split()
    word_count = len(words)
    
    # Flagi AI
    ai_flags = []
    for pattern, name in AI_PATTERN_FLAGS:
        if re.search(pattern, sentence_lower):
            ai_flags.append(name)
    
    # Sprawd≈∫ starter
    starter = ' '.join(words[:3]).lower() if len(words) >= 3 else sentence_lower
    generic_starter = any(gs in starter for gs in GENERIC_STARTERS)
    
    # Oblicz score zdania (0-100)
    score = 100
    
    # Kary
    if ai_flags:
        score -= len(ai_flags) * 15  # -15 za ka≈ºdƒÖ flagƒô AI
    
    if generic_starter:
        score -= 20  # -20 za generyczny starter
    
    # Kara za zbyt r√≥wnƒÖ d≈Çugo≈õƒá (typowe dla AI: 12-18 s≈Ç√≥w)
    if 12 <= word_count <= 18:
        score -= 5  # lekka kara za "≈õredniƒÖ" d≈Çugo≈õƒá
    
    # Bonus za kr√≥tkie (<8) lub d≈Çugie (>25) zdania
    if word_count < 8 or word_count > 25:
        score += 10
    
    score = max(0, min(100, score))
    
    # Status
    if score >= 70:
        status = "OK"
    elif score >= 50:
        status = "WARNING"
    else:
        status = "AI_LIKE"
    
    return {
        "text": sentence[:80] + ("..." if len(sentence) > 80 else ""),
        "word_count": word_count,
        "starter": starter,
        "score": score,
        "status": status,
        "ai_flags": ai_flags,
        "generic_starter": generic_starter
    }


def score_sentences(text: str, limit: int = 20) -> Dict[str, Any]:
    """
    Ocenia wszystkie zdania w tek≈õcie.
    Zwraca posortowane od najgorszych.
    """
    sentences = split_into_sentences(text)
    
    if not sentences:
        return {
            "status": "NO_DATA",
            "message": "Brak zda≈Ñ do analizy",
            "sentences": []
        }
    
    scored = []
    for s in sentences:
        result = score_single_sentence(s)
        scored.append(result)
    
    # Sortuj od najgorszych (najni≈ºszy score)
    scored.sort(key=lambda x: x["score"])
    
    # Statystyki
    scores = [s["score"] for s in scored]
    avg_score = sum(scores) / len(scores) if scores else 0
    
    ai_like_count = sum(1 for s in scored if s["status"] == "AI_LIKE")
    warning_count = sum(1 for s in scored if s["status"] == "WARNING")
    ok_count = sum(1 for s in scored if s["status"] == "OK")
    
    # Status og√≥lny
    if ai_like_count >= 3:
        overall_status = "CRITICAL"
        message = f"Znaleziono {ai_like_count} zda≈Ñ wyglƒÖdajƒÖcych na AI. Przepisz je!"
    elif ai_like_count >= 1 or warning_count >= 5:
        overall_status = "WARNING"
        message = f"Znaleziono {ai_like_count} AI-like i {warning_count} warning zda≈Ñ"
    else:
        overall_status = "OK"
        message = "Zdania wyglƒÖdajƒÖ naturalnie"
    
    # Sugestie poprawy dla najgorszych zda≈Ñ
    suggestions = []
    for s in scored[:5]:  # Top 5 najgorszych
        if s["status"] in ["AI_LIKE", "WARNING"]:
            if s["ai_flags"]:
                suggestions.append(f"Zdanie '{s['text'][:40]}...' - usu≈Ñ: {', '.join(s['ai_flags'][:2])}")
            elif s["generic_starter"]:
                suggestions.append(f"Zdanie '{s['text'][:40]}...' - zmie≈Ñ starter")
    
    return {
        "status": overall_status,
        "message": message,
        "total_sentences": len(sentences),
        "avg_score": round(avg_score, 1),
        "ai_like_count": ai_like_count,
        "warning_count": warning_count,
        "ok_count": ok_count,
        "worst_sentences": scored[:limit],
        "suggestions": suggestions[:5]
    }


# ================================================================
# üÜï FAZA 3: N-GRAM NATURALNESS CHECK (z wordfreq)
# ================================================================

# Znane nienaturalne frazy AI (blacklist)
AI_BLACKLIST_NGRAMS = [
    "kluczowy aspekt",
    "holistyczne podej≈õcie", 
    "innowacyjne rozwiƒÖzanie",
    "strategiczne znaczenie",
    "fundamentalne znaczenie",
    "nie ulega wƒÖtpliwo≈õci",
    "warto zauwa≈ºyƒá ≈ºe",
    "nale≈ºy podkre≈õliƒá ≈ºe",
    "kompleksowe rozwiƒÖzanie",
    "szeroki zakres us≈Çug",
    "indywidualne podej≈õcie",
    "wysoki standard",
    "pe≈Çen profesjonalizm",
    "bogaty do≈õwiadczenie",
    "dynamicznie rozwijajƒÖcy",
]

# Nadu≈ºywane frazy SEO (nie b≈ÇƒÖd, ale za czƒôsto = AI)
OVERUSED_SEO_PHRASES = [
    "firma oferuje",
    "profesjonalne us≈Çugi", 
    "wysoka jako≈õƒá",
    "kompleksowa obs≈Çuga",
    "konkurencyjne ceny",
    "do≈õwiadczony zesp√≥≈Ç",
    "wieloletnie do≈õwiadczenie",
    "szeroka oferta",
    "najwy≈ºsza jako≈õƒá",
]


def get_word_frequency(word: str) -> float:
    """
    Zwraca czƒôsto≈õƒá s≈Çowa (skala Zipf 0-7).
    Je≈õli wordfreq niedostƒôpny, zwraca domy≈õlnƒÖ warto≈õƒá.
    """
    if not WORDFREQ_AVAILABLE:
        return 4.0  # ≈õrednia domy≈õlna
    
    try:
        freq = zipf_frequency(word, 'pl')
        return freq if freq > 0 else 1.0  # nieznane s≈Çowa = rzadkie
    except:
        return 4.0


def calculate_ngram_frequency(ngram: str) -> Dict[str, Any]:
    """
    Oblicza ≈õredniƒÖ czƒôsto≈õƒá n-gramu na podstawie czƒôsto≈õci s≈Ç√≥w.
    """
    words = ngram.lower().split()
    if not words:
        return {"ngram": ngram, "avg_freq": 0, "min_freq": 0}
    
    freqs = [get_word_frequency(w) for w in words if w not in POLISH_STOP_WORDS]
    
    if not freqs:
        # Wszystkie s≈Çowa to stop words - wysoka czƒôsto≈õƒá
        return {"ngram": ngram, "avg_freq": 6.0, "min_freq": 6.0}
    
    return {
        "ngram": ngram,
        "avg_freq": round(sum(freqs) / len(freqs), 2),
        "min_freq": round(min(freqs), 2),
        "word_count": len(words)
    }


def extract_ngrams(text: str, n: int = 2) -> List[str]:
    """
    WyciƒÖga n-gramy z tekstu.
    """
    # Usu≈Ñ HTML i normalizuj
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    text = re.sub(r'\s+', ' ', text).strip()
    
    words = text.split()
    
    if len(words) < n:
        return []
    
    ngrams = []
    for i in range(len(words) - n + 1):
        ngram = ' '.join(words[i:i+n])
        ngrams.append(ngram)
    
    return ngrams


def check_ngram_naturalness(text: str) -> Dict[str, Any]:
    """
    Sprawdza naturalno≈õƒá fraz w tek≈õcie u≈ºywajƒÖc wordfreq.
    
    Metoda:
    1. WyciƒÖga bigramy i trigramy
    2. Oblicza czƒôsto≈õƒá ka≈ºdego n-gramu (≈õrednia Zipf s≈Ç√≥w)
    3. Identyfikuje rzadkie/nienaturalne frazy
    4. Sprawdza blacklistƒô AI
    5. Sprawdza nadu≈ºywane frazy SEO
    """
    text_lower = text.lower()
    words = text_lower.split()
    
    if len(words) < 50:
        return {
            "status": "NO_DATA",
            "message": "Za ma≈Ço tekstu do analizy n-gram√≥w",
            "wordfreq_available": WORDFREQ_AVAILABLE
        }
    
    # 1. Sprawd≈∫ blacklistƒô AI
    ai_phrases_found = []
    for phrase in AI_BLACKLIST_NGRAMS:
        count = text_lower.count(phrase)
        if count > 0:
            ai_phrases_found.append({"phrase": phrase, "count": count})
    
    # 2. Sprawd≈∫ nadu≈ºywane frazy SEO
    overused_found = []
    for phrase in OVERUSED_SEO_PHRASES:
        count = text_lower.count(phrase)
        if count >= 2:  # 2+ = nadu≈ºywane
            overused_found.append({"phrase": phrase, "count": count})
    
    # 3. WyciƒÖgnij i przeanalizuj bigramy (je≈õli wordfreq dostƒôpny)
    unusual_ngrams = []
    low_freq_ngrams = []
    
    if WORDFREQ_AVAILABLE:
        bigrams = extract_ngrams(text, n=2)
        
        # Zlicz bigramy
        bigram_counts = Counter(bigrams)
        
        # Analizuj najczƒôstsze bigramy (potencjalnie nadu≈ºywane)
        for bigram, count in bigram_counts.most_common(30):
            if count >= 3:  # Powt√≥rzone 3+ razy
                freq_data = calculate_ngram_frequency(bigram)
                
                # Je≈õli niska ≈õrednia czƒôsto≈õƒá = dziwna fraza
                if freq_data["avg_freq"] < 3.5:
                    unusual_ngrams.append({
                        "ngram": bigram,
                        "count": count,
                        "avg_freq": freq_data["avg_freq"],
                        "reason": "low_frequency"
                    })
                # Je≈õli wysoka czƒôsto≈õƒá ale du≈ºo powt√≥rze≈Ñ = nadu≈ºywane
                elif count >= 5:
                    unusual_ngrams.append({
                        "ngram": bigram,
                        "count": count,
                        "avg_freq": freq_data["avg_freq"],
                        "reason": "overused"
                    })
        
        # Znajd≈∫ og√≥lnie rzadkie bigramy (min_freq < 2.5)
        unique_bigrams = list(set(bigrams))[:100]  # Sprawd≈∫ max 100
        for bigram in unique_bigrams:
            freq_data = calculate_ngram_frequency(bigram)
            if freq_data["min_freq"] < 2.0 and freq_data["min_freq"] > 0:
                low_freq_ngrams.append({
                    "ngram": bigram,
                    "min_freq": freq_data["min_freq"],
                    "avg_freq": freq_data["avg_freq"]
                })
        
        # Sortuj po czƒôsto≈õci (najrzadsze najpierw)
        low_freq_ngrams.sort(key=lambda x: x["min_freq"])
        low_freq_ngrams = low_freq_ngrams[:10]
    
    # 4. Oblicz naturalness score
    penalty = 0
    
    # Kary za AI phrases (najwiƒôksza kara)
    penalty += len(ai_phrases_found) * 0.15
    
    # Kary za nadu≈ºywane SEO
    penalty += len(overused_found) * 0.08
    
    # Kary za unusual ngrams
    penalty += len(unusual_ngrams) * 0.05
    
    # Kary za low freq ngrams
    penalty += min(len(low_freq_ngrams) * 0.03, 0.15)
    
    naturalness_score = max(0, 1.0 - penalty)
    naturalness_score = round(naturalness_score, 2)
    
    # 5. Status
    if naturalness_score >= 0.75:
        status = "OK"
        message = f"Frazy brzmiƒÖ naturalnie (score {naturalness_score})"
    elif naturalness_score >= 0.5:
        status = "WARNING"
        message = f"Niekt√≥re frazy wymagajƒÖ poprawy (score {naturalness_score})"
    else:
        status = "CRITICAL"
        message = f"Wiele fraz brzmi nienaturalnie/AI (score {naturalness_score})"
    
    # 6. Sugestie
    suggestions = []
    
    # Sugestie dla AI phrases (priorytet)
    for item in ai_phrases_found[:3]:
        suggestions.append(f"‚ùå Usu≈Ñ AI-frazƒô: '{item['phrase']}'")
    
    # Sugestie dla nadu≈ºywanych
    for item in overused_found[:2]:
        suggestions.append(f"‚ö†Ô∏è Ogranicz '{item['phrase']}' (u≈ºyte {item['count']}√ó)")
    
    # Sugestie dla unusual
    for item in unusual_ngrams[:2]:
        if item["reason"] == "overused":
            suggestions.append(f"üìù Zmniejsz powt√≥rzenia: '{item['ngram']}' ({item['count']}√ó)")
        else:
            suggestions.append(f"üìù Sprawd≈∫ frazƒô: '{item['ngram']}' (rzadka)")
    
    return {
        "status": status,
        "message": message,
        "naturalness_score": naturalness_score,
        "wordfreq_available": WORDFREQ_AVAILABLE,
        
        # Szczeg√≥≈Çy
        "ai_phrases_found": ai_phrases_found[:5],
        "overused_seo_phrases": overused_found[:5],
        "unusual_ngrams": unusual_ngrams[:5],
        "low_frequency_ngrams": low_freq_ngrams[:5],
        
        # Statystyki
        "stats": {
            "ai_phrases_count": len(ai_phrases_found),
            "overused_count": len(overused_found),
            "unusual_count": len(unusual_ngrams)
        },
        
        "suggestions": suggestions[:7]
    }


# ================================================================
# üéØ FAZA 3: FULL ADVANCED ANALYSIS
# ================================================================
def full_advanced_analysis(
    text: str,
    previous_paragraphs: int = None,
    s1_relationships: List[Dict] = None,
    s1_entities: List[Dict] = None,
    s1_topics: List[Dict] = None
) -> Dict[str, Any]:
    """
    Pe≈Çna zaawansowana analiza tekstu - wszystkie metryki.
    """
    # Podstawowa analiza AI
    humanness = calculate_humanness_score(text)
    forbidden = check_forbidden_phrases(text)
    
    # Walidacje
    current_paragraphs = len(re.split(r'\n\s*\n', text.strip()))
    jitter = validate_jitter(current_paragraphs, previous_paragraphs)
    triplets = validate_triplets(text, s1_relationships or [])
    
    # Faza 2
    entity_split = calculate_entity_split(text, s1_entities or [])
    topic_completeness = calculate_topic_completeness(text, s1_topics or [])
    
    # Faza 3
    sentence_analysis = score_sentences(text, limit=10)
    ngram_analysis = check_ngram_naturalness(text)
    
    # ≈ÅƒÖczny status
    statuses = [
        humanness["status"],
        forbidden["status"],
        sentence_analysis["status"],
        ngram_analysis["status"]
    ]
    
    if "CRITICAL" in statuses:
        overall_status = "CRITICAL"
    elif statuses.count("WARNING") >= 2:
        overall_status = "WARNING"
    elif "WARNING" in statuses:
        overall_status = "OK"  # pojedynczy warning OK
    else:
        overall_status = "OK"
    
    # Zbierz wszystkie sugestie
    all_suggestions = []
    all_suggestions.extend(humanness.get("suggestions", []))
    all_suggestions.extend(sentence_analysis.get("suggestions", []))
    all_suggestions.extend(ngram_analysis.get("suggestions", []))
    
    # Zbierz wszystkie warnings
    all_warnings = humanness.get("warnings", [])
    if forbidden["status"] != "OK":
        all_warnings.append(forbidden["message"])
    if sentence_analysis["status"] != "OK":
        all_warnings.append(sentence_analysis["message"])
    if ngram_analysis["status"] != "OK":
        all_warnings.append(ngram_analysis["message"])
    
    return {
        "overall_status": overall_status,
        "humanness_score": humanness["humanness_score"],
        
        # Podstawowe metryki
        "components": humanness["components"],
        
        # Walidacje (Faza 1)
        "validations": {
            "forbidden_phrases": forbidden,
            "jitter": jitter,
            "triplets": triplets
        },
        
        # Faza 2
        "entity_split": entity_split,
        "topic_completeness": topic_completeness,
        
        # Faza 3
        "sentence_analysis": {
            "status": sentence_analysis["status"],
            "avg_score": sentence_analysis["avg_score"],
            "ai_like_count": sentence_analysis["ai_like_count"],
            "worst_sentences": sentence_analysis["worst_sentences"][:5]
        },
        "ngram_analysis": ngram_analysis,
        
        # Podsumowanie
        "warnings": all_warnings[:10],
        "suggestions": all_suggestions[:10]
    }
