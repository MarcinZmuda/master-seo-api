"""
Export Routes - v29.0
Eksport artyku≈Ç√≥w do DOCX/HTML/TXT + Editorial Review (Claude API)

ZMIANY v29.0:
- DIFF-BASED: Claude zwraca tylko zmiany, nie przepisuje ca≈Ço≈õci
- BURSTINESS VALIDATION: Sprawdza CV przed/po
- AUTO-ROLLBACK: Automatycznie przywraca orygina≈Ç je≈õli CV spad≈Ço
- DIFF OUTPUT: Szczeg√≥≈Çowa lista zmian applied/failed
- MANUAL ROLLBACK: Nowy endpoint /editorial_review/rollback

ZMIANY v27.1:
- rescan_keywords_after_editorial() - przelicza frazy po Claude review

ZMIANY v27.0:
- Zmiana z Gemini na Claude API
- Wplata nieu≈ºyte frazy BASIC/EXTENDED
"""

import os
import re
import io
import json
import statistics
from datetime import datetime
from flask import Blueprint, request, jsonify, Response

# Firebase Firestore only
from firebase_admin import firestore

# Document generation
from docx import Document
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH

# Claude API for Editorial Review
import anthropic

# v27.1: Import keyword counter do rescan
try:
    from keyword_counter import count_keywords, count_keywords_for_state
    KEYWORD_COUNTER_OK = True
    print("[EXPORT] ‚úÖ keyword_counter imported for rescan")
except ImportError:
    KEYWORD_COUNTER_OK = False
    print("[EXPORT] ‚ö†Ô∏è keyword_counter not available - rescan will use fallback")

# v29.0: Burstiness validation for editorial review
try:
    from ai_detection_metrics import calculate_burstiness
    BURSTINESS_CHECK_OK = True
    print("[EXPORT] ‚úÖ Burstiness validation enabled")
except ImportError:
    BURSTINESS_CHECK_OK = False
    print("[EXPORT] ‚ö†Ô∏è Burstiness validation not available")

# v27.1: ≈Åadowanie promptu z pliku (opcjonalne)
EDITORIAL_PROMPT_TEMPLATE = None
try:
    import os
    prompt_path = os.path.join(os.path.dirname(__file__), "editorial_prompt.json")
    if os.path.exists(prompt_path):
        with open(prompt_path, "r", encoding="utf-8") as f:
            prompt_data = json.load(f)
            EDITORIAL_PROMPT_TEMPLATE = prompt_data.get("full_prompt_assembled")
            print(f"[EXPORT] ‚úÖ Loaded editorial prompt from file (v{prompt_data.get('version', '?')})")
except Exception as e:
    print(f"[EXPORT] ‚ÑπÔ∏è Using built-in prompt (file load failed: {e})")

export_routes = Blueprint("export_routes", __name__)

# v51: Safe full_article extraction ‚Äî handles both string and dict formats
def _safe_get_full_article(project_data):
    """Extract text from full_article field, handling both dict and string."""
    fa = project_data.get("full_article")
    if isinstance(fa, dict):
        return fa.get("content", "")
    elif isinstance(fa, str):
        return fa
    return ""

# Claude config
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
# v50.5: Configurable editorial model via env var
# v51: Changed default to alias (no snapshot) + added fallback chain
EDITORIAL_MODEL = os.getenv("EDITORIAL_MODEL", "claude-sonnet-4-5")
EDITORIAL_MODEL_FALLBACK = os.getenv("EDITORIAL_MODEL_FALLBACK", "claude-sonnet-4-6")
claude_client = None
if ANTHROPIC_API_KEY:
    claude_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
    print(f"[EXPORT] ‚úÖ Claude API configured (editorial model: {EDITORIAL_MODEL}, fallback: {EDITORIAL_MODEL_FALLBACK})")
else:
    print("[EXPORT] ‚ö†Ô∏è ANTHROPIC_API_KEY not set")


def _claude_call(prompt: str, max_tokens: int = 6000, label: str = "CALL") -> tuple:
    """Call Claude with model fallback. Returns (response_text, model_used)."""
    models_to_try = [EDITORIAL_MODEL, EDITORIAL_MODEL_FALLBACK]
    for model in models_to_try:
        try:
            print(f"[EDITORIAL_REVIEW] {label}: trying model={model}")
            resp = claude_client.messages.create(
                model=model,
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}]
            )
            text = resp.content[0].text.strip()
            print(f"[EDITORIAL_REVIEW] {label}: ‚úÖ model={model} ‚Üí {len(text)} chars")
            return text, model
        except Exception as e:
            err_str = str(e)
            print(f"[EDITORIAL_REVIEW] {label}: ‚ùå model={model} ‚Üí {err_str[:200]}")
            if model == models_to_try[-1]:
                raise  # re-raise if last model also fails
    raise RuntimeError("All Claude models failed")

# Fallback: Gemini (je≈õli Claude niedostƒôpny)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
genai = None
if GEMINI_API_KEY:
    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        if not ANTHROPIC_API_KEY:
            print("[EXPORT] ‚ÑπÔ∏è Using Gemini as fallback")
    except ImportError:
        genai = None


def html_to_text(html: str) -> str:
    """Konwertuje HTML na czysty tekst."""
    text = re.sub(r'<h2[^>]*>(.*?)</h2>', r'\n\n## \1\n\n', html, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'<h3[^>]*>(.*?)</h3>', r'\n\n### \1\n\n', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'<p[^>]*>(.*?)</p>', r'\1\n\n', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
    text = re.sub(r'<li[^>]*>(.*?)</li>', r'‚Ä¢ \1\n', text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


def parse_article_structure(text: str) -> list:
    """Parsuje artyku≈Ç na strukturƒô nag≈Ç√≥wk√≥w i paragraf√≥w."""
    elements = []
    
    # Normalizuj format
    text = re.sub(r'^h2:\s*(.+)$', r'<h2>\1</h2>', text, flags=re.MULTILINE)
    text = re.sub(r'^h3:\s*(.+)$', r'<h3>\1</h3>', text, flags=re.MULTILINE)
    
    parts = re.split(r'(<h[23][^>]*>.*?</h[23]>)', text, flags=re.IGNORECASE | re.DOTALL)
    
    for part in parts:
        part = part.strip()
        if not part:
            continue
            
        h2_match = re.match(r'<h2[^>]*>(.*?)</h2>', part, re.IGNORECASE | re.DOTALL)
        h3_match = re.match(r'<h3[^>]*>(.*?)</h3>', part, re.IGNORECASE | re.DOTALL)
        
        if h2_match:
            elements.append({"type": "h2", "content": h2_match.group(1).strip()})
        elif h3_match:
            elements.append({"type": "h3", "content": h3_match.group(1).strip()})
        else:
            clean_text = html_to_text(part)
            if clean_text:
                elements.append({"type": "paragraph", "content": clean_text})
    
    return elements


# ================================================================
# v27.0: HELPER - Sk≈Çadanie tre≈õci z batch√≥w
# ================================================================
def extract_text_from_batches(batches: list) -> tuple:
    """
    Bezpieczne sk≈Çadanie tre≈õci z batch√≥w.
    Obs≈Çuguje r√≥≈ºne formaty: batch.text, batch.content, batch.html, lub string
    
    Returns:
        (full_text, debug_info)
    """
    texts = []
    debug_info = {
        "total_batches": len(batches),
        "batches_with_text": 0,
        "batches_empty": 0,
        "fields_found": set()
    }
    
    for i, batch in enumerate(batches):
        text = None
        
        # v51: Handle string batches
        if isinstance(batch, str):
            text = batch.strip()
            if text:
                debug_info["fields_found"].add("string")
        elif isinstance(batch, dict):
            for field in ["text", "content", "html", "batch_text", "raw_text"]:
                if field in batch and batch[field]:
                    text = str(batch[field]).strip()
                    debug_info["fields_found"].add(field)
                    break
        
        if text:
            texts.append(text)
            debug_info["batches_with_text"] += 1
        else:
            debug_info["batches_empty"] += 1
            batch_keys = list(batch.keys()) if isinstance(batch, dict) else [type(batch).__name__]
            print(f"[EXPORT] ‚ö†Ô∏è Batch {i} jest pusty. Klucze: {batch_keys}")
    
    debug_info["fields_found"] = list(debug_info["fields_found"])
    full_text = "\n\n".join(texts)
    
    return full_text, debug_info


# ================================================================
# v27.1: RESCAN KEYWORDS AFTER EDITORIAL
# ================================================================
def rescan_keywords_after_editorial(project_id: str, corrected_article: str) -> dict:
    """
    v27.1: Przelicza frazy od ZERA po edycji Claude.
    """
    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    
    if not doc.exists:
        return {"error": "Project not found", "rescanned": False}
    
    project_data = doc.to_dict()
    keywords_state = project_data.get("keywords_state", {})
    
    if not keywords_state:
        return {"error": "No keywords_state", "rescanned": False}
    
    if not corrected_article or len(corrected_article.strip()) < 50:
        return {"error": "No corrected_article to scan", "rescanned": False}
    
    # Zbierz wszystkie keywordy
    keywords = []
    rid_to_keyword = {}
    for rid, meta in keywords_state.items():
        kw = (meta.get("keyword") or "").strip()
        if kw:
            keywords.append(kw)
            rid_to_keyword[rid] = kw
    
    if not keywords:
        return {"error": "No keywords to scan", "rescanned": False}
    
    # Przelicz frazy
    changes = []
    new_counts = {}
    
    if KEYWORD_COUNTER_OK:
        new_counts = count_keywords_for_state(corrected_article, keywords_state, use_exclusive_for_nested=False)
        print(f"[RESCAN] Using keyword_counter for {len(keywords)} keywords")
    else:
        # Fallback - proste liczenie
        clean_text = re.sub(r'<[^>]+>', ' ', corrected_article.lower())
        clean_text = re.sub(r'\s+', ' ', clean_text)
        
        for rid, kw in rid_to_keyword.items():
            kw_lower = kw.lower()
            count = clean_text.count(kw_lower)
            new_counts[rid] = count
        print(f"[RESCAN] Using fallback counter for {len(keywords)} keywords")
    
    # Por√≥wnaj i zaktualizuj
    for rid, meta in keywords_state.items():
        old_actual = meta.get("actual_uses", 0)
        new_actual = new_counts.get(rid, 0)
        
        if old_actual != new_actual:
            kw = meta.get("keyword", "?")
            kw_type = meta.get("type", "BASIC")
            changes.append({
                "keyword": kw,
                "type": kw_type,
                "old": old_actual,
                "new": new_actual,
                "diff": new_actual - old_actual
            })
            print(f"[RESCAN] '{kw}' ({kw_type}): {old_actual} ‚Üí {new_actual}")
        
        # NADPISZ actual_uses
        meta["actual_uses"] = new_actual
        
        # Przelicz status
        min_t = meta.get("target_min", 1 if meta.get("type", "").upper() == "BASIC" else 1)
        max_t = meta.get("target_max", 999)
        
        if new_actual < min_t:
            meta["status"] = "UNDER"
        elif new_actual == max_t:
            meta["status"] = "OPTIMAL"
        elif min_t <= new_actual < max_t:
            meta["status"] = "OK"
        else:
            meta["status"] = "OVER"
        
        meta["remaining_max"] = max(0, max_t - new_actual)
        keywords_state[rid] = meta
    
    # Zapisz do Firebase
    try:
        db.collection("seo_projects").document(project_id).update({
            "keywords_state": keywords_state,
            "last_rescan": {
                "timestamp": firestore.SERVER_TIMESTAMP,
                "changes_count": len(changes),
                "trigger": "editorial_review"
            }
        })
        print(f"[RESCAN] ‚úÖ Saved {len(changes)} changes to Firebase")
    except Exception as e:
        print(f"[RESCAN] ‚ùå Firebase save error: {e}")
        return {"error": str(e), "rescanned": False, "changes": changes}
    
    # Policz nowe coverage
    basic_total = 0
    basic_covered = 0
    extended_total = 0
    extended_covered = 0
    
    for rid, meta in keywords_state.items():
        kw_type = meta.get("type", "BASIC").upper()
        actual = meta.get("actual_uses", 0)
        
        if kw_type == "BASIC" or kw_type == "MAIN":
            basic_total += 1
            if actual >= 1:
                basic_covered += 1
        elif kw_type == "EXTENDED":
            extended_total += 1
            if actual >= 1:
                extended_covered += 1
    
    basic_pct = round(100 * basic_covered / basic_total, 1) if basic_total > 0 else 100
    extended_pct = round(100 * extended_covered / extended_total, 1) if extended_total > 0 else 100
    
    return {
        "rescanned": True,
        "changes_count": len(changes),
        "changes": changes,
        "coverage_after_rescan": {
            "basic": f"{basic_covered}/{basic_total} ({basic_pct}%)",
            "extended": f"{extended_covered}/{extended_total} ({extended_pct}%)"
        },
        "missing_after_rescan": {
            "basic": [c["keyword"] for c in changes if c["type"].upper() in ["BASIC", "MAIN"] and c["new"] == 0],
            "extended": [c["keyword"] for c in changes if c["type"].upper() == "EXTENDED" and c["new"] == 0]
        }
    }


# ================================================================
# v29.0: DIFF PARSING HELPERS
# ================================================================
def parse_diff_response(response_text: str) -> list:
    """
    Parsuje odpowied≈∫ Claude w formacie DIFF.
    
    Format wej≈õciowy:
    [ZMIANA 1]
    ZNAJD≈π: "dok≈Çadny cytat"
    ZAMIE≈É: "poprawiona wersja"
    POW√ìD: wyja≈õnienie
    
    Zwraca: [{"find": "...", "replace": "...", "reason": "..."}]
    """
    changes = []
    
    # Wzorzec dla pojedynczej zmiany
    pattern = r'\[ZMIANA \d+\]\s*\n\s*ZNAJD≈π:\s*["\'](.+?)["\']\s*\n\s*ZAMIE≈É:\s*["\'](.+?)["\']\s*\n\s*POW√ìD:\s*(.+?)(?=\n\s*\[ZMIANA|\Z)'
    
    matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
    
    for match in matches:
        find_text = match[0].strip()
        replace_text = match[1].strip()
        reason = match[2].strip()
        
        if find_text and len(find_text) > 5:
            changes.append({
                "find": find_text,
                "replace": replace_text,
                "reason": reason,
                "applied": False
            })
    
    # Fallback: prostszy wzorzec
    if not changes:
        simple_pattern = r'ZNAJD≈π:\s*["\']?(.+?)["\']?\s*\n\s*ZAMIE≈É:\s*["\']?(.+?)["\']?\s*(?:\n|$)'
        simple_matches = re.findall(simple_pattern, response_text, re.DOTALL)
        
        for match in simple_matches:
            find_text = match[0].strip().strip('"\'')
            replace_text = match[1].strip().strip('"\'')
            
            if find_text and len(find_text) > 5:
                changes.append({
                    "find": find_text,
                    "replace": replace_text,
                    "reason": "auto-parsed",
                    "applied": False
                })
    
    return changes


def apply_diffs(original_text: str, changes: list) -> tuple:
    """
    Aplikuje zmiany diff do tekstu.
    
    Zwraca: (modified_text, applied_changes, failed_changes)
    """
    modified = original_text
    applied = []
    failed = []
    
    for change in changes:
        find_text = change["find"]
        replace_text = change["replace"]
        
        # Pr√≥ba 1: Dok≈Çadne dopasowanie
        if find_text in modified:
            modified = modified.replace(find_text, replace_text, 1)
            change["applied"] = True
            applied.append(change)
            continue
        
        # Pr√≥ba 2: Dopasowanie ignorujƒÖce whitespace
        find_normalized = ' '.join(find_text.split())
        text_lines = modified.split('\n')
        found = False
        
        for i, line in enumerate(text_lines):
            line_normalized = ' '.join(line.split())
            if find_normalized in line_normalized:
                # Zamie≈Ñ w tej linii
                text_lines[i] = line.replace(find_text.strip(), replace_text.strip())
                modified = '\n'.join(text_lines)
                change["applied"] = True
                change["match_type"] = "normalized"
                applied.append(change)
                found = True
                break
        
        if found:
            continue
        
        # Nie uda≈Ço siƒô
        change["applied"] = False
        change["error"] = "Fragment nie znaleziony w tek≈õcie"
        failed.append(change)
    
    return modified, applied, failed


def calculate_diff_stats(original: str, modified: str) -> dict:
    """Oblicza statystyki r√≥≈ºnic miƒôdzy tekstami."""
    original_words = original.split()
    modified_words = modified.split()
    
    original_sentences = [s for s in re.split(r'[.!?]+', original) if s.strip()]
    modified_sentences = [s for s in re.split(r'[.!?]+', modified) if s.strip()]
    
    words_added = len(modified_words) - len(original_words)
    sentences_diff = len(modified_sentences) - len(original_sentences)
    
    # Procent zmian
    if len(original) > 0:
        changes = sum(1 for a, b in zip(original, modified) if a != b)
        changes += abs(len(original) - len(modified))
        change_percent = round((changes / len(original)) * 100, 1)
    else:
        change_percent = 100
    
    return {
        "words_original": len(original_words),
        "words_modified": len(modified_words),
        "words_diff": words_added,
        "sentences_original": len(original_sentences),
        "sentences_modified": len(modified_sentences),
        "sentences_diff": sentences_diff,
        "change_percent": min(change_percent, 100)
    }


# ================================================================
# EDITORIAL REVIEW - v29.0 (DIFF-BASED + BURSTINESS VALIDATION)
# ================================================================
@export_routes.post("/api/project/<project_id>/editorial_review")
def editorial_review(project_id):
    """
    v29.0: DIFF-BASED Editorial Review z walidacjƒÖ burstiness.
    
    NOWO≈öCI:
    - DIFF-BASED: Claude zwraca tylko zmiany, nie przepisuje ca≈Ço≈õci
    - BURSTINESS VALIDATION: Sprawdza CV przed/po
    - AUTO-ROLLBACK: Automatycznie przywraca orygina≈Ç je≈õli CV spad≈Ço
    - DIFF OUTPUT: Szczeg√≥≈Çowa lista zmian
    """
    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404
    
    project_data = doc.to_dict()
    
    # Pobierz tre≈õƒá
    full_text = None
    debug_info = {}
    
    # v51: Defensive ‚Äî full_article can be string or dict
    fa_text = _safe_get_full_article(project_data)
    if fa_text:
        full_text = fa_text
        debug_info = {"source": "full_article"}
    else:
        batches = project_data.get("batches", [])
        if batches:
            full_text, debug_info = extract_text_from_batches(batches)
            debug_info["source"] = "batches"
    
    if not full_text:
        return jsonify({
            "error": "No content found",
            "hint": "U≈ºyj save_full_article lub addBatch aby zapisaƒá tre≈õƒá.",
            "project_keys": list(project_data.keys())
        }), 400
    
    # Sprawd≈∫ API
    if not claude_client and not genai:
        return jsonify({"error": "No AI API configured (need ANTHROPIC_API_KEY or GEMINI_API_KEY)"}), 500
    
    word_count = len(full_text.split()) if full_text else 0
    
    if word_count < 50:
        return jsonify({
            "error": f"Article too short for review ({word_count} words)",
            "debug": debug_info,
            "hint": "U≈ºyj save_full_article lub addBatch aby zapisaƒá tre≈õƒá."
        }), 400
    
    topic = project_data.get("topic") or project_data.get("main_keyword", "artyku≈Ç")
    
    # ================================================================
    # v29.0: BURSTINESS CHECK - PRZED
    # ================================================================
    burstiness_before = None
    if BURSTINESS_CHECK_OK:
        try:
            burstiness_before = calculate_burstiness(full_text)
            print(f"[EDITORIAL_REVIEW] üìä Burstiness PRZED: CV={burstiness_before.get('cv', 0)}")
        except Exception as e:
            print(f"[EDITORIAL_REVIEW] ‚ö†Ô∏è Burstiness check failed: {e}")
    
    # v27.0: Pobierz nieu≈ºyte frazy BASIC i EXTENDED
    keywords_state = project_data.get("keywords_state", {})
    unused_basic = []
    unused_extended = []
    
    for rid, meta in keywords_state.items():
        kw = meta.get("keyword", "")
        kw_type = meta.get("type", "BASIC").upper()
        actual = meta.get("actual_uses", 0)
        
        if actual == 0 and kw:
            if kw_type == "BASIC":
                unused_basic.append(kw)
            elif kw_type == "EXTENDED":
                unused_extended.append(kw)
    
    # Sekcja nieu≈ºytych fraz dla promptu
    unused_keywords_section = ""
    if unused_basic or unused_extended:
        unused_keywords_section = "\n=== ‚ö†Ô∏è NIEWYKORZYSTANE FRAZY SEO ===\n"
        if unused_basic:
            unused_keywords_section += f"BASIC (MUSZƒÑ byƒá w tek≈õcie): {', '.join(unused_basic[:15])}\n"
        if unused_extended:
            unused_keywords_section += f"EXTENDED (wpleƒá je≈õli pasujƒÖ): {', '.join(unused_extended[:15])}\n"
    
    # v50.5: Overused keywords (stuffing) from Firebase
    overused_keywords = []
    for rid, meta in keywords_state.items():
        kw = meta.get("keyword", "")
        kw_type = meta.get("type", "BASIC").upper()
        actual = meta.get("actual_uses", 0)
        target_max = meta.get("target_max", 999)
        if actual > target_max and kw:
            overused_keywords.append({
                "keyword": kw,
                "actual": actual,
                "max": target_max,
                "excess": actual - target_max
            })
    
    if overused_keywords:
        overused_section = "\n=== üî¥ FRAZY NADU≈ªYTE (STUFFING ‚Äî zmniejsz!) ===\n"
        for ow in sorted(overused_keywords, key=lambda x: -x["excess"])[:10]:
            overused_section += f"  - '{ow['keyword']}': {ow['actual']}√ó (max: {ow['max']}, usu≈Ñ {ow['excess']}√ó)\n"
        overused_section += "‚Üí ZastƒÖp nadmiarowe wystƒÖpienia synonimami lub zaimkami.\n"
        unused_keywords_section += overused_section
    
    # ================================================================
    # üÜï v45.1 + v50.5: SEMANTIC COVERAGE + ENTITY PLACEMENT
    # ================================================================
    coverage_section = ""
    s1_data = project_data.get("s1_data", {})
    
    # Check entity coverage (from S1 entities)
    entity_seo = s1_data.get("entity_seo", {})
    all_entities = [e.get("name", "") for e in entity_seo.get("entities", []) if e.get("name")]
    article_lower = full_text.lower() if full_text else ""
    
    missing_entities = [e for e in all_entities if e.lower() not in article_lower]
    covered_entities = [e for e in all_entities if e.lower() in article_lower]
    entity_coverage_pct = (len(covered_entities) / len(all_entities) * 100) if all_entities else 100
    
    # v50.5: Concept entities (topical generator) ‚Äî these are the KEY entities
    concept_entities = []
    for tc in entity_seo.get("topical_coverage", []):
        if isinstance(tc, dict):
            name = tc.get("entity", tc.get("name", ""))
            if name:
                concept_entities.append(name)
    
    # v50.5: Must-cover concepts from project data
    must_cover = project_data.get("must_cover_concepts", [])
    if must_cover:
        must_names = []
        for mc in must_cover:
            if isinstance(mc, dict):
                must_names.append(mc.get("text", mc.get("name", "")))
            elif isinstance(mc, str):
                must_names.append(mc)
        if must_names:
            concept_entities = must_names
    
    # Check which concept entities are missing from article
    missing_concepts = [e for e in concept_entities if e.lower() not in article_lower]
    
    # Check content gap coverage
    content_gaps = s1_data.get("content_gaps", {})
    gap_topics = []
    for gap in content_gaps.get("gaps", []):
        if isinstance(gap, dict):
            gap_topics.append(gap.get("topic", ""))
        elif isinstance(gap, str):
            gap_topics.append(gap)
    
    uncovered_gaps = []
    for gtopic in gap_topics:
        topic_words = [w for w in gtopic.lower().split() if len(w) > 3]
        if topic_words:
            found = sum(1 for w in topic_words if w in article_lower)
            if found / len(topic_words) < 0.4:
                uncovered_gaps.append(gtopic)
    
    if missing_entities or uncovered_gaps or missing_concepts or concept_entities:
        coverage_section = "\n=== üìä POKRYCIE ENCJI I TEMAT√ìW ===\n"
        
        if concept_entities:
            coverage_section += f"\nENCJE TEMATYCZNE (MUST ‚Äî muszƒÖ byƒá w artykule):\n"
            for ce in concept_entities[:12]:
                present = "‚úÖ" if ce.lower() in article_lower else "‚ùå BRAK"
                coverage_section += f"  {present} {ce}\n"
            coverage_section += (
                "\n‚Üí Encje oznaczone ‚ùå MUSZƒÑ pojawiƒá siƒô w tek≈õcie.\n"
                "‚Üí Wpleƒá je naturalnie w istniejƒÖce zdania ‚Äî NIE tw√≥rz nowych akapit√≥w.\n"
                "‚Üí Encje to POJƒòCIA do opisania, NIE ≈∫r√≥d≈Ça do cytowania.\n"
                "‚Üí ‚ùå NIE pisz: 'wed≈Çug [encji]...', '[encja] podaje...'\n"
                "‚Üí ‚úÖ PISZ: 'W kontek≈õcie [encji] warto zauwa≈ºyƒá, ≈ºe...'\n"
            )
        
        if missing_entities and not concept_entities:
            coverage_section += f"\nEncje z S1: {len(covered_entities)}/{len(all_entities)} pokryte ({entity_coverage_pct:.0f}%)\n"
            if missing_entities[:10]:
                coverage_section += f"BRAKUJƒÑCE ENCJE: {', '.join(missing_entities[:10])}\n"
        
        if uncovered_gaps[:5]:
            coverage_section += f"\nNIEPOKRYTE TEMATY (Information Gain): {', '.join(uncovered_gaps[:5])}\n"
            coverage_section += "‚Üí Dodanie tych temat√≥w daje przewagƒô nad konkurencjƒÖ.\n"
    
    # üÜï v44.5: YMYL context for editorial review
    detected_category = project_data.get("detected_category", "inne")
    ymyl_section = ""
    if detected_category == "prawo":
        ymyl_section = """
=== ‚öñÔ∏è ARTYKU≈Å PRAWNY (YMYL) ===
Ten artyku≈Ç dotyczy tematyki prawnej. Sprawd≈∫ SZCZEG√ìLNIE:
1. Czy artyku≈Ç zawiera cytowania przepis√≥w (art. X ¬ß Y ustawy Z)?
2. Czy sƒÖ odwo≈Çania do orzecze≈Ñ sƒÖdowych (sygn. akt)?
3. Czy NIE MA wymy≈õlonych sygnatur, dat ani przepis√≥w?
4. Czy na ko≈Ñcu jest zastrze≈ºenie prawne (disclaimer)?
5. Je≈õli brakuje cytowa≈Ñ ‚Äî DODAJ je w sekcji "luki_tresciowe"!
"""
    elif detected_category == "medycyna":
        ymyl_section = """
=== üè• ARTYKU≈Å MEDYCZNY (YMYL) ===
Ten artyku≈Ç dotyczy tematyki medycznej/zdrowotnej. Sprawd≈∫ SZCZEG√ìLNIE:
1. Czy artyku≈Ç zawiera odwo≈Çania do publikacji naukowych?
2. Czy cytowane ≈∫r√≥d≈Ça istniejƒÖ (PMID, autorzy, rok)?
3. Czy NIE MA wymy≈õlonych statystyk, bada≈Ñ ani lek√≥w?
4. Czy na ko≈Ñcu jest zastrze≈ºenie medyczne (disclaimer)?
5. Czy tekst nie zawiera niebezpiecznych rad zdrowotnych?
6. Je≈õli brakuje cytowa≈Ñ ‚Äî DODAJ je w sekcji "luki_tresciowe"!
"""

    try:
        analysis = None
        corrected_article = ""
        ai_model = "unknown"
        applied_changes = []
        failed_changes = []
        
        if claude_client:
            # ============================================================
            # WYWO≈ÅANIE 1: RECENZJA REDAKTORSKA (v41.0)
            # ============================================================
            analysis_prompt = f"""Jeste≈õ REDAKTOREM NACZELNYM polskiego wydawnictwa specjalizujƒÖcego siƒô w tre≈õciach eksperckich.

Dostajesz artyku≈Ç pt. "{topic}" ({word_count} s≈Ç√≥w). Napisz PE≈ÅNƒÑ RECENZJƒò REDAKTORSKƒÑ.
{unused_keywords_section}
{ymyl_section}
{coverage_section}

=== NADRZƒòDNA ZASADA ===
üî¥ POPRAWIAJ tekst, NIE PRZEPISUJ go. Zachowaj oryginalny styl, ton i strukturƒô.
Ka≈ºda Twoja sugestia powinna prowadziƒá do PUNKTOWEJ POPRAWY, nie do przepisania akapitu.

=== CZEGO SZUKAƒÜ (KRYTYCZNE) ===

üö´ ANTY-FILLER: Znajd≈∫ zdania, kt√≥re nie dodajƒÖ ≈ºadnej informacji:
  - Truizmy: ‚ÄûPrzewodnik elektryczny przewodzi prƒÖd." ‚Äî to nic nie wnosi
  - Puste przej≈õcia: ‚ÄûTo prowadzi do kolejnego aspektu." ‚Äî usu≈Ñ lub zamie≈Ñ na tre≈õƒá
  - Bana≈Çy: ‚ÄûWarto zauwa≈ºyƒá, ≈ºe temat jest wa≈ºny." ‚Äî zamie≈Ñ na konkret

üö´ ANTY-HALUCYNACJA: Znajd≈∫ wymy≈õlone dane:
  - Wymy≈õlone statystyki: ‚ÄûWed≈Çug GUS w 2022 roku dosz≈Ço do 300 wypadk√≥w..."
  - Wymy≈õlone rozporzƒÖdzenia: ‚ÄûRozporzƒÖdzenie Ministra X z dnia Y..."
  - Wymy≈õlone ceny/daty: ‚Äûod 1 stycznia 2026 stawka wynosi..."
  ‚Üí Je≈õli znajdziesz ‚Äî zaznacz jako HALUCYNACJA w errors_to_fix

üö´ ENCJE JAKO ≈πR√ìD≈ÅA: Znajd≈∫ zdania w stylu:
  - ‚ÄûWikipedia podaje, ≈ºe..." ‚Äî max 1√ó w artykule
  - ‚ÄûWed≈Çug [nazwy encji]..." ‚Äî encje to pojƒôcia, nie ≈∫r√≥d≈Ça
  - ‚Äû[cokolwiek] potwierdza / podaje / przywo≈Çuje..."
  ‚Üí Zamie≈Ñ na bezpo≈õrednie stwierdzenie faktu

üìä ENCJE TEMATYCZNE: Sprawd≈∫ czy encje z sekcji POKRYCIE ENCJI sƒÖ w tek≈õcie.
  BrakujƒÖce encje wpleƒá w istniejƒÖce zdania ‚Äî NIE tw√≥rz nowych akapit√≥w.

=== STRUKTURA RECENZJI (odpowiedz TYLKO JSON) ===

{{
  "overall_score": <0-10>,
  "scores": {{
    "merytoryka": <0-10>,
    "struktura": <0-10>,
    "styl": <0-10>,
    "seo": <0-10>
  }},

  "editorial_feedback": {{
    
    "recenzja_ogolna": "<3-5 zda≈Ñ: og√≥lna ocena artyku≈Çu. Co jest mocne? Co wymaga pracy? Jaki ton ma tekst i czy jest odpowiedni? Czy artyku≈Ç odpowiada na pytanie czytelnika?>",
    
    "merytoryka": [
      {{
        "sekcja": "<H2/H3 kt√≥rego dotyczy>",
        "uwaga": "<co jest nie tak merytorycznie>",
        "cytat": "<fragment tekstu ‚Äî min 10 s≈Ç√≥w>",
        "sugestia": "<jak poprawiƒá ‚Äî konkretnie>"
      }}
    ],
    
    "styl_i_jezyk": [
      {{
        "problem": "powt√≥rzenie|niezrƒôczno≈õƒá|fraza_AI|kolokacja|strona_bierna|monotonia|filler|truizm",
        "cytat": "<fragment z tekstu>",
        "sugestia": "<jak poprawiƒá>"
      }}
    ],
    
    "struktura_i_narracja": [
      {{
        "uwaga": "<problem ze strukturƒÖ>",
        "gdzie": "<miƒôdzy kt√≥rymi sekcjami / w kt√≥rej sekcji>",
        "sugestia": "<co zmieniƒá>"
      }}
    ],
    
    "luki_tresciowe": [
      {{
        "brakujacy_temat": "<czego czytelnik m√≥g≈Çby szukaƒá, a tekst tego nie pokrywa>",
        "gdzie_dodac": "<w kt√≥rej sekcji najlepiej>",
        "sugestia": "<2-3 zdania co konkretnie dopisaƒá>"
      }}
    ],

    "halucynacje": [
      {{
        "cytat": "<fragment z wymy≈õlonƒÖ statystykƒÖ/datƒÖ/≈∫r√≥d≈Çem>",
        "dlaczego_falsz": "<kr√≥tkie wyja≈õnienie>"
      }}
    ],

    "brakujace_encje": [
      {{
        "encja": "<nazwa encji brakujƒÖcej w tek≈õcie>",
        "gdzie_wplesc": "<w kt√≥rym zdaniu/akapicie>",
        "jak": "<konkretna sugestia wplecenia>"
      }}
    ]
  }},

  "errors_to_fix": [
    {{
      "type": "HALUCYNACJA|FILLER|ENCJA_JAKO_ZRODLO|TERMINOLOGIA|FRAZA_AI|KOLOKACJA|STYL|BRAK_ENCJI",
      "priority": <1-3 gdzie 1=krytyczne>,
      "original": "<cytat z tekstu ‚Äî min 10 s≈Ç√≥w>",
      "replacement": "<poprawka ‚Äî ZACHOWAJ d≈Çugo≈õƒá orygina≈Çu>",
      "action": "POPRAW|USU≈É_HALUCYNACJƒò|WPLEƒÜ_ENCJƒò"
    }}
  ],

  "keywords_to_add": {json.dumps(unused_basic + unused_extended, ensure_ascii=False)},
  
  "summary": "<2-3 zdania: najwa≈ºniejsze co trzeba zrobiƒá z tym artyku≈Çem>"
}}

=== WSKAZ√ìWKI ===
- W "merytoryka" szukaj: brak ≈∫r√≥de≈Ç, nieprecyzyjne twierdzenia, nadmierne uproszczenia
- W "styl_i_jezyk" szukaj: truizmy, filler, frazy AI ("warto zauwa≈ºyƒá", "kluczowym elementem"), puste przej≈õcia
- W "halucynacje" szukaj: konkretne liczby, daty, nazwy bada≈Ñ, rozporzƒÖdzenia ‚Äî czy brzmiƒÖ wiarygodnie?
- W "brakujace_encje" szukaj: encje z sekcji POKRYCIE ENCJI oznaczone ‚ùå
- Ka≈ºda poprawka w errors_to_fix musi mieƒá DOK≈ÅADNY cytat z tekstu (min 10 s≈Ç√≥w)
- REPLACEMENT nie mo≈ºe byƒá kr√≥tszy ni≈º ORIGINAL
- NIE przepisuj ca≈Çych akapit√≥w ‚Äî poprawiaj punktowo

=== ARTYKU≈Å ({word_count} s≈Ç√≥w) ===

{full_text}"""

            print(f"[EDITORIAL_REVIEW] ========== CALL 1: RECENZJA REDAKTORSKA ==========")
            
            analysis_text, ai_model = _claude_call(analysis_prompt, max_tokens=6000, label="CALL 1")
            
            # Parsuj JSON analizy
            try:
                clean = analysis_text
                if "```json" in clean:
                    clean = re.sub(r'```json\s*', '', clean)
                    clean = re.sub(r'```\s*$', '', clean)
                elif "```" in clean:
                    clean = re.sub(r'```\s*', '', clean)
                
                first_brace = clean.find('{')
                last_brace = clean.rfind('}')
                if first_brace != -1 and last_brace > first_brace:
                    analysis = json.loads(clean[first_brace:last_brace + 1])
                    print(f"[EDITORIAL_REVIEW] ‚úÖ Analysis parsed: score={analysis.get('overall_score')}")
            except Exception as e:
                print(f"[EDITORIAL_REVIEW] ‚ö†Ô∏è Analysis parse error: {e}")
                analysis = {"overall_score": 5, "summary": "Analiza nie sparsowana", "raw": analysis_text[:500]}
            
            # ============================================================
            # WYWO≈ÅANIE 2: DIFF-BASED CORRECTION (v29.0)
            # ============================================================
            errors_list = analysis.get("errors_to_fix", []) if analysis else []
            keywords_to_add = analysis.get("keywords_to_add", []) if analysis else []
            
            diff_prompt = f"""Przeanalizuj artyku≈Ç pt. "{topic}" i zwr√≥ƒá TYLKO ZMIANY w formacie diff.

üî¥ NADRZƒòDNA ZASADA: POPRAWIAJ, NIE PRZEPISUJ!
Artyku≈Ç ma {word_count} s≈Ç√≥w. Zachowaj oryginalny styl, ton i strukturƒô.
Zmieniaj TYLKO to, co jest b≈Çƒôdne lub wymaga poprawy. Reszta musi zostaƒá nietkniƒôta.

‚õî KRYTYCZNE ZASADY:
- Zwr√≥ƒá MAX 15 zmian (tylko najwa≈ºniejsze!)
- NIE przepisuj ca≈Çego artyku≈Çu ‚Äî to KOREKTA, nie rewrite
- Kr√≥tkie zdania (2-5 s≈Ç√≥w) sƒÖ CELOWE ‚Äî NIE ≈ÇƒÖcz ich!
- Zachowaj styl i rytm tekstu ‚Äî ka≈ºda zmiana musi pasowaƒá do kontekstu
- Cytat w ZNAJD≈π musi byƒá DOK≈ÅADNY (min 10 s≈Ç√≥w, copy-paste z artyku≈Çu)
- ZAMIE≈É musi mieƒá PODOBNƒÑ d≈Çugo≈õƒá do ZNAJD≈π (¬±20%)
- Jedyny wyjƒÖtek: halucynacje (zmy≈õlone dane) ‚Äî te USU≈É

=== PRIORYTET ZMIAN (od najwa≈ºniejszych) ===
1. üî¥ HALUCYNACJE ‚Äî wymy≈õlone statystyki, rozporzƒÖdzenia, daty ‚Üí USU≈É lub zamie≈Ñ na pewne fakty
2. üî¥ FILLER/TRUIZMY ‚Äî zdania bez informacji ("To prowadzi do...", "Warto zauwa≈ºyƒá...") ‚Üí zamie≈Ñ na tre≈õƒá merytorycznƒÖ
3. üî¥ ENCJA JAKO ≈πR√ìD≈ÅO ‚Äî "Wikipedia podaje...", "Wed≈Çug [encji]..." ‚Üí zamie≈Ñ na bezpo≈õrednie stwierdzenie
4. üü° BRAKUJƒÑCE ENCJE ‚Äî wpleƒá brakujƒÖce encje tematyczne w istniejƒÖce zdania
5. üü° BRAKUJƒÑCE FRAZY SEO ‚Äî wpleƒá nieu≈ºyte frazy naturalnie w tekst
6. üü¢ STYL ‚Äî popraw frazy AI, nienaturalne kolokacje, powt√≥rzenia

=== B≈ÅƒòDY DO POPRAWY ===
{json.dumps(errors_list[:10], ensure_ascii=False, indent=2) if errors_list else "Brak krytycznych b≈Çƒôd√≥w."}

=== FRAZY DO WPLECENIA (w istniejƒÖce zdania!) ===
{', '.join(keywords_to_add[:10]) if keywords_to_add else "Wszystkie frazy sƒÖ w tek≈õcie."}

{coverage_section}

=== FORMAT ODPOWIEDZI ===

[ZMIANA 1]
ZNAJD≈π: "dok≈Çadny cytat z artyku≈Çu (min 10 s≈Ç√≥w, copy-paste)"
ZAMIE≈É: "poprawiona wersja ‚Äî zachowaj styl i podobnƒÖ d≈Çugo≈õƒá"
POW√ìD: kr√≥tkie wyja≈õnienie (max 10 s≈Ç√≥w)

[ZMIANA 2]
ZNAJD≈π: "..."
ZAMIE≈É: "..."
POW√ìD: ...

(kontynuuj do max 15 zmian)

=== ARTYKU≈Å DO KOREKTY ({word_count} s≈Ç√≥w ‚Äî zachowaj d≈Çugo≈õƒá!) ===

{full_text}"""

            print(f"[EDITORIAL_REVIEW] ========== CALL 2: DIFF-BASED CORRECTION ==========")
            
            diff_response, _ = _claude_call(diff_prompt, max_tokens=8000, label="CALL 2")
            
            # Parsuj diffy
            changes = parse_diff_response(diff_response)
            print(f"[EDITORIAL_REVIEW] üìù Parsed {len(changes)} changes from Claude response")
            
            # Aplikuj diffy
            if changes:
                corrected_article, applied_changes, failed_changes = apply_diffs(full_text, changes)
                print(f"[EDITORIAL_REVIEW] ‚úÖ Applied: {len(applied_changes)}, ‚ùå Failed: {len(failed_changes)}")
            else:
                corrected_article = full_text
                print(f"[EDITORIAL_REVIEW] ‚ÑπÔ∏è No changes parsed, using original text")
            
        elif genai:
            # Gemini fallback (stary spos√≥b - przepisuje ca≈Ço≈õƒá)
            print(f"[EDITORIAL_REVIEW] Using Gemini (fallback) for project {project_id}")
            model = genai.GenerativeModel("gemini-2.0-flash")
            
            old_prompt = f"""Jeste≈õ REDAKTOREM. Popraw artyku≈Ç i zwr√≥ƒá JSON:
{{"analysis": {{"overall_score": <0-10>, "summary": "<podsumowanie>"}}, "corrected_article": "<ca≈Çy poprawiony tekst>"}}

Artyku≈Ç ({word_count} s≈Ç√≥w):
{full_text[:10000]}"""
            
            response = model.generate_content(old_prompt)
            review_text = response.text.strip()
            ai_model = "gemini"
            
            try:
                clean = review_text
                if "```json" in clean:
                    clean = re.sub(r'```json\s*', '', clean)
                    clean = re.sub(r'```\s*$', '', clean)
                first_brace = clean.find('{')
                last_brace = clean.rfind('}')
                if first_brace != -1 and last_brace > first_brace:
                    data = json.loads(clean[first_brace:last_brace + 1])
                    analysis = data.get("analysis", {})
                    corrected_article = data.get("corrected_article", "")
            except:
                analysis = {"overall_score": 5, "summary": "Gemini parse error"}
                corrected_article = full_text
        else:
            return jsonify({
                "error": "No AI API configured",
                "hint": "Set ANTHROPIC_API_KEY or GEMINI_API_KEY"
            }), 500
        
        # Fallback je≈õli brak corrected_article
        if not corrected_article or len(corrected_article) < 100:
            print(f"[EDITORIAL_REVIEW] ‚ö†Ô∏è Using original text as fallback")
            corrected_article = full_text
        
        # ================================================================
        # v41.0: WORD COUNT GUARD ‚Äî artyku≈Ç nie powinien siƒô skr√≥ciƒá
        # ================================================================
        corrected_wc = len(corrected_article.split())
        word_count_shrinkage = False
        shrinkage_pct = 0
        
        if corrected_wc < word_count and corrected_article != full_text:
            shrinkage_pct = round((1 - corrected_wc / word_count) * 100, 1)
            print(f"[EDITORIAL_REVIEW] ‚ö†Ô∏è WORD COUNT SHRINKAGE: {word_count} ‚Üí {corrected_wc} (-{shrinkage_pct}%)")
            
            if shrinkage_pct > 5:
                # Skr√≥cenie o >5% = rollback do orygina≈Çu
                word_count_shrinkage = True
                print(f"[EDITORIAL_REVIEW] üî¥ AUTO-ROLLBACK: artyku≈Ç skr√≥ci≈Ç siƒô o {shrinkage_pct}% ‚Äî przywracam orygina≈Ç")
                corrected_article = full_text
        
        # ================================================================
        # v29.0: BURSTINESS CHECK - PO + AUTO-ROLLBACK
        # ================================================================
        burstiness_after = None
        rollback_triggered = False
        rollback_reason = None
        
        if BURSTINESS_CHECK_OK and burstiness_before and corrected_article != full_text:
            try:
                burstiness_after = calculate_burstiness(corrected_article)
                cv_before = burstiness_before.get("cv", 0)
                cv_after = burstiness_after.get("cv", 0)
                
                print(f"[EDITORIAL_REVIEW] üìä Burstiness PO: CV={cv_after}")
                
                # ROLLBACK je≈õli CV spad≈Ço o wiƒôcej ni≈º 0.1 LUB spad≈Ço poni≈ºej 0.3
                cv_drop = cv_before - cv_after
                
                if cv_drop > 0.1:
                    rollback_triggered = True
                    rollback_reason = f"CV spad≈Ço o {cv_drop:.2f} (z {cv_before:.2f} do {cv_after:.2f}) ‚Äî tekst sta≈Ç siƒô zbyt monotonny"
                    print(f"[EDITORIAL_REVIEW] ‚ö†Ô∏è AUTO-ROLLBACK: {rollback_reason}")
                    corrected_article = full_text
                    
                elif cv_after < 0.3 and cv_before >= 0.3:
                    rollback_triggered = True
                    rollback_reason = f"CV spad≈Ço poni≈ºej progu AI ({cv_after:.2f} < 0.3) ‚Äî tekst wyglƒÖda na wygenerowany przez AI"
                    print(f"[EDITORIAL_REVIEW] ‚ö†Ô∏è AUTO-ROLLBACK: {rollback_reason}")
                    corrected_article = full_text
                    
            except Exception as e:
                print(f"[EDITORIAL_REVIEW] ‚ö†Ô∏è Burstiness after check failed: {e}")
        
        # v41.0: Rollback z powodu skr√≥cenia artyku≈Çu
        if word_count_shrinkage and not rollback_triggered:
            rollback_triggered = True
            rollback_reason = f"Artyku≈Ç skr√≥ci≈Ç siƒô o {shrinkage_pct}% ({word_count} ‚Üí {corrected_wc} s≈Ç√≥w) ‚Äî editorial review powinien ROZBUDOWYWAƒÜ, nie skracaƒá"
        
        # Statystyki diff
        diff_stats = calculate_diff_stats(full_text, corrected_article)
        
        # v28.1: GRAMMAR CORRECTION (je≈õli nie by≈Ço rollbacku)
        grammar_stats = {"fixes": 0, "removed": []}
        if not rollback_triggered:
            try:
                from grammar_checker import full_correction
                grammar_result = full_correction(corrected_article)
                corrected_article = grammar_result["corrected"]
                grammar_stats = {
                    "fixes": grammar_result["grammar_fixes"],
                    "removed": grammar_result["phrases_removed"],
                    "backend": grammar_result.get("backend", "unknown")
                }
                if grammar_stats["fixes"] > 0 or grammar_stats["removed"]:
                    print(f"[EDITORIAL_REVIEW] ‚úÖ Grammar: {grammar_stats['fixes']} fixes, {len(grammar_stats['removed'])} phrases removed")
            except ImportError:
                print(f"[EDITORIAL_REVIEW] ‚ö†Ô∏è grammar_checker not available, skipping grammar correction")
            except Exception as e:
                print(f"[EDITORIAL_REVIEW] ‚ö†Ô∏è Grammar correction error: {e}")
        
        corrected_word_count = len(corrected_article.split())
        
        # Zapisz w Firestore
        db.collection("seo_projects").document(project_id).update({
            "editorial_review": {
                "analysis": analysis,
                "corrected_article": corrected_article,
                "original_article": full_text,  # v29.0: Zawsze zachowuj orygina≈Ç!
                "original_word_count": word_count,
                "corrected_word_count": corrected_word_count,
                "unused_keywords": {
                    "basic": unused_basic,
                    "extended": unused_extended
                },
                # v29.0: Nowe pola
                "applied_changes": applied_changes[:20],
                "failed_changes": failed_changes[:10],
                "burstiness_before": burstiness_before,
                "burstiness_after": burstiness_after,
                "rollback_triggered": rollback_triggered,
                "rollback_reason": rollback_reason,
                "diff_stats": diff_stats,
                "grammar_correction": grammar_stats,
                "ai_model": ai_model,
                "version": "v41.0-expand",
                "created_at": firestore.SERVER_TIMESTAMP
            }
        })
        
        # v27.1: RESCAN KEYWORDS
        rescan_result = {"rescanned": False, "reason": "no_corrected_article"}
        if not rollback_triggered and corrected_article and len(corrected_article.strip()) > 50:
            print(f"[EDITORIAL_REVIEW] Running rescan_keywords for project {project_id}...")
            rescan_result = rescan_keywords_after_editorial(project_id, corrected_article)
            print(f"[EDITORIAL_REVIEW] Rescan result: {rescan_result.get('changes_count', 0)} changes")
        
        return jsonify({
            "status": "REVIEWED",
            "version": "v41.0-DIFF-EXPAND",
            "overall_score": analysis.get("overall_score") if analysis else None,
            "scores": analysis.get("scores", {}) if analysis else {},
            "summary": analysis.get("summary", "") if analysis else "",
            
            # üÜï v41.0: PE≈ÅNA RECENZJA REDAKTORSKA
            "editorial_feedback": analysis.get("editorial_feedback", {}) if analysis else {},
            
            # v29.0: DIFF OUTPUT
            "diff_result": {
                "total_changes_parsed": len(applied_changes) + len(failed_changes),
                "applied": len(applied_changes),
                "failed": len(failed_changes),
                "applied_changes": applied_changes[:10],
                "failed_changes": failed_changes[:5]
            },
            "diff_stats": diff_stats,
            
            # v29.0: BURSTINESS VALIDATION
            "burstiness_check": {
                "enabled": BURSTINESS_CHECK_OK,
                "before": {
                    "cv": burstiness_before.get("cv") if burstiness_before else None,
                    "value": burstiness_before.get("value") if burstiness_before else None,
                    "status": burstiness_before.get("status") if burstiness_before else None
                },
                "after": {
                    "cv": burstiness_after.get("cv") if burstiness_after else None,
                    "value": burstiness_after.get("value") if burstiness_after else None,
                    "status": burstiness_after.get("status") if burstiness_after else None
                },
                "cv_change": round((burstiness_after.get("cv", 0) - burstiness_before.get("cv", 0)), 3) if (burstiness_before and burstiness_after) else None
            },
            
            # v41.0: WORD COUNT GUARD
            "word_count_guard": {
                "original": word_count,
                "corrected": corrected_word_count,
                "shrinkage_pct": shrinkage_pct,
                "rollback_triggered": word_count_shrinkage,
                "policy": "Editorial review powinien ROZBUDOWYWAƒÜ tre≈õƒá, nie skracaƒá"
            },
            
            # v29.0: ROLLBACK INFO
            "rollback": {
                "triggered": rollback_triggered,
                "reason": rollback_reason,
                "original_preserved": True
            },
            
            "corrected_article": corrected_article,
            "word_count": {
                "original": word_count,
                "corrected": corrected_word_count
            },
            "unused_keywords_input": {
                "basic": unused_basic,
                "extended": unused_extended
            },
            "ai_model": ai_model,
            "grammar_correction": grammar_stats,
            "rescan_result": rescan_result,
            
            # üÜï v44.5: YMYL validation
            "ymyl_check": {
                "detected_category": detected_category,
                "is_ymyl": detected_category in ("prawo", "medycyna"),
                "ymyl_reviewed": detected_category in ("prawo", "medycyna")
            } if detected_category in ("prawo", "medycyna") else None
        }), 200
        
    except Exception as e:
        import traceback
        tb = traceback.format_exc()
        print(f"[EDITORIAL_REVIEW] ‚ùå Error: {e}")
        print(f"[EDITORIAL_REVIEW] Traceback:\n{tb}")
        return jsonify({
            "error": f"Review failed: {str(e)}",
            "error_type": type(e).__name__,
            "word_count": word_count if 'word_count' in dir() else 0,
            "debug": debug_info if 'debug_info' in dir() else {},
            "traceback_hint": tb.strip().split("\n")[-1] if tb else ""
        }), 500


# ================================================================
# v29.0: MANUAL ROLLBACK ENDPOINT
# ================================================================
@export_routes.post("/api/project/<project_id>/editorial_review/rollback")
def editorial_rollback(project_id):
    """
    v29.0: Rƒôczny rollback do oryginalnego tekstu.
    U≈ºyj gdy automatyczny rollback nie zadzia≈Ça≈Ç lub chcesz przywr√≥ciƒá orygina≈Ç.
    """
    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404
    
    project_data = doc.to_dict()
    editorial = project_data.get("editorial_review", {})
    
    original = editorial.get("original_article")
    if not original:
        return jsonify({
            "error": "No original article saved",
            "hint": "Original article is only available after editorial_review v29.0+"
        }), 400
    
    current = editorial.get("corrected_article", "")
    
    if original == current:
        return jsonify({
            "status": "NO_CHANGE",
            "message": "Tekst ju≈º jest oryginalny (poprzedni rollback lub brak zmian)"
        }), 200
    
    # Przywr√≥ƒá orygina≈Ç
    db.collection("seo_projects").document(project_id).update({
        "editorial_review.corrected_article": original,
        "editorial_review.manual_rollback": True,
        "editorial_review.rollback_at": firestore.SERVER_TIMESTAMP,
        "editorial_review.pre_rollback_article": current
    })
    
    return jsonify({
        "status": "ROLLED_BACK",
        "message": "Przywr√≥cono oryginalny tekst",
        "word_count": {
            "original": len(original.split()),
            "was_corrected": len(current.split())
        }
    }), 200


# ================================================================
# EXPORT STATUS
# ================================================================
@export_routes.get("/api/project/<project_id>/export_status")
def export_status(project_id):
    """Sprawdza status projektu przed eksportem."""
    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404
    
    project_data = doc.to_dict()
    
    full_text = None
    debug_info = {"source": "none"}
    batches_count = 0
    
    if _safe_get_full_article(project_data):
        full_text = _safe_get_full_article(project_data)
        debug_info = {"source": "full_article"}
    else:
        batches = project_data.get("batches", [])
        batches_count = len(batches)
        if batches:
            full_text, debug_info = extract_text_from_batches(batches)
            debug_info["source"] = "batches"
    
    word_count = len(full_text.split()) if full_text else 0
    
    editorial = project_data.get("editorial_review", {})
    
    return jsonify({
        "project_id": project_id,
        "topic": project_data.get("topic", ""),
        "has_content": bool(full_text),
        "word_count": word_count,
        "batches_count": batches_count,
        "content_source": debug_info.get("source", "none"),
        "editorial_review": {
            "done": bool(editorial),
            "score": editorial.get("analysis", {}).get("overall_score") if editorial else None,
            "version": editorial.get("version", "unknown"),
            "rollback_triggered": editorial.get("rollback_triggered", False),
            "burstiness_cv_before": editorial.get("burstiness_before", {}).get("cv") if editorial.get("burstiness_before") else None,
            "burstiness_cv_after": editorial.get("burstiness_after", {}).get("cv") if editorial.get("burstiness_after") else None
        },
        "available_exports": {
            "docx": f"/api/project/{project_id}/export/docx",
            "html": f"/api/project/{project_id}/export/html",
            "txt": f"/api/project/{project_id}/export/txt"
        },
        "next_steps": {
            "editorial_review": f"POST /api/project/{project_id}/editorial_review",
            "export_docx": f"GET /api/project/{project_id}/export/docx",
            "export_html": f"GET /api/project/{project_id}/export/html"
        }
    }), 200


# ================================================================
# EXPORT DOCX
# ================================================================
@export_routes.get("/api/project/<project_id>/export/docx")
def export_docx(project_id):
    """Eksportuje artyku≈Ç do formatu DOCX."""
    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404
    
    project_data = doc.to_dict()
    
    # v32.4: Priorytet ≈∫r√≥de≈Ç tre≈õci
    # 1. editorial_review.corrected_article (po Claude review)
    # 2. full_article.content (zapisany przez save_full_article)
    # 3. batches (surowe batche)
    editorial = project_data.get("editorial_review", {})
    corrected = editorial.get("corrected_article", "")
    
    if corrected:
        full_text = corrected
        print(f"[EXPORT_DOCX] Using corrected article ({len(corrected.split())} words)")
    elif _safe_get_full_article(project_data):
        full_text = _safe_get_full_article(project_data)
        print(f"[EXPORT_DOCX] Using full_article ({len(full_text.split())} words)")
    else:
        batches = project_data.get("batches", [])
        full_text, _ = extract_text_from_batches(batches)
        print(f"[EXPORT_DOCX] Using batches ({len(full_text.split()) if full_text else 0} words)")
    
    if not full_text:
        return jsonify({"error": "No content to export"}), 400
    
    # üÜï v44.5: YMYL disclaimer guard
    detected_category = project_data.get("detected_category", "inne")
    if detected_category == "prawo" and "zastrze≈ºenie" not in full_text.lower():
        full_text += (
            "\n\n---\n\n"
            "**Zastrze≈ºenie prawne:** Niniejszy artyku≈Ç ma charakter wy≈ÇƒÖcznie informacyjny "
            "i nie stanowi porady prawnej. W indywidualnych sprawach zalecamy konsultacjƒô "
            "z wykwalifikowanym prawnikiem."
        )
        print(f"[EXPORT_DOCX] ‚öñÔ∏è Auto-dodano brakujƒÖcy disclaimer prawny")
    elif detected_category == "medycyna" and "zastrze≈ºenie" not in full_text.lower():
        full_text += (
            "\n\n---\n\n"
            "**Zastrze≈ºenie medyczne:** Niniejszy artyku≈Ç ma charakter wy≈ÇƒÖcznie informacyjny "
            "i edukacyjny. Nie stanowi porady medycznej ani nie zastƒôpuje konsultacji "
            "z lekarzem lub innym wykwalifikowanym specjalistƒÖ."
        )
        print(f"[EXPORT_DOCX] üè• Auto-dodano brakujƒÖcy disclaimer medyczny")
    
    topic = project_data.get("topic", "Artyku≈Ç")
    elements = parse_article_structure(full_text)
    
    # Tw√≥rz dokument
    document = Document()
    
    # Tytu≈Ç
    title = document.add_heading(topic, 0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    # Tre≈õƒá
    for el in elements:
        if el["type"] == "h2":
            document.add_heading(el["content"], level=2)
        elif el["type"] == "h3":
            document.add_heading(el["content"], level=3)
        elif el["type"] == "paragraph":
            p = document.add_paragraph(el["content"])
            p.style.font.size = Pt(11)
    
    # Zapisz do bufora
    buffer = io.BytesIO()
    document.save(buffer)
    buffer.seek(0)
    
    # Bezpieczna nazwa pliku
    polish_map = str.maketrans({
        'ƒÖ': 'a', 'ƒá': 'c', 'ƒô': 'e', '≈Ç': 'l', '≈Ñ': 'n',
        '√≥': 'o', '≈õ': 's', '≈∫': 'z', '≈º': 'z',
        'ƒÑ': 'A', 'ƒÜ': 'C', 'ƒò': 'E', '≈Å': 'L', '≈É': 'N',
        '√ì': 'O', '≈ö': 'S', '≈π': 'Z', '≈ª': 'Z'
    })
    safe_topic = topic.translate(polish_map)
    filename = re.sub(r'[^\w\s-]', '', safe_topic)[:50] + ".docx"
    
    from urllib.parse import quote
    filename_utf8 = quote(re.sub(r'[^\w\s-]', '', topic)[:50] + ".docx")
    
    return Response(
        buffer.getvalue(),
        mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        headers={
            "Content-Disposition": f"attachment; filename={filename}; filename*=UTF-8''{filename_utf8}"
        }
    )


# ================================================================
# EXPORT HTML
# ================================================================
@export_routes.get("/api/project/<project_id>/export/html")
def export_html(project_id):
    """Eksportuje artyku≈Ç do formatu HTML."""
    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404
    
    project_data = doc.to_dict()
    
    editorial = project_data.get("editorial_review", {})
    corrected = editorial.get("corrected_article", "")
    
    if corrected:
        full_text = corrected
    elif _safe_get_full_article(project_data):
        full_text = _safe_get_full_article(project_data)
    else:
        batches = project_data.get("batches", [])
        full_text, _ = extract_text_from_batches(batches)
    
    if not full_text:
        return jsonify({"error": "No content to export"}), 400
    
    # üÜï v44.5: YMYL disclaimer guard (HTML export)
    detected_category = project_data.get("detected_category", "inne")
    if detected_category == "prawo" and "zastrze≈ºenie" not in full_text.lower():
        full_text += (
            "\n\n---\n\n"
            "**Zastrze≈ºenie prawne:** Niniejszy artyku≈Ç ma charakter wy≈ÇƒÖcznie informacyjny "
            "i nie stanowi porady prawnej. W indywidualnych sprawach zalecamy konsultacjƒô "
            "z wykwalifikowanym prawnikiem."
        )
    elif detected_category == "medycyna" and "zastrze≈ºenie" not in full_text.lower():
        full_text += (
            "\n\n---\n\n"
            "**Zastrze≈ºenie medyczne:** Niniejszy artyku≈Ç ma charakter wy≈ÇƒÖcznie informacyjny "
            "i edukacyjny. Nie stanowi porady medycznej ani nie zastƒôpuje konsultacji "
            "z lekarzem lub innym wykwalifikowanym specjalistƒÖ."
        )
    
    topic = project_data.get("topic", "Artyku≈Ç")
    
    # Konwertuj markery na HTML
    html = full_text
    html = re.sub(r'^h2:\s*(.+)$', r'<h2>\1</h2>', html, flags=re.MULTILINE)
    html = re.sub(r'^h3:\s*(.+)$', r'<h3>\1</h3>', html, flags=re.MULTILINE)
    
    # Paragrafy
    lines = html.split('\n')
    processed = []
    for line in lines:
        line = line.strip()
        if line and not line.startswith('<h'):
            if line.startswith('- ') or line.startswith('‚Ä¢ '):
                processed.append(f'<li>{line[2:]}</li>')
            else:
                processed.append(f'<p>{line}</p>')
        else:
            processed.append(line)
    
    html_content = f"""<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <title>{topic}</title>
    <style>
        body {{ font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; line-height: 1.6; }}
        h1 {{ color: #333; }}
        h2 {{ color: #444; margin-top: 30px; }}
        h3 {{ color: #555; }}
        p {{ margin: 15px 0; }}
    </style>
</head>
<body>
    <h1>{topic}</h1>
    {''.join(processed)}
</body>
</html>"""
    
    polish_map = str.maketrans({
        'ƒÖ': 'a', 'ƒá': 'c', 'ƒô': 'e', '≈Ç': 'l', '≈Ñ': 'n',
        '√≥': 'o', '≈õ': 's', '≈∫': 'z', '≈º': 'z',
        'ƒÑ': 'A', 'ƒÜ': 'C', 'ƒò': 'E', '≈Å': 'L', '≈É': 'N',
        '√ì': 'O', '≈ö': 'S', '≈π': 'Z', '≈ª': 'Z'
    })
    safe_topic = topic.translate(polish_map)
    filename = re.sub(r'[^\w\s-]', '', safe_topic)[:50] + ".html"
    
    from urllib.parse import quote
    filename_utf8 = quote(re.sub(r'[^\w\s-]', '', topic)[:50] + ".html")
    
    return Response(
        html_content,
        mimetype="text/html",
        headers={
            "Content-Disposition": f"attachment; filename={filename}; filename*=UTF-8''{filename_utf8}"
        }
    )


# ================================================================
# EXPORT TXT
# ================================================================
@export_routes.get("/api/project/<project_id>/export/txt")
def export_txt(project_id):
    """Eksportuje artyku≈Ç do formatu TXT."""
    db = firestore.client()
    doc = db.collection("seo_projects").document(project_id).get()
    
    if not doc.exists:
        return jsonify({"error": "Project not found"}), 404
    
    project_data = doc.to_dict()
    
    editorial = project_data.get("editorial_review", {})
    corrected = editorial.get("corrected_article", "")
    
    if corrected:
        full_text = corrected
    elif _safe_get_full_article(project_data):
        full_text = _safe_get_full_article(project_data)
    else:
        batches = project_data.get("batches", [])
        full_text, _ = extract_text_from_batches(batches)
    
    if not full_text:
        return jsonify({"error": "No content to export"}), 400
    
    topic = project_data.get("topic", "Artyku≈Ç")
    
    # Konwertuj do czytelnego tekstu
    txt = f"{topic}\n{'='*len(topic)}\n\n"
    txt += re.sub(r'^h2:\s*(.+)$', r'\n## \1\n', full_text, flags=re.MULTILINE)
    txt = re.sub(r'^h3:\s*(.+)$', r'\n### \1\n', txt, flags=re.MULTILINE)
    
    polish_map = str.maketrans({
        'ƒÖ': 'a', 'ƒá': 'c', 'ƒô': 'e', '≈Ç': 'l', '≈Ñ': 'n',
        '√≥': 'o', '≈õ': 's', '≈∫': 'z', '≈º': 'z',
        'ƒÑ': 'A', 'ƒÜ': 'C', 'ƒò': 'E', '≈Å': 'L', '≈É': 'N',
        '√ì': 'O', '≈ö': 'S', '≈π': 'Z', '≈ª': 'Z'
    })
    safe_topic = topic.translate(polish_map)
    filename = re.sub(r'[^\w\s-]', '', safe_topic)[:50] + ".txt"
    
    from urllib.parse import quote
    filename_utf8 = quote(re.sub(r'[^\w\s-]', '', topic)[:50] + ".txt")
    
    return Response(
        txt,
        mimetype="text/plain; charset=utf-8",
        headers={
            "Content-Disposition": f"attachment; filename={filename}; filename*=UTF-8''{filename_utf8}"
        }
    )


# ================================================================
# ALIASY dla kompatybilno≈õci
# ================================================================
@export_routes.post("/api/project/<project_id>/gemini_review")
def gemini_review_alias(project_id):
    """Alias do editorial_review dla kompatybilno≈õci"""
    return editorial_review(project_id)


@export_routes.post("/api/project/<project_id>/claude_review")
def claude_review_alias(project_id):
    """Alias do editorial_review"""
    return editorial_review(project_id)
